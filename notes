 



For ever Idea 1 - y subarray find the minimum height.

Idea 2 - For every height find the width.

we have to find the nearest smaller on the left of the of the current element and nearews smaller on right of the current element.


if wee consider nearst smaller on left = j and nearest smaller on right = k, then total width will be k-j-1.

and height will be = A[i]

so from this height and width we can find out the total area covered by the rectangle.


pseudo code - 

NSL[N]
NSR[N]

maxArea = 0

for i from 0 to N-1 {

height = A[i]

l = NSL[i]
r = NSR[i]

width = r -l -1;
area = height * width;

maxArea = max( maxArea, area);
}

print maxArea;


TC - O(n)
SC - O(n)


Q - Find sum of (max - min) for all subarrays.


max minus min


arr - [1, 2, 3]

subarray     max       min      max-min
[1]	      1         1          0
[1,2]         2         1          2-1 = 1
[1, 2, 3]     3         1          3-1 = 2
[2]           2         2          2-2 =0
[2,3]         3         2          3-2=1
[3]	      3         3	    3-3=0

so the total sum will be 4


BF - for every subarray find min, find max then take sum of their difference.

TC - O(n^3)

if we use the carry forwars the min and max then TC will reduce to O(n^2)



Optmization - 

sumMax = sum of all the max of all subarrays .
sumMin = sum of all the min  of all subarrays

ans will be sumMax = sumMin

in above example it will be -  14 - 10 = 4

now lets try to solve sum of all subarrays max - 

for arr A = 2, 7 , 3

subarray  	max element
2			2
2, 7			7
2,7,3			7
7			7
7,3			7
3			3

so the total sum of the max will be = 33

7*4 +  3* 1+ 2*1 = 33

so here 7 is contributed 4 times, 3 is contributed once and 2 is contributed once.

in this we can use the contribution technique.


in how many subarrays ith element is the maximum element - 

arr = [1, 8, 3, 5, 4, 2, 11, 7, 2]
index  0 , 1, 2, 3,4, 5, 6,  7, 8

so from the above array we can say that the 5 will be maximum in below subarrays 

3, 5
3, 5, 4
3, 5, 4, 2
5
5, 4
5, 4, 2


contribution will be = summation of all A[i] * no of all subarrays in which A[i] is max

so from above example we can say that 

on left side of the 5 we can create subarray from 3 and on right we can have subarrayupto 2.

the subarrays starting from the  element value 3 and ending on the element value 2, 5 will be the maximum value element.

so to find out the sub array range we need to find the Nearest greater element on left and nearest greater element on the right of the element with the value 5.


choices of left index  for element 5 = index 2
 or 3. ie two choices.
 
 choices of right index = 3, 4, 5. here we have three choices.
 
 so the no of subarrays in which 5 will be the max value  = choices of left index * choices of right index = 2 * 3 = 6
 
 so in 6 subarrays  will be the maximum value.
 
 Generalization - 
 
  total subarrays where arr[i] is max =  (i-NGL[i])  * (NGR[i]-i)
  
  
  
  code - 
  
  ans =0
  	
  NGR[N]
  NGL[N]
  
  
  for ( i from 0 to N-1) {
  
   count = (NGR[i] -i) * (i - NGL[i]);
   ans += A[i] * count;
   }
   
   return ans;
   
   
   
   so the code for finding out total sum of max element will be - 
   
   
   int findMax(A[]) {
   
   ans =0
   NGR[N]
   NGL[N]
   
   for( i from 0 to N-1){
   
   count = (NGR[i]-i) *  (i - NGL[i])
   ans += A[i] * cpount;
   }

return ans;   
}



now calculate the sum of min of every subarray - 

arr = 2, 7, 4
		min
2		2
2, 7		2
2, 7, 4		2
7		7
7, 4		4
4		4

so the total sum of the min is 21.

we can use the code written for the finding the sum of max of all subarrays, we just have to make the values of the array negative, so it will give the sum of all minimum elements in an array then take the negative of the returned value which will be our sum of the minimum elements in each subarray.


TC - O(N)
SC - O(N)


make the values of the array A negative and then pass that array to the findMax function above. then whatever it is returning, take that values negative, that will be the sum of all the minimums of all subarrays.



int maxSum = findMax(A);

// make array values negative 

for( int i=0; i < A.length; i++){
 	A[i] = 0-A[i];
 }
 
 int minSum = 0 - findMax(A);
 
 so the final ans will be =  maxSum - minSum;




code  - 

not using above approach of finding the minimum sum of all subarrays.


  public int solve(ArrayList<Integer> A) {

        long maxTotal =0;
        long count =0;
         long mod = 1000000007;

         ArrayList<Integer> ngl = NGL(A);
         ArrayList<Integer> ngr = NGR(A);


        for( int i= 0; i < A.size(); i++){

    long left = i - ngl.get(i);
    long right = ngr.get(i) - i;

          count = (left * right) % mod;
    maxTotal = (maxTotal + ((long)A.get(i) * count) % mod) % mod;
        }

        long minTotal =0;

         ArrayList<Integer> nsl = NSL(A);
         ArrayList<Integer> nsr = NSR(A);

        for( int i= 0; i < A.size(); i++){
            long left = i - nsl.get(i);
    long right = nsr.get(i) - i;
    count = (left * right) % mod;
    minTotal = (minTotal + ((long)A.get(i) * count) % mod) % mod;
        }

       

       return (int)((maxTotal - minTotal + mod) % mod);

    }


    public  ArrayList<Integer> NGL(ArrayList<Integer> A){
         Stack<Integer> st = new Stack<Integer>();
        ArrayList<Integer> ans = new ArrayList<Integer>();

        for( int i =0; i < A.size(); i++){

            while(!st.isEmpty() && A.get(st.peek()) < A.get(i)){
                st.pop();
            }

            if( st.isEmpty()){
                ans.add(-1);
            }else{
                ans.add(st.peek());
            }

            st.push(i);

        }

        return ans;
    }

    public  ArrayList<Integer> NGR(ArrayList<Integer> A) {

           Stack<Integer> st = new Stack<Integer>();
        ArrayList<Integer> ans = new ArrayList<Integer>();

        for( int i =A.size()-1; i >= 0; i--){

            while(!st.isEmpty() && A.get(st.peek()) <= A.get(i)){
                st.pop();
            }

            if( st.isEmpty()){
                ans.add(A.size()); 
            }else{
                ans.add(st.peek());
            }

            st.push(i);

        }

        Collections.reverse(ans);
        return ans;

    }

    public  ArrayList<Integer> NSL(ArrayList<Integer> A){
         Stack<Integer> st = new Stack<Integer>();
        ArrayList<Integer> ans = new ArrayList<Integer>();

        for( int i =0; i < A.size(); i++){

            while(!st.isEmpty() && A.get(st.peek()) > A.get(i)){
                st.pop();
            }

            if( st.isEmpty()){
                ans.add(-1);
            }else{
                ans.add(st.peek());
            }

            st.push(i);

        }

        return ans;
    }

    public  ArrayList<Integer> NSR(ArrayList<Integer> A) {

           Stack<Integer> st = new Stack<Integer>();
        ArrayList<Integer> ans = new ArrayList<Integer>();

        for( int i =A.size()-1; i >= 0; i--){

            while(!st.isEmpty() && A.get(st.peek()) >= A.get(i)){
                st.pop();
            }

            if( st.isEmpty()){
                ans.add(A.size()); 
            }else{
                ans.add(st.peek());
            }

            st.push(i);

        }

        Collections.reverse(ans);
        return ans;

    }	
    
    
    
DSA: Queues: Implementation & Problems


Think of the brute force first 
Vocalize while coding

try to take the hints if u are stuck.

give mock interviews


Queue - first in first out.

operations supported - 

Enqueue(x) - add x at the rear end
Dequeue() - remove front element.
front() / peek() - return element at front.
isEmpty()
size()

implement queue using array - 

class Queue {
    private int[] arr;
    private int front;
    private int rear;
    private int size;
    private int capacity;

    public Queue(int capacity) {
        this.capacity = capacity;
        this.arr = new int[capacity];
        this.front = 0;
        this.size = 0;
        this.rear = -1;
    }

    // Add an element to the queue
    public void enqueue(int item) {
        if (isFull()) {
            System.out.println("Queue is full!");
            return;
        }
        rear = (rear + 1) % capacity; // Circular increment
        arr[rear] = item;
        size++;
    }

    // Remove an element from the queue
    public int dequeue() {
        if (isEmpty()) {
            System.out.println("Queue is empty!");
            return -1; // Indicates the queue is empty
        }
        int item = arr[front];
        front = (front + 1) % capacity; // Circular increment
        size--;
        return item;
    }

    // Peek the front element of the queue
    public int peek() {
        if (isEmpty()) {
            System.out.println("Queue is empty!");
            return -1; // Indicates the queue is empty
        }
        return arr[front];
    }

    // Check if the queue is empty
    public boolean isEmpty() {
        return size == 0;
    }

    // Check if the queue is full
    public boolean isFull() {
        return size == capacity;
    }

    // Return the current size of the queue
    public int getSize() {
        return size;
    }
}
Time Complexities of Operations
enqueue(int item):

Time Complexity: 

O(1)
The operation involves calculating the next rear index and inserting the element.
dequeue():

Time Complexity: 

O(1)
The operation involves retrieving the front element and updating the front index.
peek():

Time Complexity: 
O(1)
The operation only accesses the front element of the queue.
isEmpty():

Time Complexity: 

O(1)
Simply checks the size variable.
isFull():

Time Complexity: 

O(1)
Simply compares the size with the capacity.
getSize():

Time Complexity: 

O(1)
Simply returns the size variable.


Implement queue using linked list - 

Enqueue at tail and dequeue at head

class QueueNode {
    int data;
    QueueNode next;

    public QueueNode(int data) {
        this.data = data;
        this.next = null;
    }
}

class LinkedListQueue {
    private QueueNode front, rear;

    public LinkedListQueue() {
        this.front = this.rear = null;
    }

    // Add an element to the queue
    public void enqueue(int item) {
        QueueNode newNode = new QueueNode(item);

        // If the queue is empty, both front and rear are the new node
        if (rear == null) {
            front = rear = newNode;
            return;
        }

        // Add the new node at the end of the queue and update rear
        rear.next = newNode;
        rear = newNode;
    }

    // Remove an element from the queue
    public int dequeue() {
        if (isEmpty()) {
            System.out.println("Queue is empty!");
            return -1; // Indicates the queue is empty
        }

        int item = front.data;
        front = front.next;

        // If the queue becomes empty, set rear to null
        if (front == null) {
            rear = null;
        }
        return item;
    }

    // Peek the front element of the queue
    public int peek() {
        if (isEmpty()) {
            System.out.println("Queue is empty!");
            return -1; // Indicates the queue is empty
        }
        return front.data;
    }

    // Check if the queue is empty
    public boolean isEmpty() {
        return front == null;
    }
}
Time Complexities of Operations
enqueue(int item):

Time Complexity: 

O(1)
Adding a new node at the end of the linked list is done in constant time.
dequeue():

Time Complexity: 

O(1)
Removing the front node is done in constant time.
peek():

Time Complexity: 

O(1)
Accessing the front node is done in constant time.
isEmpty():

Time Complexity: 

O(1)
Simply checks if the front node is null.


Q - Implement queue using stacks - Directi

we can only use the push, pop, top, peek, isEmpty, size functions of the stack.

so we can not simulate two ends of the queue using only one stack so we will have to use the two stacks.


import java.util.Stack;

class QueueUsingStacks {
    private Stack<Integer> stack1;
    private Stack<Integer> stack2;

    public QueueUsingStacks() {
        stack1 = new Stack<>(); // Stack for enqueue operations
        stack2 = new Stack<>(); // Stack for dequeue operations
    }

    // Enqueue an element into the queue
    public void enqueue(int item) {
        stack1.push(item);
    }

    // Dequeue an element from the queue
    public int dequeue() {
        if (stack2.isEmpty()) {
            if (stack1.isEmpty()) {
                System.out.println("Queue is empty!");
                return -1; // Indicates the queue is empty
            }
            // Transfer all elements from stack1 to stack2
            while (!stack1.isEmpty()) {
                stack2.push(stack1.pop());
            }
        }
        return stack2.pop();
    }

    // Peek at the front element of the queue
    public int peek() {
        if (stack2.isEmpty()) {
            if (stack1.isEmpty()) {
                System.out.println("Queue is empty!");
                return -1; // Indicates the queue is empty
            }
            // Transfer all elements from stack1 to stack2
            while (!stack1.isEmpty()) {
                stack2.push(stack1.pop());
            }
        }
        return stack2.peek();
    }

    // Check if the queue is empty
    public boolean isEmpty() {
        return stack1.isEmpty() && stack2.isEmpty();
    }
}
Explanation of Operations
Enqueue Operation (enqueue):
Mechanism:
Simply push the new element onto stack1.
Steps:
Add the element to stack1.
No additional action is required.
Time Complexity: 

O(1), as the push operation in a stack is constant time.
Dequeue Operation (dequeue):
Mechanism:
If stack2 is empty:
Pop all elements from stack1 and push them onto stack2. This reverses the order of elements.
Pop the top element from stack2 (which corresponds to the front of the queue). If stack2 is not empty:
Simply pop the top element from stack2.
Steps:
Case 1: Both stacks are empty: The queue is empty.
Case 2: stack2 is empty but stack1 is not: Transfer elements from stack1 to stack2.
Case 3: stack2 is not empty: Pop the top element from stack2.
Time Complexity:
Amortized Time Complexity: 

O(1) per operation over a sequence of 
𝑛
n operations.
Worst-case Time Complexity: 

O(n), when all elements need to be transferred from stack1 to stack2.
Peek Operation (peek):
Mechanism:
Similar to dequeue, except the top element of stack2 is returned instead of popped. If stack2 is empty, transfer elements from stack1 to stack2.
Steps:
If stack2 is empty, transfer all elements from stack1 to stack2.
Return the top element of stack2.
Time Complexity:
Amortized Time Complexity: 

O(1) per operation over a sequence of 
𝑛
n operations.
Worst-case Time Complexity: 

O(n), during the transfer.
Check if the Queue is Empty (isEmpty):
Mechanism:
Check if both stack1 and stack2 are empty.
Time Complexity: 

O(1), as it involves only a few comparisons.
Detailed Time Complexity of Dequeue
Amortized Analysis:
When dequeue is called:
If stack2 is empty, all elements from stack1 are moved to stack2. This happens only once for each element during its lifecycle in the queue.
After transfer, all subsequent dequeues for these elements are 

O(1).
Therefore, the amortized time complexity for dequeue over 
𝑛
n operations is 

O(1), even though a single dequeue may take 

O(n) in the worst case.



Deque - notes remaining

in python - deque
in java - array deque


Q. sliding window maximum - 

find max of every subarray of size k.

BF - 
  for every subarray of size k , iterate and find max in each.
  
  TC - O(n * k )
  
Optimization - 

observation - we need the datastructure which will follow the LIFO and FIFO approach.

Idea - maintain the monotonic decreasing deque. 

  to find max --> A[d.front()]
  
  and remove any indexes from the deque which are out of window.
  
  and also remove the elements which are less than the current element in the deque.


Pseudo code - 

deque deque;

for( i from 0 to N-1) {

val = A[i];

while( !deque.isEmpty() && A[deque.rear] <= val) {
deque.removeRear();


// if front is out of window then remove it also.

if( deque.isEmpty() && i-k >= deque.front()) {

deque.removeFront();
}

deque.addRear(i);


if( i >= k-1) {
print(A[deque.front()]);
}

}

do dry run on your own to understand it better.

https://www.scaler.com/academy/mentee-dashboard/class/297761/session?joinSession=1

02:18:00

above time stamp for dry run.

TC - O(N)
SC - O(N)



Q - At techTrade INC, a trading team focuses on day-trading technology stocks. They employ a strategy that involves selling stocks at short term peaks to maximize the profits. The team uses an algorithm to determine the best time to sell stocks based on minute-tominute price data.

stock prices array A.
consider the minute by minute stock prices of a tech company, TechCorp, over a 10 minute interval:

A = [ 220, 215, 230, 225, 240, 235, 230, 245, 250, 240 ]


this can be solved by using above approach.



  
--------------

HLD 1 - 

ipv4 and ipv6 - 
subnetting - 
DHCP -
intranet - 
NAT - 
network hops - 
public ip
private ip
static ip

public and static ip addresses are required for the server.

DHS cahing at the browser, router, local DNS. explain how this tree structure works.

What if the ip address changed then how it is updated at all these DNS.

---------------------------

Microservices - 

In monolith the application is built as a single unit.

Such applications comprises of client side interface, server side application and a database.

Normally a monolith application have one large code base it lack modularity.


A monolithic application is an architecture where all components of the application (frontend, backend, database, business logic, etc.) are packaged and deployed as a single unit. This is in contrast to microservices, where the application is divided into smaller, independently deployable services.

monolith layers - 

Client -> Presentation layer -> Controller layer -> Business layer -> Repository layer -> Database.

disadvantes of monollith apllication - 
difficult to manage.
can not introduce new technology easily.
a single bug in teh app can bring down the whole application down.
difficult to scale
continuous deployment is very difficult, just to update the one component we need to deploy the whole application.

what is microservice - 
While monolith application works as a single component, a microservice architecture breaks it down to independent standalone small applications, each serving one particular requirement. ex one microservice for handling all vaccination center operations, one for handling all the user base.

within this microservice architecture, the entire functionality is split in independent deployable module which communicate with each other through API's ( REST ful web services)

The API which is accessed through the internet is called as the web service.

advantages - 

All services are independant of each other. There fore testing and deployment is easy as compared to monolith application.

if there is bug in one microservice it has an ompact only on a particular service and does not affetct the entire applicatoin. Even if your vaccination center service is down , u still have your user service running to onboard the users.

its easy to build the complex apps.
It will give flexibility to choose technologies and framework for each microservices independently.

 
How to start converting the monolith to microservices - 

identify all possible standalone functionalities.
then create stand alone projects for these microservices.
use messaging or rest api to interact with each other.

we also need load balancer, eureka for service discovery, API gateways and other stuff to create the microservice architecture.


Each one of the microservice can have seperate database.

Eureka -

In microservice architecture, Eureka is primarily used as a Service Discovery Server. It is a part of the Spring Cloud Netflix ecosystem and facilitates service registration and discovery, making it easier for microservices to dynamically find and communicate with each other without needing hardcoded IP addresses or endpoints.

Key Uses of Eureka in Microservice Architecture
1. Service Registry
Eureka acts as a centralized registry where all microservices in the system register themselves.
Each service provides its name, instance ID, and address (IP/port) to Eureka upon startup.
This registry keeps track of the instances of each service and their availability.
2. Dynamic Service Discovery
Services can query Eureka to discover other services by their names.
This eliminates the need to hardcode service locations (IP addresses and ports) in the application, allowing dynamic and flexible communication.
3. Load Balancing
Eureka works seamlessly with Ribbon (a client-side load balancer) to distribute requests among multiple instances of a service.
When a service has multiple instances, Eureka provides Ribbon with a list of available instances, enabling efficient load balancing.
4. Fault Tolerance
If a service goes down, Eureka removes it from the registry after a timeout.
Eureka clients periodically refresh their registry information, ensuring they are aware of the current set of available services.
5. Decoupling Services
By using Eureka, services are loosely coupled, as they do not need to know about the exact location of other services beforehand.
This promotes scalability and easier deployment in dynamic environments like Kubernetes or cloud platforms.
6. High Availability
Eureka can be deployed in a cluster mode where multiple Eureka servers replicate data among themselves to ensure availability and fault tolerance of the registry.
How Eureka Works
Service Registration:

A microservice registers itself with the Eureka server by sending its details (e.g., name, IP, port) using the Eureka client library.
The Eureka server maintains a registry of all active services.
Heartbeat Mechanism:

Registered services periodically send heartbeats to the Eureka server to indicate they are alive.
If a service fails to send a heartbeat within a configured timeout, Eureka marks the service as unavailable and removes it from the registry.
Service Discovery:

A microservice that needs to communicate with another service queries the Eureka server to get a list of available instances of the required service.
The Eureka client caches this information locally to reduce dependency on the server.
Example
1. Eureka Server Configuration:
yaml
Copy code
# application.yml
server:
  port: 8761

eureka:
  client:
    register-with-eureka: false
    fetch-registry: false
2. Service Registration (Eureka Client):
yaml
Copy code
# application.yml
spring:
  application:
    name: service-name

eureka:
  client:
    service-url:
      defaultZone: http://localhost:8761/eureka/
3. Service Discovery:
java
Copy code
@LoadBalanced
@Bean
public RestTemplate restTemplate() {
    return new RestTemplate();
}

// Example usage
@Autowired
private RestTemplate restTemplate;

public String callAnotherService() {
    String url = "http://other-service/endpoint";
    return restTemplate.getForObject(url, String.class);
}
Benefits of Using Eureka
Dynamic Scalability: Services can be added or removed dynamically without reconfiguration.
Cloud Native: Ideal for dynamic cloud environments where IPs/ports frequently change.
Improved Fault Tolerance: Automatically removes unhealthy services from the registry.
Simplified Inter-Service Communication: Enables services to find each other easily by their logical names.
Alternatives to Eureka
Consul
Zookeeper
Kubernetes Service Discovery
While Eureka is widely used in Spring-based microservices, other tools may be preferred based on the technology stack and environment. For instance, Kubernetes has built-in service discovery and load balancing, making Eureka less necessary in Kubernetes-based setups.

Eureka operates using two primary components: the Discovery Server and the Discovery Client. Here's an explanation of each component and how they are configured in a microservice architecture.

1. Discovery Server
The Discovery Server acts as a Service Registry where all microservices register themselves and discover other services. This is the central hub for dynamic service management.

Responsibilities:
Maintains a registry of all active services.
Accepts registration requests from services.
Provides service instance details upon request by clients.
Monitors service health via periodic heartbeats.
Configuration of Discovery Server:
In a Spring Boot application, you can set up a Eureka Discovery Server as follows:

Add Dependencies: Add the Eureka server dependency in the pom.xml or build.gradle.

Maven:

xml
Copy code
<dependency>
    <groupId>org.springframework.cloud</groupId>
    <artifactId>spring-cloud-starter-netflix-eureka-server</artifactId>
</dependency>
Annotate Main Application: Use the @EnableEurekaServer annotation to make it a Eureka server.

java
Copy code
@SpringBootApplication
@EnableEurekaServer
public class EurekaServerApplication {
    public static void main(String[] args) {
        SpringApplication.run(EurekaServerApplication.class, args);
    }
}
Configure application.yml: Set up the server properties.

yaml
Copy code
server:
  port: 8761 # Port for Eureka Server

eureka:
  client:
    register-with-eureka: false  # This server won't register with itself
    fetch-registry: false        # This server won't fetch registry info
  instance:
    hostname: localhost          # Hostname of the server

spring:
  application:
    name: discovery-server       # Logical name of the server
Run the Server: Launch the application, and the Eureka dashboard will be available at http://localhost:8761.

2. Discovery Client
A Discovery Client is a microservice that interacts with the Discovery Server to:

Register itself with the server.
Discover other registered services.
Responsibilities:
Sends its details (name, IP, port) to the Discovery Server for registration.
Periodically sends heartbeats to indicate it's healthy.
Queries the Discovery Server to obtain addresses of other services.
Configuration of Discovery Client:
Add Dependencies: Include the Eureka client dependency.

Maven:

xml
Copy code
<dependency>
    <groupId>org.springframework.cloud</groupId>
    <artifactId>spring-cloud-starter-netflix-eureka-client</artifactId>
</dependency>
Annotate Main Application: Use the @EnableEurekaClient annotation or rely on auto-configuration (default for Spring Cloud).

java
Copy code
@SpringBootApplication
@EnableEurekaClient
public class ServiceApplication {
    public static void main(String[] args) {
        SpringApplication.run(ServiceApplication.class, args);
    }
}
Configure application.yml: Set up the client to register with the Discovery Server.

yaml
Copy code
server:
  port: 8081 # Port for the service

spring:
  application:
    name: service-name # Logical name for the service

eureka:
  client:
    service-url:
      defaultZone: http://localhost:8761/eureka/ # Discovery server URL
Access Other Services: Use a logical service name to access other services registered in Eureka.

java
Copy code
@RestController
public class TestController {

    @Autowired
    private RestTemplate restTemplate;

    @GetMapping("/call-other-service")
    public String callOtherService() {
        return restTemplate.getForObject("http://other-service/api", String.class);
    }
}

@Bean
@LoadBalanced  // Enables load balancing using Eureka
public RestTemplate restTemplate() {
    return new RestTemplate();
}
How It Works Together
Service Registration:

Each client registers itself with the Discovery Server upon startup.
For example, a service named payment-service registers its IP and port in Eureka.
Heartbeat Mechanism:

Clients send heartbeats periodically to indicate they are alive.
If a client stops sending heartbeats, the Discovery Server removes it from the registry after a timeout.
Service Discovery:

A service (e.g., order-service) queries Eureka for the address of another service (e.g., payment-service).
Eureka responds with a list of available instances, and the client can choose one (usually via load balancing).
Configuration Summary
Discovery Server
Provides a central registry for all services.
Does not register itself or fetch a registry.
Discovery Client
Registers itself with the Discovery Server.
Queries the server for addresses of other services.
Additional Features
High Availability: Eureka servers can be set up in a cluster for redundancy.
Ribbon Integration: Clients can use Ribbon for client-side load balancing.
Self-Preservation Mode: If too many clients go offline abruptly, Eureka assumes a network partition and retains the registry state to avoid disruption.


Eureka server can also acts as a eureka client so we need to configure the behaviour on eureka server.



--------------------------



Searching 1 - Binary Search on Array


mid = (l+r)/2.

if the value of l and r is 10^9 then it will overflow.

use below to avoid the overflow -

mid = l + ( (r-l) / 2 );


Q - Given a sorted arr[N]. find the first occurrence of k.

there are duplicate elements in an array.

l=0;
r=n-1;

ans = -1;

while( l <= r) {

mid = (l+r)/2;

if( A[mid] == k){

ans = mid;
right = mid -1; // go left
}
else if( A[mid] < k) {
left = mid +1;
}
else {
right = mid -1;
}

}

return ans;

Q - find the last occurrence of the k in an array.

solve


Q - Every element occus twice except for 1. find that unique element. 

Duplicates are adjacent to each other and array isn't necessary sorted.

solution - 

BF - 

XOR all  - TC - O(n), SC - O(1)

optimized - 

optimizing beyond O(n) means we will have to think in terms of the binary search.

arr = [8, 8, 5, 5, 9, 9, 6, 2, 2, 4, 4]

observation - 

First occurence of the element is present at even index before unique element.

First occurence of the element is present at odd index after unique element.

dry run -

l =0
r = 10
mid =5
first occurrence (fo)= 4
fo %2 == 0 -> yes then our unique element will be on right so, go right

l =6
r = 10
mid =8
first occurrence (fo)= 7
fo %2 == 0 -> no then our unique element will be on left so, go left.

l =
r =7
mid =6
first occurrence (fo)= 6

AS we can see there is no element which is equal to the element at index 6, on right and left side of the index 6.

so from this we can say that element at index 6 is is our ans.

code - 

l =0
r = N-1

while( l <= r) {

mid = (l+r)/2;


if( A[mid] != A[mid-1] && A[mid] != A[mid+1] ) {

return A[mid];
}

fo = mid;

if( A[mid] == A[mid -1]){
	fo = mid -1;
	}
	
if( fo % 2 == 0){
// even - go right 
	l=mid+1;
	}
	else {
	r = mid -1;
   }
}

return -1;


handling array index out of bounds exception - 

just return some fixed and decided value when we are getting the index which is out of bounds.

below function will solve the issue.

int safeGet( A[], index) {

int N = A.length;

if( 0 <= index && index < N) {
return A[index];
}

return some_value_which_you_think_will_work;

return Integer.MAX_VALUE; // or MIN_VALUE

}


replace all the A[mid] calls with the safeGet( A, mid) method calls, and make sure to return correct value in case of index is out of bounds from safeGet.

TC - O(logn)
SC O(1)

To apply binary search - 

identify the search space.
Can mid be potentially our ans.
identify which half to discard.

Q - Given an increasing-descreasing array with distinct elements. Find the max element.

 arr = [1, 2, 3, 6, 4, 3, 2, 1]
 
 BF - find the max of the array itself.
 
 TC - O(n)
 SC - O(1)
 
 optimization - 
 
 the max element value will be greater than its both neighbours.
 
 if ( A[mid-1] < A[mid] < A[mid+1] ) then go to the right side for finding the max element.
 
 if( A[mid -1] > A[mid] > A[mid+1] )  then go to the left side for finding the max element.
 
 
 Edge cases - when max element is the last elmeent in the array or when first element is the max element in the array.
 
 
 code - 
 
 l=0;
 r = N-1;
 
 while( l <= r) {
 
 mid = (l+r)>>1;
 
 
 if(safeGet(A, mid) > safeGet(A, mid -1) && safeGet( A, mid) > safeGet( A, mid +1) {
 
 return A[mid];
 
 }
 
 
 if(safeGet(A, mid-1) < safeGet(A, mid) && safeGet(A, mid) < safeGet(A, mid+1)){
 
 l = mid+1;
 }
 else {
 r = mid -1;
 }
 
 }
 
 
 return -1;
 
 public int safeGet(int[] A, int mid){
 
 if( mid < 0 || mid >= A.length )
 	return Integer.MIN_VALUE;
 
 return A[mid];
 }
 
 in this we are finding the max element so we are returning Integer.MIN_VALUE from the safeget function.
 
 TC - O(log n)
 SC - O(1)
 
 
 Q - Given arr[N], find any one local minima.
 
 local minima - An element which is less than or equal to it's adjacent elements.
 
 arr = 3, 6, 1, 0, 9, 15, 8
 
 here local minimas are 3, 0, 8
 
 we can return any one of the above values.
 
 BF - find min of the array - 
 
 TC - O(N)
 SC - O(1)
 
 idea - 
 
 if ( A[mid-1] > A[mid] < A[mid+1] ) then we got the ans.
 
  if( A[mid-1] > A[mid] > A[mid-1]) 
  then go right
  
  if( A[mid-1] < A[mid] <  A[mid+1] ) then go left
  
  if( A[mid-1] < A[mid] < A[mid+1])
  then go anywhere.
  
  code -

	l=0; r = n-1;
	
	while( l <= r ) {
	mid = (l+r) >> 2;
	
if( safeGet(A, mid) < safeGet(A, mid-1) && safeGet(A, mid) < safeGet(A, mid+1) )
  return safeGet(A, mid);   


if( safeGet(A, mid) > safeGet(A, mid-1) && safeGet(A, mid) < safeGet(A, mid+1) )
	r = mid -1;   

else {

	l = mid +1;   
}




  
  public int safeGet( int[] A, int mid){
  
  if(mid < 0 || mid >= A.length) {
  return Integer.MAX_VALUE;
  }
  
  return A[mid];
  }
  
  in above safeGet we want to find the minimum element so we are returning MAX_VALUE.
  
  
  
  

Searching 2 : Binary Search Problems - 

 


Focus on identifing the  pattern.

problem solving framework - 

start the 25 minutes timer.

read the problem carefully.

understand the probblem and identify the inputs and outputs.
Think of a brute force approach to solve the problem.

optimize the brute force approach to reduce time complexity.

in this if you were wble to solve the problem in 25 minuts then mpve tp next question.

if could not solve the question then bookmark that question for revision on sunday.

and take hints progressively and if u could able to solve the question then ask yourself what concept if i knew would have helped me solve this question on my own without any help and note it down and move on to the next question.

if you could not able to solve the question with the help of hints then watch video solution or read the blog solving this question.

and solve the question and make sure to revise the question on sunday.


Q - Search an element in sorted and rotated array.

 [8, 10, 15, 2, 4]
 
 target = 4
 
 
 BF - linear search - 
 
 TC - O(n)
 SC - O(1)
 
 optimization -
  
 idea - 
 in this some part of the array will be in ascending order and some part of the array in descending order.
 
Binary search - 

 in this the ans can be on the right and can be on left side of the mid element.
 
  if the target element is present in left area then go to left area.
  
  if the target element is present in right area then go to the right area.
  
  if the target is present in the current area then do the binary search.
  
  
  int getArea( int val, A[]){
  
  if( val >= A[0]  return 1;
  
  return 2;
  }
  
  l=0
  r=n-1

targetArea = getArea(k, A);

  
  while( l <= r) {

  mid = (l+r) >> 1;
  
midArea = getAread(A[mid]);

if( A[mid] == k) return mid;

// case 1 

if( midArea 	 == 1 && targetArea == 2) {
l = mid+1;
}

// case 2 

else if( midArea == 2 && targetArea == 1) {
r = mid-1;
}else {

// normal binary search

if( A[mid] < k ) {
l = mid+1;
}
else {
	r = mid -1;
}
}

// when A is sorted and not rotated then we will always end up in case 3.

 TC - O(logn)
 SC - O(1)
 
 Q - find the floor of sqrt(N)
    
 N = 10, ans = 3
 N = 16, ans = 4
 N = 29 ans = 5
 
 
 idead  - math.sqrt()
 
 TC - O(log(n))
 SC - O(1)
 
 idea - 
 
 for this the search space will be from 1 to n.
 
 ans =0;
 
 for( int i =1; i <= n ; i++){
 
 if( i * i <= n){
 ans = i;
 }
 }
 
 print ans;   


idea 3 -> 

for( int i =1; i*i <= n; i++){

if( i*i <= n) ans =i;
}

TC - O(sqrt(n))
SC - O(1)

idea 4 ->

Binary search on the ans -

l = 1 ; r= n;
ans = 0;

 while( l <= r) {
 
 mid = (l+r) >> 1;
 
 if( mid * mid <= n) {
 // go right 
	ans = mid; // store ans 
	l = mid+1;
	} else {
	r = mid -1;
  }
}

print ans;

TC - O(log n)
SC-O(1)


------------------------------------------


Typing cerficate
AI Certificate
Excel certificate

Outside country certifications


About - skills, 8 to 10 lines, looking ofr job outside 

make connections outside india
IELTS



-------------------------------------


CAP Theorem & Master Slave 


Shards which are mutually exclusive + collectively exhaustive.

Above statement means each shards will store the different data sets and  if we combine all the shards then we will get the data of the entire database.

 
 Consistent hashing is used to store and retrieve the data in shards.
 
 
Slaves are also called as read replicas.

data is written in the master and data is read from the slave in master and slave architecture.



in case of the asynchronous updates in the master and slave - 
While promoting the slave to the master in case of master failure the timestamp of the latest data updation on the master and time stamp of the latest data updation in the slaves is checked, if it mateches or closer then that slave will be promoted as a master.


Leader election algorithms for master and slave architecture.

Financial application - Immediate consistency is required.
Facebook, Instagram, Youtube - It requires the eventual consistency.


 CAP - Consistency, Availability, Partition tolerence.
 
Consistency - Every time when we make a read request, ideally we should get the latest data or information.

Availibility - if we send a query, the system must be available to send the response back.

Partition Tolerance -  

A system is called as partition tolerant if any event of network partition can be handled by the system.

In single machine there is no need of the Partition tolerence then in that case We will try to achieve the Consistency and Availability.

In financial applications we need to achieve the consistency and availability.


In Distributed system partition
tolerence will always will be there, so in this case we will try to achieve the consistency or availability with partition tolerence.


----------------------------

System Design - FB news feed 

in news feed we get the posts which are only few days older.

but in profile we get the all the posts of the user irrespective of the timeline.

Users data - 

id
name
email
password
phone no
location
relationship status


User_Friends - 

user_id
friend_id

Posts - 
id
user_id
text
timestamp
img_vdo_path

shard the users data based on the user id.

data will be stored in the stateful database.

and the friends list will also be stored on the same database in which user data is stored. in this the friends list of the user is stored on the same database in which the user data is stored which is stateful.

and we will shard the posts data based on user_id, by doing this the data will be fetched from the same table wihich increases the performance of the query or endpoint.


so all the data we will be sharding based on the user id.

we will need the load balancer with the consistent hashing.

In this the app servers are also stateful, so before the app servers we need the API gateway and load balancer and consistent hashing algorithm.

and before the database we need the loadbalancer with consistent hashing.

so in this architecture when we fetch the posts of one user, we will fetch the data from the one database server in paginated format, and when user fetches first 20 posts then it will fetch the next 20 posts and store them in the application server to cache the next 20 posts, so it will reduce the latency to fetch next 20 posts.

Newsfeed - 

In this the posts of all friends are shown in decreasing order of the timestamp.

in this first frieds of the user are taken then all the recent posts of the friends are are taken and then this list is sorted in decreasing order of the timestamp, so that we will get the latest posts in the top.

But in above approach is not optimised and it can also lead to network congestion.

if one database is receiving the lot of requests for the data then it will go down, in above case this will happen.

So to solve this problem, we can store the friends list of the  user in the app server cache and then store the posts created by all those friends in the last 24 hours in the global cache.

TTL for this global cache will be 24 hours.

Scale Estimations - 


Guess + Estinmation = Guestimations.

Total account on facebook - 2 billion

we can assume that the 25% of the people are the Daily Active Users.

 DAU - 500 million
 
 out of these only 5% of the people posts on the facebook.
 
 so this count will be 25 million.
 
 we can take an average each user makes 4 posts a day.
 
 25 * 4 = 100 million posts.
 
 Content size will be - 
 
 Text - 500 char = 500 bytes
 media -
 
 Meta data -
 post id= 8 bytes
 user id = 8 bytes
 Location = 16 bytes
 tag = 
 url = of media = 100 bytes
 like these take average of 200 bytes.
 
 so total size of one post will be 700 bytes.
 
 so to store the 100 million posts facebook will need 100 * 700 = 70 GB of space.
 
 so facebook will need 70 GB per day memory space and cache space per day to store the posts data, which is not a big deal for facebook.
 
 to cache the posts of last 30 days it will need the 30 * 70 = 2100 GB space in cache.
 
 
------------------------------

System Design - SQL vs NoSQL.

Availability - means the feature will not be available for some time, not the entire application.

PACELC - This an extension of CAP theorem.


 To achieve the strong consistency then latency will be high.
 
 whatsapp chooses the consistency.
 
 The PACELC theorem is an extension of the CAP theorem that adds more nuance to understanding the trade-offs in distributed systems. Introduced by Daniel J. Abadi, it addresses a limitation of the CAP theorem: CAP focuses only on what happens during a network partition, but it does not account for trade-offs that occur when the system is running normally, i.e., without partitions.

PACELC stands for:

P: Partition Tolerance
A: Availability
C: Consistency
E: Else
L: Latency
C: Consistency
PACELC Explained
Partition Scenario (PAC):

During a network partition, a distributed system must choose between:
Consistency (C): Ensuring all nodes have the same up-to-date data.
Availability (A): Ensuring the system can respond to all requests.
Normal Scenario (ELC):

When there is no partition, the system must choose between:
Latency (L): Responding quickly to requests.
Consistency (C): Ensuring all nodes provide the most recent data.
In essence:

CAP focuses on the trade-off during network partitions.
PACELC expands this to include the trade-off between latency and consistency in the absence of partitions.
Key Scenarios in PACELC
Partitioned State (PAC):

The system behaves like the CAP theorem: it must choose between availability and consistency.
Non-Partitioned State (ELC):

The system decides whether to prioritize:
Low latency: Responding quickly to user requests, possibly with stale data.
Strong consistency: Ensuring all nodes return the most recent data, even if it delays responses.
Examples of PACELC in Practice
AP/EL Systems (Availability + Low Latency):
During partitions: The system prioritizes availability over consistency.
Without partitions: The system prioritizes low latency over strong consistency.
Examples: DynamoDB, Cassandra (eventual consistency models).
CP/EC Systems (Consistency + Strong Consistency):
During partitions: The system prioritizes consistency over availability.
Without partitions: The system prioritizes strong consistency over latency.
Examples: Google Spanner, HBase.
AP/EC Systems (Availability + Strong Consistency):
During partitions: The system prioritizes availability over consistency.
Without partitions: The system prioritizes strong consistency over latency.
Examples: MongoDB (configured for eventual consistency with strong consistency in normal operations).
CP/EL Systems (Consistency + Low Latency):
During partitions: The system prioritizes consistency over availability.
Without partitions: The system prioritizes low latency over strong consistency.
Examples: Systems that aim for fast responses but can sacrifice strong consistency when possible.
Differences Between CAP and PACELC
Aspect	CAP Theorem	PACELC Theorem
Scope	Focuses on network partitions only.	Considers both partitioned and non-partitioned states.
Key Trade-Offs	Consistency vs. Availability.	Consistency vs. Availability during partitions; Latency vs. Consistency otherwise.
Insight Provided	Highlights limitations of distributed systems during partitions.	Offers a broader perspective, including trade-offs under normal conditions.
PACELC Trade-Offs Table
Scenario	Partitioned State (PAC)	Non-Partitioned State (ELC)
AP/EL Systems	Prioritize availability	Prioritize low latency.
CP/EC Systems	Prioritize consistency	Prioritize strong consistency.
AP/EC Systems	Prioritize availability	Prioritize strong consistency.
CP/EL Systems	Prioritize consistency	Prioritize low latency.
How to Use PACELC in System Design
Choosing AP/EL (Availability + Low Latency):
When to Use:
Use in applications where uptime and responsiveness are more critical than data accuracy.
Examples: Social media, messaging apps.
Choosing CP/EC (Consistency + Strong Consistency):
When to Use:
Use in applications where accuracy is crucial, such as financial systems or transaction-based systems.
Examples: Banking systems, distributed ledgers.
Choosing AP/EC (Availability + Strong Consistency):
When to Use:
Use in systems where availability is important, but consistency needs to be guaranteed eventually.
Examples: E-commerce platforms with eventual order confirmation.
Choosing CP/EL (Consistency + Low Latency):
When to Use:
Use in applications where fast responses and consistent views are required.
Examples: Cache-coordinated databases.
Conclusion
The PACELC theorem extends the CAP theorem by acknowledging the trade-offs that distributed systems face both during partitions and normal operations. It provides a richer framework for understanding and designing distributed systems by balancing partition tolerance, availability, latency, and consistency based on the specific needs of an application.


SQL databases - are the databases which supports the Structured query language.

in this data is organised in the tables.
Table is the collections of the rows or tuples.

RDBMS.

in this there are relations between the tables, which has cardinality for these relations, for which there will be the mapping tables in needed.

Properties of the SQL database - 

Normalization - no data redundancy.

redundancy can lead to in-consistency in data.

Redudnacy - storing same data in in different tables again and again.

ACID - This is the guarantees provided by SQL databases.

Atomicity - In case of transaction failure, roll back the transaction. Each transaction is automic in nature.

Consistency - All the validations or constraints of the data must be fullfill, means data types format will stay consistent.


Isolation - Transaction should be isolated.

Durability - No data loss from the database. 

SQL - 
RDBMS
data is stored in the tables.
follows ACID properties.

SQL databases were designed on the a basic assumption of single machine support, not distributed.

In distributed database environment we can not guarantee ACID properties.

The balance data of the banks is stored on the one single machine to follow the ACID properties and achieve the consistency.

SQL databases are very popular for fixed schema.


SQL vs NoSQL databases - 

SQL - Imagined to be vertically scalled ( We can still horizontally scale SQL databases but ACID properties might get compromise in that case).
It follows ACID.

Fixed Schema.
Follows normalization - normalization can impact the read performance not write performance.


NoSQL - these are imagined to be horizontally scaled. This supports sharding out of the box.
Sharding keys should be defined to make sure that the most frequently accessed query becomes intra shard not inter shard.

It follows BASE.
BASE - Basically Available & Eventually Consistent - This is usually followed, sometimes it also offer  more availability and eventual consistency, we can also have system which is high consistent.

Schema is not fixed.
Follows Denormalization - copy the data somewhere to optimize the inter shard query.

 
 
 SQL databases were imagined to be single machine or vertically scaled systems to provide strong ACID gurantees & idea of normalization.
 
 but if our data is distributed across multiple machinese then some of our queries might become inter-shard queries, to optimise these queries we might have to add redundant data multiple machines ( de-normalization).
 
 
even if we do sharding to improve the performance of the query, still there will be some queries which will be inter shard, so to improve these queries we can pre process the data and store it in the cache.

 RAM are 10 to 20 times faster than HDD.
 
 
 
NoSQL - 

Key value pair database - 
REDIS, Memcached, AWS dynabo db.

cache = Radis + RAM
database = Radis + HDD.

In this there is no fixed schema.
We can store any types of key and any type of value in this type of database.

these are sharded on the kay.

Generally these are used for caching.


Document database -
Mongo DB, Couch DB, Elastic search.

It is enhanced version of the key value dadtabase.
This has document id, whcih stores the XML or json values.

in this the queries are optimized by indexing the values of the document fields like category, price, or name in case of product category.

Document based databases are intelligent version of key value pair database. It allows us to create indexes on the selected fields of the document to opimise certain type of queries.


Column family database - 



Graph database -


CPU can not read the data from the HDD, because of the speed difference they have. so OS will fetch the data from the HDD and then push it to the RAM and then CPU will access this data from the RAM at very high speed.

SQL - Structured data, fixed schema

Data is stored in block like starting from the first row then next to this second row will be stored and like that all the rows are stored in memory.

and when we try to access the data from the database, first it will fetch entire block data and then read whatever is required and then discard the remaining data.

while doing indexing 

NoSQL - unstructured / semi structured data, No fixed schema.

NoSQL database does not store the data in the sequential manner.

Every NoSQL uses the Write ahead log to store the data.

Write ahead log - it is a append only file and it is stored on the disk. Mainly we will perform read and write operations on the disk.

whenever we will get the write or update request we will just append the data in the WAL file.

so write operation TC will be constant.

when we delete the value then that value will be replaced by undefined value in the WAL.

and to read - as data is stored on the HDD and we can not access the data on this in bottom to top approach we have to access it in the forward direction.

so the read will be O(n) operation in NoSQL.

we need to optimize this. 

in SQL data is stored in the Balanced binary search tree, so the Read and write TC will be log(n).

so to store the data in NoSQL - 

approach 1) If we only used WAL then TC for read - O(n) and write - O(1).

Approach 2) We can maintain WAL file in HDD and use hashmap stored in the RAM. in this hashmap we will maintain the key and HDD address of the value. 

So TC will be Read - Get the address from the HashMap and Read from the address in WAL - O(1) and write - append WAL and update hashmap - O(1)

but we might not be able to store this huge data inside the map inside the RAM and we will have to reinitilzee the hashmap in case of the machine gets restarted.

for binary search we need sorted as well as equal size data, so we can not use that here.

Approach 4) WAL + Tree  map 

Tc wil be write - log(n), Read - log(n) 
and also the space complexity is increased.

write the background script which will run after every one hour, take the complete data from TreeMap, create a file and reset the Treemap

and start filling the new data values into the reseted tree map.

 and we will store the actual values in the tree map instead of the address of the values. 
 
the old data will be available in the file which  was created by the script. 

each time script runs it will create new file to store the Treemap data.

in this 

write - Write in WAL file O(1), Add in TreeMap log(n).

Read - first try check if data is present in the treemap, then if not present then read the data from files created by script from newest file to the oldest file.

These files will also contain the sorted data but the data size is not uniform, so we can not use the binary search on this.

TC - O(log n) ( to read from the traamap) + no_of_files * N ( no of entries in the file).

as no of files will be kind of constant.

in below 24 is just an example considering script runs after each 1 hour.

TC - 24 * N ( in worst case).

in the worst case scenario we might need to read the data from the WAL file if not found in the files.

 
after certain time period all the files created by the script will be merged in one file.

For this there will be one more background script to merge the all the files into one file.


in this TC will be - 

Write -
Write in WAL.
Write in TreeMap.

O(log(n))

Read - 
Read treemap. - log(n)
iterate all the Files. - O(n) 

The overall TC will be - n.

Note - Somehow if we are able to search in o(log(n)) TC within every file.

but the data is sorted but it is unstructured, we can not use binary search directly.

we can divide the data in the file into logical blocks of equal size like 64 or 128.

Along with each file we also maintain a metadata which contains the address of first element of each block. 

now blocks are of equal size and sorted we can appply binary search on all these blocks.

and inside the block apply linear search and find the element we want.

in this approach TC will be - 

Write - log(n)
Read - 
Tree Map - O(log(n))
Iterate each file using binary search - O(log(n))


And while merging the files, if there is duplicate data in two files then we will take only the data from latest file and skip the data from the old file.

And merging the files will happen in the background that's why it is not considered in TC calculation.

The name of this approach is LSM trees.

Extract notes from below document - 

https://docs.google.com/document/d/1qVUOgfGeFeB8Xh8Zay-NbCjgDUjPVlYaN7_y8S2BTtE/edit?tab=t.0


----------------------




Searching 3 : Binary Search on Answer -

Q. Painters partition problem - 1 

Given N boards with length of each board
1) A painter takes 1 unit of time to paint 1 unit of length.
2) A board can only be painted by 1 painter.
3) A painter can only paint boards placed next to each other ( i.e. continuous segment).

Find min no of painters required to paint all the boards in T unit of time. If its not possible return infinity.





Q. Painters partition problem - 2 

Given N boards with length of each board
1) A painter takes 1 unit of time to paint 1 unit of length.
2) A board can only be painted by 1 painter.
3) A painter can only paint boards placed next to each other ( i.e. continuous segment).

Find minimum time to paint all boards if  painters are available.


Q. Email Response Handler - 

Situation - Imagine you are tasked with developing a system for evenly distributinh the workload among the team of email response handlers in a customer service department. Each mail is assigned a complexity score which represents the estimated time and effort required to address it. The complexity scores are represented as an array, where each element corresponds to a single email.

Task -

The goal is to divide the array into k contiguous blocks ( where K is the number of email handlers). Such that the maximum sum of the complexity scores in any block is minimized. This approach aims to ensure that no single email handler is over whelmed with high complexity emails while others have a lighter load.



Q. Farmer has built a bar with N stalls.
A[i] - location of ith stall in sorted order.
M - Number of cows the farmer has

Cows are aggressive towards eath other. So, farmer wants to maximise the minimum distance between any pair of cows.

Find max possible minimum distance.


--------------------------------------


 
 -----------------------
 
 System Design - case study 2 Typeahead 
 
 in this the suggestions starting from the prefix are fetched from the server and shown to the user.
 
 
 4 steps to design any HLD system - 
 
 1) MVP - minimum  viable product - suggest the most important features and leave the add on features.
 
 2) Scale estimation ( Back of the Envelop calculation ) -  
 	
 in this we will try to identify the no of users, queries per second, storage requirements, is sharding required, no of read queries and write queries ( Read heavy or write heavy or both heavy) 
 
 
 if no of write are atleast more  than 50 to 100 times of no of write operations then it can be considered as write heavy system.
 
 3) Design Tradeofffs-  
 In this we have to decide High Consistency and High Availability, leatency, SQL , no SQL, Stateful, stateless, caching or not, global cache or local cache etc.  we need to make sure for these things.
 
 4) Design Deep dive - 
 
 decide apis ( 3 to 4 main apis), HLD ( Diagram) , and data flow and draw the diagram
 
 
 Typeahead - 
 
 This sytem is the system which suggests the next words based on the characters we have written in the search bar.
 
 MVP -   
 
 Search prefix and then get the suggestions.
 Maximum 10 suggestions should be returned.
 Minimum three characters are required to get the suggestions.
 
 Relevant suggestions - based on the recent searches.
 
 Scale Estimation , Back of the Envelop calculation - 
 
  No of users - 3 billion
  
  Daily active user - These are the 5 to 10 % of the total users.
  
  but google is the essential so for this we will consider 20 % of the total no of users.
  
Average no of searches per  user per day -  20 (Average)
  
  No of search queries per day  = Average no of searches per day per user * DAU = 20 * 1 Billion = 20 billion.
  
  
  So when we type the keywords in the search then we wiil be reading the data from the server, byut when i actually search the keyword then system has to update the searching count of that word in the system then it will become the write query.
  
  so no of write queries per day will be = 20 Billion.
  
  write queries per second (QPS) = total_on_of_queries_per_day / total_no_of_seconds = 
  
  20 Billion / 86400 
  
  to make this calculation simple lests assume there are 1 lacks of seconds.
  
  20 * 10^9 / 10^5 = 2 8* 10^5 = 200k qps.
  
Read queries per second - 

average no of getSuggestoins call per search query as we will keep typing the word = 5.

 so the total no of read queries per second = 5 * 200k = 1 million
 

The difference between the read and writes in the system should be more than 50 to 100 times, to consider it that heavy system.

in this system there is not much difference between read and write operation counts so this is the both heavy means read and write heavy system.


Now peak qps = 2 * Average qps.

two times of the average qps. 

Storage Requirement - 

Write qps = 200k
Read qps = 1 million

while write operation we will be increamenting the frequency of the word.


Few words we will search will be already there is the system and few new words will not be there in the system.

if a word is already present in the system then we just go and update the count of the words, if word is not present in the system then we will hage to add that word in the system with count 1.

and we will store these values in the form of key and value. 

<Search_String_Query, Long>

long will take 8 Bytes space.
and the average space taken by the search query will be 70 to 80 Bytes.

so to store the data of one word  we will need 100 Bytes of the space.

on google there 10 to 15 % of total no of queries will be new or searched for the first time on the google every day.

New queries per day = 10 % * 20 Billion =  2  Billion

so per day we will need 2 Billion * 100 Bytes new space which will be equal to 200GB space per day.

so for entire year = 200 GB * 365 = 80 TB of the data.

so for 10 years we will need the 0.8 peta bytes of the space.

for one year space 80 TB, we will need  the multiple machines to store this data and this can be implemented using the consistent hashing.


Design trade offs -


For this system we need the High availablility and eventual consistency and ultra low latency.


Desing deep dive - 

APIS - GetSuggestions ( prefix, limit), updateFrequency( query)


In this we will need to store the data in the trie as we will be checking the prefix of the word and then decide which word or query to return.

so for this case trie is the best data structure.

In trie data is stored in the format of the tree and in each node we will store one character of the query.

for "Microsoft" - in first node store M and then its child node store i and then child node of i store c and then in the child node of the c store r like this we will store the entire query.

But to get the top 5 words from this trie will be huge operation, so we can store a map on each node in which we will store the top five words whcih are starting f
rom that prefix.


in this modification we do not have to iterate the entire trie to get the top 5 words.

but this also will lead to the massive no of maps and will increase the space complexity.


Instead of the trie we can store the words in the map but there also we will store the repetative characters in the map which will lead to the space wastage.

in map we will store the search query and frequency of that query.

in this write query will become o(1)

but read queries will become O(n) which is huge considering the data google receives.

to optimize the query we can take the top five words of the prefix and store it in the seperate hashmap <prefix, top_five_wrods_with_count>  on the cache like redis and update this map whenever there is an update in the frequency of the query.

Trie was hard to shard but redis supports shardinng out of the box.


 Both the maps we will store in the redis database.
 
 Write - go to hashmap 1 check if the query word is present in the already present, if present update the frequency, if no insert the query with frequency 1 in hashmap 1.
 
and then 

for example we are searching a word "Microsoft", 

then we will fetch all the prefix words for below and check the count of frequency of each word and then update if the word in all those prefix location in hashmap 2 where the frequency of the current wiord is greater than the word in the hashmap 2.

Mic
Micr 
Micro
Micros
Microso
Microsof
Microsoft


so we will get  the prefix values of all above and then update the words in hashmap 2 if its frequency is less than the words whose frequency we have updated in the hash map 1.

In whise we are okay with the eventual consistency so, we can update this words count in the batches instead of writing every single write operation in the database, Till that point we will keep the records updated in the global cache and update it in the database after every time interval.

Read - in this we will read the data from the hashmap 2 in constant time.

Time Decay - The popularity of the keywords will keep on changing and it will be removed from the suggestions. This concept is called as Time Decay.
This decay rate is dynamic depend on the culture and other billion things.

 ----------------------------------------
 
 
 To improve the abosrpptions of the vitamins and minerals -
 
 Eat raw vegatables before lunch to increase vitamins absorptions.
 
 Orange juice, lemon on the food, Jaggery ( low quantity), 
 
 
 To improve the absorption of the Vitamin D - take all  below recommendations with the high fat s like ghee, olive oil, Sunflower oil, Coconut oil, Dark chocolate, Seeds.
 
 also consider taking supplements with warm milk as milk has fats.
also take the supplement in the morning as it affects the melatonin ( sleeps harmone).

To improve the absorption of B12 - 

  
 
 D - Milk ( + milk fortified with vitamin D) , cheese, curd, Mushrooms exposed to sunlight( keep in sunlight for 30 mins and then do not cook on hard flame), Cesame seed ( Black or brown), white soya bean ( soya milk fortified with vitamin D,  Walnut, Corn, Ragi, Bel ( bel che zadache fal), Amaranth leaves,    
 and most important - for dark skin 60 mins sunlight.
 
 B12 - Banana, Garlic, Milk, cheese, Curd, Fermented foods like Idli, dosa, pickles. 
 
 
 --------------------------------
 
 Skin health - Eat non processed and minimal processed foods like vegetables, Grains, rice oat meals, pasta, cheese, butter, olive oil etc.low imflamatory diet
 
 Follow dr, Andrea Suarez for this matter.
 
 Eat oranges, starwberries, vitamin A from oranges, sweet potatos, Berry, Avoid sugar food, ea fibreous food like fruit , vegetables, Grains, low sugar fermented foods.
 
 Make sure to have a body which is less imflamatory. 
 
 Sleep better, do not drink alcohol, Drink plenty of water and drink electrolytes, Meditation, Non Sleep deep rest.
 
 Reduce imflemation in the body.
 
   
----------------------------------

Diet - 


There are three types of carbohydrates - Sugar, Fiber, resistent starch.

When diettician talks about the low carbohydrate diet it means reduce the fucking sugar not Fiber and starch.

peoples who have healthy body are eating the high fiber and starch.

Fiber is only found in plants.
Eat more fiber. Eat 50 Grams fiber per day ( 25 grams is the scientific recommendation).

To include a fiber in diet - eat a fruit before your meal. 

To eat the 50 grams of fiber we need to eat whole grains ( ex- millets) and pulses meal.

the outer cover of the millets or whole grains has the tendency to absorb the flavours and make the dish less testy for this we can add more spices ( not recommended).

When we eat whole grain, lentils, pulses and food which is closer to the nature we will not gain weight (Fat).

Do not avoid grains and pulses.

Do not eat ultra processed food, This food increases the fat in the body.

Eat three servings of Whole grains, lentils per day - in each serving we will get 6 to 7 grams of the fiber.

One serving should contain 1 to 2 chapati.

1 Gram of carbohydrate = 4 calories.

So in one gram of carbohydrate will have below amount of sugar  -

Pure Sugar - 1 gm carbohydrate = 	1 gram of sugar.
Fruits and diary products - 

Banana - 1 gm of carbohydrate = 0.5 gm of sugar.
Milk - 1 gm of carbohydrate = 0.5 to 0.8 gm of sugar.

Whole grains or vegetables -  1 gm of carbohydrate = 0.1 grams of sugar.

Excess proteins and excess carbs are converted to the Fat, but fiber can not be converted to the fat so if we each the whole grains and lentils we will not gain fat as much as we will eat processed food which contains the sugar.

To get the resistance starch - root vegetables, pulses and lentils, beat root, carrot, sweet potato, cheak peas and kidney beans, masur dals and tur dals.

Jaggery is also the table sugar just avoid it.

eat at least food from 5 different plants per day like bajra, wheat, seeds, lentils, pulses from different plants.

Roshni sanghvi suggests 10 plant points per day.

26:26 - https://www.youtube.com/watch?v=hI2TZW9H5iA
      
 
-------------------------------------


System Design : Case study - Messanger -

  Facebook messanger - 
  
  in this user sends message to the server then server stores this message in the database and then it sends that message to the recevier of the message.
  
  Steps to build HLD - 
  
  Decide the MVP.
  Back of the envelop calculation - scale estimation.
  Trade offs - consistency vs Availability, latency.
API design - 
HLD Deep Dive -



MVP of Facebook messanger -  

suggest 5 features to the interviewer.

Send or receive a message. Message can be image, file, audio, video, text.

Messages should be delivered in almost real time - Ultra low latency.

( as signup and login is not the core functionality of the messanger we will skip it, it is core functionality of the authentication service).

Message history.

Multiple conversations - one user chatting to the multiple people.

one to one conversation and Group conversation.

Delete Message.



Scale estimation -

No of users on facebook - 3 Billion.

Daily active users - DAU - 20% of total users = 1 Billion ( approax - easy for calculation )

Average messages per user per day - 20.

Total messages per day = 20 * 1 Billion = 20 Billion messages.


Read and write QPS - as write and read queries will be almost same ( difference only 5 to 10 times) so this system is both read and write system.


Write QPS -

 20 Billion / 86400 Seconds = 20 * 10^4 = 200K QPS.

Read QPS - it is almost same as write QPS.

Peak QPS - this will be 2 to 4 times the average QPS.

Storage Calculations - 


20 Billion messages per day.

Message will have 
message id - 8 Bytes
Time stamp - 8 Bytes
Sender id - 8 Bytes
Conversation id ( receiver) - 8 Bytes
content - content can be text, audio, video, image, file. - 100 Bytes 

metadata - 50 Bytes

so total size will be 182 Bytes. 

200 Bytes per message approximately.

so daily we will need 20 Billion * 200 Bytes space = 4 TB of data per day. 


for 10 years = 4 TB * 365 *10 = 16 Peta Bytes of data.

in one machine we can store 4 to 8 TB of data.


Tradeoffs - 
	
	this system should be consistent than available.
	
	So our system will be high consistent and eventual available and ultra low latency. 


APIs - 

sendMessage(sender_id, conversation_id, message, message_id)

getConversations( user_id, limit, offset) - limit and offset for pagination.

getMessages( conversation_id, user_id) - This api will be paginated.

Polling - fetching messsages from the server continuously so that we will receive the messages.

in this receiver will make a call to the server and check if there is any new message.

Client keeps on making a call to the backend to check if there is any new message for them.

This leads to lot of calls to the server and this can also leads to the network congestion.

Long polling - Client or receiver will not make a call every second


Pull model - in this messages are pulled from the server by sending the periodic requests.

Push Model - In this message are pushed by the server when there is new message.


Polling and Long Polling in Messaging Applications
Polling and long polling are two common methods used to enable real-time communication between a client (e.g., a messaging app) and a server. Let’s break them down in detail:

1. Polling
Polling is a technique where the client periodically sends requests to the server to check if there are any new updates or messages.

How Polling Works
The client sends an HTTP request to the server at regular intervals (e.g., every 5 seconds).
The server checks if there are new messages or updates for the client.
If there are new messages, the server sends them in the response.
If there are no new messages, the server sends an empty response.
The client waits for the interval to complete and then sends another request.
This cycle continues until the user is actively using the application.
Example (Polling)
Client: “Hey server, do I have new messages?”
Server: “Nope, not yet.”
Client: (after 5 seconds) “Hey server, do I have new messages now?”
Server: “Yes, here’s your new message!”
Advantages of Polling
Simple to Implement: Easy to set up on both the client and server sides.
Widely Supported: Works with any HTTP server and doesn’t require advanced configuration.
Disadvantages of Polling
Inefficient: Most requests return empty responses, wasting network bandwidth and server resources.
Higher Latency: Updates are delayed until the next polling interval. For example, if the polling interval is 5 seconds, there can be up to a 5-second delay in receiving new messages.
Scalability Issues: If many clients are polling the server frequently, it can overload the server.
2. Long Polling
Long polling is an improved version of polling that reduces inefficiencies by keeping the connection open until the server has new data to send.

How Long Polling Works
The client sends an HTTP request to the server and waits for a response.
The server does not immediately respond. Instead, it keeps the connection open until:
A new message or update is available, or
A timeout period is reached (e.g., 30 seconds).
When the server has a new message, it sends the response to the client.
The client processes the message and immediately sends a new request to the server to wait for the next update.
This process continues as long as the user is active.
Example (Long Polling)
Client: “Hey server, do I have new messages? I’ll wait for your reply.”
Server: (waits until it has a new message) “Yes, here’s your new message!”
Client: “Got it! Do I have more messages?”
Server: (waits again)
Advantages of Long Polling
Efficient Use of Resources: Reduces the number of empty responses, as the server only responds when there is new data.
Low Latency: Messages are delivered as soon as they become available, with minimal delay.
Easy to Implement: Simpler than advanced alternatives like WebSockets.
Disadvantages of Long Polling
Connection Overhead: Each long-polling request is a separate HTTP connection, which can be resource-intensive if there are many users.
Scalability Challenges: Keeping many connections open simultaneously can strain the server.
Timeouts and Reconnections: If the connection times out (e.g., after 30 seconds), the client must reconnect, which adds overhead.


When to Use Polling vs. Long Polling in Messaging Applications
Polling:

Suitable for small-scale applications with fewer users or where real-time updates are not critical.
Example: A notification system that updates every few minutes.
Long Polling:

Better for applications where near real-time updates are required but WebSockets are not feasible.
Example: Messaging apps where users expect updates as soon as possible.
Why Long Polling is Still Not Ideal for Real-Time Applications
While long polling improves efficiency compared to polling, it still has limitations in terms of scalability and performance. For true real-time communication, modern applications often use WebSockets or Server-Sent Events (SSE):

WebSockets: Create a persistent, bi-directional connection between the client and server. Perfect for high-performance chat apps.
SSE: A lightweight, uni-directional alternative for real-time updates from the server to the client.
Summary
Polling sends frequent requests regardless of whether there’s new data, leading to inefficiencies.
Long Polling holds the request open until new data is available, reducing unnecessary requests and improving latency.
For modern messaging apps with high user demand, transitioning to WebSockets is often the best long-term solution.


WebSocket - 

But in this it will lead to the billions of active connections on the server which is very compute intensive.

WebSockets: Real-Time Communication
WebSockets are a modern, advanced technology that enables full-duplex (two-way) communication between a client (e.g., browser, mobile app) and a server over a single, persistent connection. This makes WebSockets highly efficient and ideal for real-time applications like messaging apps, gaming, financial dashboards, and live collaboration tools.

How WebSockets Work
Initial Handshake (Upgrade Protocol):

The communication starts with a standard HTTP request from the client to the server.
The client requests to "upgrade" the connection from HTTP to WebSocket using the Upgrade header.
If the server supports WebSockets, it responds with a status code 101 Switching Protocols, confirming the connection upgrade.
Persistent Connection:

Once upgraded, the connection remains open, allowing bi-directional data transfer without the overhead of creating a new HTTP request for each message.
Data Exchange:

Both the client and server can send messages to each other at any time, without the need for the client to make repeated requests.
Messages are exchanged in a lightweight, binary, or text-based format.
Connection Termination:

The connection remains open until either the client or server explicitly closes it or if a network issue occurs.
Key Features of WebSockets
Full-Duplex Communication:

Both the client and server can send and receive data simultaneously over the same connection.
Low Latency:

No need for repeated HTTP requests, so messages are delivered almost instantly.
Persistent Connection:

The connection remains open, reducing the overhead of establishing multiple connections.
Lightweight:

WebSocket frames are smaller compared to HTTP headers, making it ideal for high-performance, real-time applications.
Efficient Resource Usage:

Fewer connections and less data overhead mean reduced strain on the server and network.
WebSocket Handshake Example
Step 1: Client Request (Upgrade to WebSocket):

http
Copy code
GET /chat HTTP/1.1
Host: example.com
Upgrade: websocket
Connection: Upgrade
Sec-WebSocket-Key: dGhlIHNhbXBsZSBub25jZQ==
Sec-WebSocket-Version: 13
Step 2: Server Response (Switch Protocols):

http
Copy code
HTTP/1.1 101 Switching Protocols
Upgrade: websocket
Connection: Upgrade
Sec-WebSocket-Accept: s3pPLMBiTxaQ9kYGzzhZRbK+xOo=
After this handshake, the WebSocket connection is established, and both parties can send and receive messages.

Advantages of WebSockets
Real-Time Communication:

Suitable for applications where instant updates are required, such as messaging, stock trading, or multiplayer games.
Reduced Overhead:

Unlike HTTP requests, WebSocket messages have minimal overhead, improving performance for frequent updates.
Scalable:

WebSockets are more efficient for handling large numbers of clients simultaneously compared to polling or long polling.
Cross-Platform:

WebSockets work across different platforms, programming languages, and devices.
Disadvantages of WebSockets
Complexity:

WebSockets are more complex to implement and manage compared to HTTP-based solutions.
Firewall Issues:

Some firewalls or proxies may block WebSocket traffic, as it does not follow traditional HTTP rules.
Resource Consumption:

A large number of open WebSocket connections can consume significant server resources if not managed properly.
WebSocket Use Case: Messaging Application
Here’s how WebSockets work in a messaging application:

1. Connection Establishment:
When a user opens the app, the client establishes a WebSocket connection with the server.
2. Real-Time Messaging:
When User A sends a message, it is sent via the WebSocket connection to the server.
The server processes the message and pushes it to the intended recipient (User B) over their WebSocket connection.
3. Event Notifications:
WebSockets can also be used to send real-time notifications, such as "User is typing…" or "User is online."
4. Bi-Directional Updates:
The server can push other updates (e.g., group message notifications, system messages) to the client without waiting for the client to request them.

When to Use WebSockets
WebSockets are ideal for applications requiring real-time updates, such as:

Messaging Applications (e.g., WhatsApp, Slack).
Live Chat Support.
Online Gaming.
Stock Market Updates.
Collaborative Tools (e.g., Google Docs, Trello).
Live Streaming (e.g., sports scores, video streams).
Conclusion
WebSockets offer a highly efficient and scalable solution for real-time, bi-directional communication. They are a significant improvement over traditional polling and long polling methods, particularly in scenarios where low latency and high-frequency updates are required. If you're building a messaging app or any real-time system, WebSockets are the go-to solution.

Blog - https://blog.whatsapp.com/1-million-is-so-2011


Message send mechanism - 

This is a common scenario in messaging systems where message delivery guarantees need to be ensured, particularly in unreliable networks. Let's break this down step by step, addressing the key issues:

1. How Message Sending and Acknowledgement Work
In a typical messaging system, the process of sending and acknowledging messages works like this:

Steps for Message Delivery
Client Sends a Message:

The client sends a message (or request) to the server.
This is typically done over a transport layer protocol like TCP, WebSockets, or HTTP.
Server Receives the Message:

The server processes the message and stores it (in memory or in a persistent store like a database or queue).
After processing, the server sends an acknowledgement (ACK) back to the client to confirm that the message was received.
Client Receives the Acknowledgement:

Once the client gets the ACK, it marks the message as delivered and does not retry sending it.
2. Problems in Message Delivery
Case 1: Message Does Not Reach the Server
This could happen due to network issues or server unavailability.
The client will wait for an acknowledgement within a timeout period. If no ACK is received:
Retry Mechanism: The client retries sending the message until it is acknowledged or the maximum retry limit is reached.
Case 2: Message is Received by the Server, but ACK is Not Received by the Client
The server processes the message and sends an ACK, but the ACK is lost due to a network issue.
The client, not receiving the ACK, assumes the message was not delivered and retries sending the same message.
This results in duplicate messages being sent to the server.
3. How Duplicate Messages are Handled
Deduplication Mechanism
To handle duplicate messages, messaging systems implement a deduplication mechanism based on unique identifiers. Here's how it works:

Unique Message ID:

Each message sent by the client has a unique ID (e.g., messageId).
The server stores the messageId of every message it processes.
Check for Duplicate Messages:

When a new message is received, the server checks if the messageId is already present in its system (in a cache or database).
If it finds a match, it ignores the duplicate message and does not process it again.
If it doesn’t find the messageId, it processes the message and stores its ID.
Send Acknowledgement:

After processing a message, the server always sends an ACK, regardless of whether the message was new or a duplicate.
4. Handling Network Failures and Retries
Messaging systems are designed to tolerate unreliable networks by implementing at-least-once delivery guarantees with deduplication mechanisms.

Scenario 1: Lost Message
The client retries sending the message until an ACK is received.
The server processes the message only once (deduplication ensures this).
Scenario 2: Lost Acknowledgement
The client retries sending the message because it assumes the first message was not received.
The server identifies the duplicate message and skips re-processing it.
5. Example: Messaging Workflow with Deduplication
Step 1: Message Sent
Client sends a message with messageId: 123.
Step 2: Server Processes Message
Server receives the message, processes it, and stores messageId: 123 in a cache or database.
Step 3: ACK Lost
Server sends an ACK to the client, but the ACK is lost due to a network failure.
Step 4: Client Retries
Client retries sending the message with messageId: 123.
Step 5: Server Deduplication
Server checks its records and finds that messageId: 123 has already been processed.
Server skips re-processing and simply sends a new ACK to the client.
6. Strategies for Message Delivery Guarantees
At-Least-Once Delivery
The message is delivered at least once, but duplicates may occur.
Deduplication is required to avoid processing duplicate messages.
At-Most-Once Delivery
The message is delivered at most once. If the ACK is lost, the client will not retry.
Suitable for scenarios where duplicates cannot be tolerated but occasional message loss is acceptable.
Exactly-Once Delivery
Each message is delivered exactly once, without duplicates.
This is achieved by combining deduplication and transactional mechanisms (e.g., two-phase commits).
Cost: Higher complexity and overhead compared to other delivery guarantees.
7. Tools and Protocols Supporting Reliable Messaging
Messaging Protocols
HTTP/REST:

Requires client-side retry logic and unique identifiers for deduplication.
AMQP (Advanced Message Queuing Protocol):

Used in RabbitMQ. Supports delivery guarantees, deduplication, and ACKs.
Kafka:

Implements unique offsets for deduplication.
Supports exactly-once semantics with transactional producers.
MQTT (Message Queuing Telemetry Transport):

Lightweight protocol with QoS levels for different delivery guarantees.
WebSockets:

WebSocket-based messaging systems typically handle retries and deduplication manually at the application level.

Real-World Examples
WhatsApp/Signal:

Messages are assigned unique IDs.
Duplicate messages are ignored on the server side.
Payment Systems:

Ensure that duplicate payment requests do not result in multiple deductions.
Use unique transaction IDs and database transactions.
Message Queues (RabbitMQ, Kafka):

Use offsets or message IDs to handle duplicates and ensure delivery guarantees.
Conclusion
Lost Requests: Client retries the request until an acknowledgement is received.
Lost Acknowledgements: Deduplication ensures the server processes the message only once, even if the client resends it.
Implementing unique identifiers for messages and maintaining a deduplication log is critical for ensuring reliable messaging systems.


Message id will be generated by using below mechanism - 



 Message id = timestamp + userId + deviceInfo or browser info.
 
here device info will be browser id or device id or session id etc.

and time stamp  will be the timestamp of the last typed character.


Idempotency - 

 In the context of messaging systems (or any system handling retries and duplicates), idempotency ensures that repeating the same operation multiple times produces the same result as performing it once. This property is critical when handling scenarios like retries, duplicate messages, or lost acknowledgments.


Sharding - 

How to choose the sharding key -

we will try to optimize getMessage , getConversation, sendMessage api.


Conversation id - If we shard based on conversation id then 

getMessage(conversation_id)  - only one server will get the request.

getConversations(user_id) - for this we will have to visit all the servers to get the data.

sendMessage( user_id, conversation_id, ...)  - only one server will get the request.

 User id - 

If we shard based on user id then 

all the conversation of a particular user will be stored in a single machine.


and we will have two databases - 

1 - where all the messages will be stored where the sharding key is going to be the conversation id.

2 - in this database all the conversations will be stored and its sharding key will be user id.
   
    
getMessage(conversation_id)  - for this we will have to go the message database and only one machine to get the data.

getConversations(user_id) -  Conversations data base and only one machine.

sendMessage( user_id, conversation_id, ...)  -  we will have to go to Messages database and only one machine we need to access.

and in the conversations database we will have to go through 2 machines to update the latest message in the conversation, one to update the senders message and one to update the receviers messages.

but this will be heavy for group chats because we will have to visit lot of machine to update the latest message. for one to one conversations this will work.

Our design should be such that the time complexity should not increase with scale.

Messanger design notes link - https://docs.google.com/document/d/1FE10Pu4nd6sz89RHq82rkcO2fdNoOZA0jQgO8pLgsOc/edit?tab=t.0

-----------------------

System Design - Zookeeper + Kafka 

Problem statement - 


Order placement on amazon - 
When we place the order on the amazon then amazon must be taking lot of actions once an order gets confirmed.

1) Update inventory in inventory service.
2) Seller service to inform the seller.
3) Notification service to send the notification.
4) Invoice service to generate the invoice.
5) Delivery service to pick up the parcel and deliver it to the user.

Once the order is placed then there is no dependency on the customer, all above tasks are performed by amazon internally in parallel by using asynchronous manner.


The order service will send the conformation to the user that the order has been placed but it is not conformed yet. 

if we perform above operations in the one by one ( Synchronous) format then in this case if there is load on the order service, we  will have to scale up the order service and also due to massive load we will also have to scale the Inventory service and all other services in the line.

And in case of Asynchronous manner Order service will push the event inside the event queue. Event will have different parameters like type of the event, Order id, item id etc.

then other above services will read the event from the queue and then perform the operation. ex Inventory service will read the event and update the inventory, like this all other services will get the event from queue and perform operations accordingly.

In this way even if there is a load on the order service it will scale and push the more no of events to the queue and other services will tconsume the events based on their speed and there is no need to scale other services.

 and if average waiting time complete one event is increasing then we will scale other services based on the need.
 
  
In this mechanism the service which produces the events is called as Publisher or producer and the service which consumes the events is called as Consumers or Subscribers.

Kafka, Rabbit MQ, Active MQ, SQS are the examples of the queues.

Kafka can be used whereever there is asynchronous communication.    


These message queues are persistent message queues, and store the data in hard disk like database.

Each event will have a timeout, once the timeout reaches for the event it will remove that event from the queue and put it in the  DLQ ( Dead letter queue).

if some service was not availabel to read the events from the queue before timeout then it can read those events from the DLQ.

and DLQ also has the timeout.


when service will come up then it will read the events from the queue and also from the DLQ to clear the backlog.

Kafka maintains offset  for every consumer to keep the track of events read by that consumer.

Kafka stores events in hard disk it is not in memory.

Video upload on youtube - 

when video is uploaded on the youtube then it checks for copyright, convert video into multiple resolutions, check for community guidelines, generate captions all these things are checked asynchronously by using kafka or similar queue. Once video is uploaded it will trigger all of these services by using kafka.

 to increase the consumption speed of the consumer we need to increase the computational power of the service.
 

Consumer Groups - Kafka makes sure only one consumer from a consumer group will be able to read a msg from the queue.
As there will be multiple machine in the one service, to avoid one event being  read by multiple machines of the same service, kafka uses consumer groups.


 
Internal architecture of kafka - 

inside kafka there will be sharding because events will be too high in numbers.

Topic is the type of event we can store in the kafka.

 in one kafka queue there can be many different topics.
 
 each event will be stored in that particular topic only.
 
 suscriber or consumer will have to subscribe to the particular topic.
 
 example - 
 
 there can be different topics like order placed, order canceled, sign up etc.  and seller service will subscribe to order placed and order canceled topics and notification service will subscribe to Sign up event.


There will be kafka cluster because we can not store huge data on a single machine. and inside there will be multiple machines inside one cluster.
To each machine in kafka cluster we call broker.


we can not store all the events of one topic in one single broker, then we will store all these  events in different partitions.

Partition means dividing the events in one topic into multiple parts, so that it can be stored in different brokers.


like this we will store the big topics into smaller partitions on different brokers.

and one broker can store the partitions of different topics on the same machine ( Broker).

and also one broker can store the different partitions of the same topic.


in this structure when we push the event into the kafka, then on which partition it will be stored is decided by the sharding key.

Each broker will have replica by using master, slave architecture so that it will not go down.

doing partitioning will also reduce the load on one broker to process the event.

Zookeeper and Kafka notes - https://docs.google.com/document/d/1nXeJJC-8tLNUyna-QAI9SiISBZTv93-gmeKh5qPiR6Y/edit?tab=t.0

Go thorugh Column No SQL database.

Kafka is message queue, it is used when we need asynchronous commnunication.

calling services one by one in one single service, this approach is called Synchronous communication.

service will wait for the response from another service before move on to the next task.

Total no of partitions can be more than the total no of brokers present in the kafka cluster.

and replica of brokers can be less than or equal to the no of brokers.

all these configurations can be added to the xml file while setting up the kafka.


Sharding Vs partitioning - 

Sharding and partitioning are often used interchangeably, but they differ conceptually and practically, especially in systems like Apache Kafka. Let’s break them down thoroughly in the context of Kafka.

1. What is Partitioning in Kafka?
Definition:
In Kafka, partitioning refers to dividing a topic into multiple smaller, logical units called partitions. Each partition is a subset of the total data for that topic and is stored separately across Kafka brokers.

Key Points:
A Kafka topic is split into partitions, each of which is an ordered, immutable sequence of records.
Producers can decide which partition a message goes to, often using a key and a partitioning strategy.
Partitions enable parallelism and scalability by allowing multiple producers and consumers to read and write simultaneously.
Partitions in Kafka are physical units of storage.
Why Partitioning is Important in Kafka:
Parallelism: Producers and consumers can work with multiple partitions in parallel, improving throughput.
Scalability: Partitions allow Kafka topics to scale horizontally by distributing them across multiple brokers.
Fault Tolerance: Kafka replicates partitions across brokers for durability, ensuring no data loss in case of a broker failure.
How Partitioning Works:
When producing a message, Kafka determines the partition for the message using:
A key-based strategy (e.g., using a hash of the key to select a partition).
A round-robin approach if no key is specified.
Consumers typically read messages from partitions in parallel.
2. What is Sharding?
Definition:
Sharding is a broader concept than partitioning and refers to the process of dividing a dataset or workload across multiple shards (physical or logical units) to improve scalability and performance.

A shard is a subset of the total dataset, often independent of the others.
Sharding is commonly used in databases (e.g., MongoDB, MySQL) and systems like Elasticsearch, but the term can apply to Kafka as well.
How Sharding Relates to Kafka:
In Kafka, partitions can be considered a form of sharding, as each partition represents a shard of the topic data.
The concept of sharding applies at a higher level in Kafka, e.g., distributing topics or partitions across brokers in the cluster.


Comparison: Partitioning vs. Sharding
Aspect	Partitioning (in Kafka)	Sharding
Scope	Specific to Kafka and used for dividing a topic into smaller units.	A general concept applicable to distributed systems (e.g., databases, search engines, or Kafka).
Purpose	Improve parallelism, scalability, and fault tolerance within a Kafka topic.	Distribute data/workload across multiple machines to handle scalability and performance.
Granularity	Operates within a Kafka topic. Each partition belongs to a single topic.	Operates at a higher level. Can involve entire topics, databases, or datasets split into multiple parts.
Data Distribution	Messages are distributed to partitions based on a key, partitioning strategy, or round-robin.	Data is distributed across shards based on sharding keys or a distribution algorithm.
Physical Representation	Partitions are stored as files on Kafka brokers.	Shards may represent independent datasets stored on separate servers or nodes.
Fault Tolerance	Achieved via replication of partitions across brokers.	Achieved via redundancy mechanisms, such as multiple replicas of shards.
Example in Kafka	A topic orders is divided into 10 partitions, distributed across 3 brokers.	A Kafka cluster with topics orders, payments, and invoices distributed across multiple brokers.




How Kafka Implements Partitioning (with Example)
Imagine a Kafka topic named orders with 5 partitions distributed across 3 brokers:

Producer Behavior:

Producer sends a message with a key (e.g., orderId).
Kafka uses the key and a hash function to decide which partition to send the message to.
Example: Hash(orderId) % 5 (number of partitions).
If no key is provided, Kafka may use a round-robin strategy to distribute messages evenly.
Consumer Behavior:

Consumers read data from partitions.
If there are 5 partitions and 2 consumers in a consumer group, partitions will be assigned like:
Consumer 1 reads from partitions 0, 1, and 2.
Consumer 2 reads from partitions 3 and 4.
Partition Distribution:

Kafka ensures that partitions are distributed across brokers to balance the load.
Example:
Partition 0 → Broker 1
Partition 1 → Broker 2
Partition 2 → Broker 3
Partition 3 → Broker 1
Partition 4 → Broker 2
How Sharding Applies to Kafka (Higher-Level View)
Sharding in Kafka often refers to distributing topics or partitions across brokers:

Cluster with Multiple Topics:

Imagine a Kafka cluster with 3 brokers and 10 topics (orders, payments, inventory, etc.).
Kafka distributes the partitions of each topic (shards of the topic data) across the brokers.
Example:
Broker 1 stores partitions 0, 3, 6 of various topics.
Broker 2 stores partitions 1, 4, 7.
Broker 3 stores partitions 2, 5, 8.
Sharding Keys:

At the producer level, you can shard data by specifying a key (e.g., customer ID or order ID).
This ensures all related data (e.g., for a customer) lands in the same partition.
When to Use Partitioning/Sharding in Kafka
Use partitioning to scale a single topic horizontally and increase throughput by distributing data across partitions.
Use sharding when managing Kafka at a higher level, such as:
Distributing multiple topics across brokers.
Deciding how to distribute data in Kafka to handle load balancing across the cluster.
Conclusion
Partitioning in Kafka is a specific implementation of data division within a topic. It is a core feature that ensures scalability, parallelism, and fault tolerance.
Sharding is a more general concept that applies to data distribution across systems. In Kafka, partitioning can be considered a form of sharding when applied within topics, while sharding applies more broadly to the cluster as a whole.

Partitioning vs Sharding in Kafka

Scope

Partitioning: Specific to Kafka and used for dividing a topic into smaller units.
Sharding: A general concept applicable to distributed systems (e.g., databases, search engines, or Kafka).
Purpose

Partitioning: Improves parallelism, scalability, and fault tolerance within a Kafka topic.
Sharding: Distributes data/workload across multiple machines to handle scalability and performance.
Granularity

Partitioning: Operates within a Kafka topic. Each partition belongs to a single topic.
Sharding: Operates at a higher level. Can involve entire topics, databases, or datasets split into multiple parts.
Data Distribution

Partitioning: Messages are distributed to partitions based on a key, partitioning strategy, or round-robin.
Sharding: Data is distributed across shards based on sharding keys or a distribution algorithm.
Physical Representation

Partitioning: Partitions are stored as files on Kafka brokers.
Sharding: Shards may represent independent datasets stored on separate servers or nodes.
Fault Tolerance

Partitioning: Achieved via replication of partitions across brokers.
Sharding: Achieved via redundancy mechanisms, such as multiple replicas of shards.
Example in Kafka

Partitioning: A topic orders is divided into 10 partitions, distributed across 3 brokers.
Sharding: A Kafka cluster with topics orders, payments, and invoices distributed across multiple brokers.

 
  Apache Kafka does not use a traditional Master-Slave architecture in its design. Instead, Kafka employs a distributed and replicated architecture that achieves fault tolerance, high availability, and scalability through leader-follower replication for its partitions.

However, the concepts of "Master-Slave" can be loosely related to Kafka's Leader-Follower architecture, which we will explore step by step.

Kafka's Leader-Follower Architecture (Related to Master-Slave)
Kafka implements replication for partitions to ensure fault tolerance. Each partition in a Kafka topic has one leader and one or more followers. Here's how it works:

Leader Partition (Master):

The leader is responsible for handling read and write requests from producers and consumers for that partition.
Each partition in Kafka elects a leader, which resides on one of the brokers.
The leader acts as the "master" for that partition.
Follower Partition (Slave):

Followers replicate the data from the leader.
Followers stay in sync with the leader by fetching the latest records.
If the leader fails, one of the followers is elected as the new leader to ensure availability.
Followers cannot handle read or write requests; they are passive unless promoted to leader.
How Kafka Implements Leader-Follower (Master-Slave) Architecture
Partition and Replication:

A Kafka topic is divided into partitions.
Each partition is replicated across multiple brokers (based on the replication factor).
For example, if a topic has 3 partitions and a replication factor of 2, each partition will have 1 leader and 1 follower (replica).
Broker Roles (Master-Slave Concept):

Each broker can act as a leader for some partitions and as a follower for others.
There is no single "master" broker for the entire cluster, as Kafka avoids single points of failure.
The partition leader dynamically switches between brokers in case of failure.
Producer and Consumer Behavior:

Producers always send data to the leader partition for a topic.
Consumers always read data from the leader partition.
Followers are used only for replication, not for serving requests.
Replication Process:

The leader partition writes the incoming data to its local log.
Followers pull data from the leader and write it to their logs.
Kafka ensures in-sync replicas (ISR), where followers keep up-to-date with the leader.
Followers lagging behind the leader are removed from the ISR until they catch up.
Leader Election:

If the broker hosting the leader partition fails, one of the in-sync followers is promoted to the new leader.
The Kafka Controller (a role within the Kafka cluster) manages this leader election process.
Implementation Steps for Master-Slave (Leader-Follower) in Kafka
1. Replication Configuration:
Set the replication.factor for a topic when creating it.
bash
Copy code
kafka-topics --create \
  --topic my-topic \
  --partitions 3 \
  --replication-factor 2 \
  --bootstrap-server localhost:9092
Here:
3 partitions will be created.
Each partition will have 1 leader and 1 follower (total of 2 replicas per partition).
2. Partition Assignment:
Kafka distributes partitions across brokers.
Example: With 3 partitions and 3 brokers, the assignments might look like this:
Partition 0: Leader on Broker 1, Follower on Broker 2
Partition 1: Leader on Broker 2, Follower on Broker 3
Partition 2: Leader on Broker 3, Follower on Broker 1
3. Data Flow (Master-Slave Interaction):
Producer writes to the leader of a partition.
The leader writes data to its local log and propagates it to the followers.
Consumers read from the leader of a partition.
4. Leader Election:
If Broker 1 fails, Kafka elects a new leader for Partition 0 from the in-sync replicas.
This ensures that the partition remains available and operational.
5. Monitoring Replication:
Use Kafka tools to monitor the state of leaders and replicas.
bash
Copy code
kafka-topics --describe --topic my-topic --bootstrap-server localhost:9092
Advantages of Kafka's Leader-Follower (Master-Slave) Architecture
High Availability:

If a leader (master) fails, a follower (slave) takes over as the leader.
This eliminates single points of failure.
Scalability:

Kafka distributes leaders and followers across brokers, balancing the load.
Fault Tolerance:

Replication ensures no data loss, even if a broker goes down.
Parallelism:

Producers and consumers interact with multiple partitions and brokers in parallel, improving throughput.
Key Differences Between Traditional Master-Slave and Kafka's Implementation
Dynamic Leadership:

In Kafka, the leader (master) is dynamic and can change due to failures.
Traditional master-slave systems often have a static master node.
No Single Master Node:

Kafka avoids a central master node; instead, it distributes leadership across partitions.
Traditional systems often rely on a single master controlling the slaves.
Decentralized Design:

Kafka brokers can act as leaders and followers simultaneously for different partitions.
This ensures better resource utilization and avoids bottlenecks.
Example Scenario
Cluster Setup: 3 brokers, 3 partitions, replication factor = 2.

Partition Assignments:

Partition 0: Leader → Broker 1, Follower → Broker 2
Partition 1: Leader → Broker 2, Follower → Broker 3
Partition 2: Leader → Broker 3, Follower → Broker 1
Data Flow:

Producer sends data for Partition 0 to Broker 1 (leader).
Broker 1 replicates data to Broker 2 (follower).
Consumer reads data for Partition 0 from Broker 1 (leader).
Failure Handling:

If Broker 1 goes down, Kafka elects Broker 2 as the new leader for Partition 0.
Broker 3 continues to replicate data as the new follower.
Summary
Kafka uses a Leader-Follower architecture instead of a strict Master-Slave model.
Leaders handle all client interactions (reads and writes).
Followers replicate data and provide fault tolerance.
Leadership is dynamic and managed by Kafka's controller.


 Zookeeper - is the central manager to manage the kafka cluster and it is not kafka specific, whereever we have shards and master slave architecture we can use the zookeeper to manage it.
 
 

Apache ZooKeeper is an open-source, centralized service for maintaining configuration information, naming, providing distributed synchronization, and managing group services. In the context of Apache Kafka, ZooKeeper is an essential component used to coordinate and manage the distributed nature of Kafka clusters.

Role of ZooKeeper in Kafka
ZooKeeper plays a critical role in managing the Kafka cluster's metadata and ensuring coordination among brokers. Here's how ZooKeeper is used with Kafka:

1. Cluster Management
ZooKeeper keeps track of all brokers in a Kafka cluster.
When a broker starts, it registers itself with ZooKeeper, making the cluster aware of its presence.
If a broker fails, ZooKeeper detects this failure and notifies Kafka.
2. Leader Election
For each topic partition, Kafka assigns a leader broker responsible for all reads and writes to that partition.
ZooKeeper manages the leader election process to ensure a single leader per partition.
If the leader broker fails, ZooKeeper initiates a new leader election for the affected partitions.
3. Topic and Partition Metadata
ZooKeeper stores metadata about topics, partitions, and their configuration (e.g., replication factor).
Producers and consumers can query ZooKeeper indirectly (via brokers) to understand the cluster's structure.
4. Configuration Management
ZooKeeper stores Kafka's configuration settings, such as broker IDs, topic information, and quotas.
Dynamic reconfiguration of Kafka often involves ZooKeeper updates.
5. Distributed Synchronization
ZooKeeper provides a mechanism for distributed synchronization, ensuring consistent decisions across Kafka brokers.
For example, when a broker joins or leaves, ZooKeeper coordinates the necessary rebalancing tasks.
6. Offset Storage (Deprecated)
Before Kafka version 0.9.0, ZooKeeper was used to store consumer offsets.
Since version 0.9.0, Kafka stores offsets in an internal topic (__consumer_offsets), making offset management more efficient and scalable.
How Kafka Uses ZooKeeper
Broker Registration

When a broker starts, it creates an ephemeral node in ZooKeeper to register itself.
If the broker disconnects or fails, the ephemeral node is deleted, alerting the cluster of the broker's unavailability.
Leader and ISR Management

Kafka uses ZooKeeper to elect leaders for partitions and maintain the "In-Sync Replicas" (ISR) list.
The ISR list ensures that replicas are up-to-date with the partition leader.
Topic Creation and Metadata Storage

ZooKeeper holds information about topics, such as the number of partitions, replication factors, and configuration.
Controller Election

One Kafka broker is elected as the controller for the cluster using ZooKeeper.
The controller is responsible for managing the cluster, including leader election and rebalancing.
Limitations of Using ZooKeeper
Single Point of Failure: ZooKeeper itself must be highly available, requiring a quorum-based setup (3 or 5 nodes).
Complexity: Managing ZooKeeper adds operational overhead.
Performance Bottleneck: Heavy reliance on ZooKeeper can become a bottleneck as the Kafka cluster scales.
Kafka Without ZooKeeper
Kafka has moved towards a ZooKeeper-less architecture with the introduction of Kafka Raft Metadata Mode (KRaft).
KRaft replaces ZooKeeper with a built-in consensus protocol, simplifying Kafka's architecture and improving scalability and fault tolerance.


Issues with zookeeper -

It introduces the additional network hop and increases the network latency.

Single point of failure.

How to decide the master - 

Zookeeper maintains the file which contains contains the  topic and partition and ip address of the master. this file is called as the Ephimeral files and these files are temporary , and the master is decided by using the leader election algorithm.

when master goas down then it is the task of zookeepr to inform the slaves or listeners that the leader election is going to happen, so that slaves will try to become the master.

 
the data of the ephimeral file is cached on the application server and whenever there is change in the masters data in the file, zookeeper will inform to the application server about the recent changes and app server will update the cache.


Leader election is also used in the database.

How to avoid single point of failure - 

To avoid the single point of failure in case of zookeeper, it will also have the master slave architecture for zookeeper also.

and when master of the zookeeper goas down then in that case the algorithm called RAFT is used to select the new master to run the zookeeper. 


Earlier kafka used to use the Zookeeper now it has migrated to use the KRAFT.

we can use the zookeeper when ever managing the master slave architecture.

Go through the RAFT algorithm in short to get the idea.	


----------------------------------


----------------------------------



Telegram login - 8830449186 - Shyam Gandhi.

Sadhguru - dhananjayjadhav2151@gmail.com

Yogi Varunanand - 918830449186 - dhananjayjadhav2151@gmail.com - Dhananjay

Substack - dhananjayjadhav2151@gmail.com - OTP login.

e pik pahani app.
Proton mail app.

------------------------------

Black clover
Solo levelling, 
Haiku
Naruto

-----------------------------------------


System Design - Elastic search 


Elastic search is used to implement the full text search.


1) Attribute based searching - 

select * from users where username LIKE '%shahid%'

because of indexing this query will be feasible.

2) Full text search - 

select * from posts where content 
LIKE '%joined amazon%'

and it is not advisable to create an index on the content which has huge characters.

and we also want to search based on only few characters, for this we will have to search character by character.


If we have lot of documents and want to search something in these documents.

Brute force approach to full text searching is to iterate all the documents and check if the keyword is present or not.

Elastic Search - it is document based database which very similar to the mongo DB and it uses the inverted index to perform the documnet based seraching.

 
Elastic search will never be the primary database, it will always be secondary database.

 Inverted Index in Elasticsearch
An inverted index is the core data structure used in Elasticsearch for full-text search. It is a fundamental concept borrowed from information retrieval systems and is designed to make text search fast and efficient.

What is an Inverted Index?
An inverted index is like a "lookup table" that maps terms (words) to the documents (or records) that contain those terms. Instead of storing each document's content and scanning them one by one during a search, Elasticsearch creates an index of terms and their associated documents.

For example:

Suppose we have three documents:

"The quick brown fox"
"The lazy dog"
"The fox jumps over the lazy dog"
An inverted index for these documents would look like this:

Term    -> Document IDs
brown   -> 1
dog     -> 2, 3
fox     -> 1, 3
jumps   -> 3
lazy    -> 2, 3
over    -> 3
quick   -> 1
the     -> 1, 2, 3

How Elasticsearch Builds an Inverted Index
Tokenization: The input text is broken into tokens (words or phrases) using an analyzer.
Normalization: Each token is processed (e.g., converted to lowercase, stripped of punctuation, etc.).
Indexing: Each token is stored in the inverted index, along with the document ID and positional information (optional).
Structure of an Inverted Index
Terms: Unique words or tokens in the dataset.
Postings List: A list of document IDs where the term appears. Additional details like the term frequency, positions, and offsets may also be stored for advanced features like phrase matching.
Use Cases of Inverted Index in Elasticsearch
1. Full-Text Search
Scenario: Searching for documents that contain the word "Elasticsearch".
How it works: Elasticsearch looks up the term "Elasticsearch" in the inverted index and retrieves all documents containing the term, instead of scanning every document.
2. Phrase and Proximity Search
Scenario: Searching for the phrase "quick brown fox".
How it works: The inverted index stores positional information for terms. Elasticsearch uses this information to determine whether the words appear together and in the correct order.
3. Autocomplete and Suggestions
Scenario: A search bar that suggests terms as the user types.
How it works: Elasticsearch can use n-gram or edge n-gram tokenizers to break terms into smaller fragments, enabling fast suggestions.
4. Faceted Search
Scenario: Displaying the count of documents for categories (e.g., products by brand or price range).
How it works: Elasticsearch uses inverted indexes to quickly aggregate data and provide facets.
5. Log and Event Analysis
Scenario: Searching through logs for error messages or specific patterns.
How it works: The inverted index allows rapid searches through text-heavy log files.
6. Handling Multilingual Text
Scenario: Indexing and searching content in multiple languages.
How it works: Custom analyzers tokenize and normalize text based on language-specific rules.
Why Inverted Index is Efficient
Quick Lookup: The mapping of terms to document IDs eliminates the need to scan entire documents.
Space Optimization: Terms are stored once in the dictionary, reducing redundancy.
Advanced Querying: Positional data enables complex queries like phrase matching, proximity search, and ranking.
Example in Elasticsearch
Consider you index the following documents into an Elasticsearch index named articles:

Document 1
json
Copy code
{
  "title": "Elasticsearch Basics",
  "content": "Elasticsearch is a distributed, RESTful search engine."
}
Document 2
json
Copy code
{
  "title": "Full-Text Search",
  "content": "Elasticsearch uses an inverted index for text search."
}
Search Query
json
Copy code
{
  "query": {
    "match": {
      "content": "search engine"
    }
  }
}
How Elasticsearch Handles It
Tokenizes the search terms search and engine.
Looks up the terms in the inverted index.
Returns the documents (with relevance scores) that contain these terms.
Conclusion
The inverted index is the backbone of Elasticsearch's fast and efficient search capabilities. It allows Elasticsearch to:

Perform quick lookups.
Handle complex queries like phrase and proximity searches.
Scale horizontally for massive datasets.


Additional Key Points About Inverted Index
1. Analyzers in Elasticsearch
Role: Before a document is stored in the inverted index, it goes through an analyzer.
Components of an Analyzer:
Tokenizer: Splits the text into tokens (words or phrases).
Token Filters: Modify tokens (e.g., lowercase filter, stopword filter, stemming).
Character Filters: Preprocess the text (e.g., removing HTML tags).
Customization: Elasticsearch allows custom analyzers for specific use cases, such as handling synonyms, different languages, or case sensitivity.
Interview Tip: Be prepared to explain how analyzers work and when to use a custom analyzer.

2. Term Frequency and Document Frequency
Term Frequency (TF): Measures how often a term appears in a document.
Document Frequency (DF): Measures how many documents contain the term.
Importance: TF and DF are used to calculate TF-IDF (Term Frequency-Inverse Document Frequency), which determines the relevance of a term to a document in search results.
Interview Tip: Mention that Elasticsearch now uses BM25 (a ranking algorithm) instead of pure TF-IDF for scoring.

3. Field Data Types and Inverted Index
Not all fields are stored in an inverted index. For example:
Text fields: Indexed for full-text search.
Keyword fields: Not tokenized but indexed as is, used for exact matches.
Numeric and date fields: Stored differently for range queries but can still leverage indexing for speed.
Interview Tip: Explain how understanding field types can optimize indexing and query performance.

4. Index Size Optimization
Reducing Index Size:
Use filters like stopwords to ignore common words (e.g., "the", "is").
Combine synonyms at indexing time to reduce duplicate data.
Limit the number of fields indexed.
Compression: Elasticsearch compresses index data to save disk space.
Interview Tip: Discuss how you would handle large datasets by optimizing the inverted index.

5. Deletion and Updates in Inverted Index
Deletes: Elasticsearch doesn't remove documents immediately. It marks them as "deleted" and cleans them up during a background process called merge.
Updates: Elasticsearch doesn't update the document in place. Instead, it creates a new version and marks the old one as deleted.
Interview Tip: Highlight this to explain how Elasticsearch handles document lifecycle and the impact on performance.

6. Position and Offset Information
Position: Tracks the position of a term in the document for phrase queries (e.g., "quick brown fox").
Offsets: Store the start and end character positions of terms, used for highlighting matched terms in the search results.
Interview Tip: Demonstrate understanding of how these features enable advanced querying like proximity searches or snippets in results.

7. Shards and Replicas
Elasticsearch splits data into shards. Each shard has its own inverted index.
Replicas: Copies of shards for fault tolerance and load distribution.
Queries are distributed across shards and aggregated.
Interview Tip: Discuss how the inverted index interacts with shards and replicas to ensure scalability and high availability.

8. Query Execution with Inverted Index
Match Query: Tokenizes the search string and retrieves documents based on the inverted index.
Term Query: Looks up exact matches (e.g., for keywords).
Boolean Query: Combines multiple queries with logical operators like AND, OR, and NOT.
Interview Tip: Be prepared to explain how different queries leverage the inverted index.

9. Limitations of Inverted Index
Not suitable for:
Structured data: Like relational databases (but Elasticsearch can handle structured queries using filters).
Binary data: Such as images or videos (stored as attachments but not indexed for content).
Challenges in handling real-time updates due to the cost of reindexing.
Interview Tip: Show awareness of when Elasticsearch might not be the best tool.

Key Use Cases to Mention
E-Commerce: Product search with filters for categories, brands, and price ranges.
Log Analysis: Searching and analyzing text-heavy log data.
Content Management: Powering search for large-scale websites or document repositories.
Real-Time Analytics: Combining search and aggregation for dashboards.


for Search system availibility is more important than consistency.

we need to pre process all the documents to convert into inverted index, to make our searches optimize.

Intergrating Elastic search in our system - 

High-Level Elasticsearch Architecture
Data Source Layer

Description: The layer where your application's raw data resides.
Examples: Databases (MySQL, PostgreSQL), APIs, log files, or event streams.
Responsibilities:
Serve as the primary source of truth for your data.
Provide data to Elasticsearch for indexing.
Data Ingestion Layer

Description: A pipeline that processes and prepares data for Elasticsearch.
Components:
ETL (Extract, Transform, Load) Tools:
Tools like Logstash, Apache Kafka, or custom scripts extract data, transform it into a suitable format, and load it into Elasticsearch.
Preprocessors:
Clean and normalize data (e.g., remove duplicates, standardize date formats).
APIs:
Custom APIs to push data directly into Elasticsearch.
Responsibilities:
Ensure data is formatted and enriched appropriately before indexing.
Handle batch processing or real-time streaming of data.
Elasticsearch Cluster

Description: The core of the architecture where data is indexed, stored, and searched.
Components:
Nodes:
Elasticsearch instances that work together in a cluster. There are several types:
Master Nodes: Manage cluster-wide changes (e.g., adding/removing nodes).
Data Nodes: Store and index the actual data.
Ingest Nodes: Perform data transformation during indexing.
Coordinating Nodes: Route search and indexing requests.
Shards:
Indexes are divided into smaller pieces (shards) for horizontal scalability.
Shards can be primary or replicas for fault tolerance.
Index:
Logical structure that contains documents. Each document is indexed for efficient querying.
Responsibilities:
Store and manage indexed data.
Perform distributed and scalable search operations.
Ensure high availability and fault tolerance.
Application Layer

Description: Your application's backend, which interacts with Elasticsearch to perform search and data-related operations.
Components:
Search APIs:
Use Elasticsearch RESTful APIs for operations like search, bulk, update, etc.
Query Builders:
Tools or libraries (e.g., Elasticsearch Java API, Python elasticsearch library) to construct queries programmatically.
Responsibilities:
Handle search queries from users.
Fetch and process results from Elasticsearch.
Integrate search results into the application's UI or APIs.
User Interface (UI) Layer

Description: The front-end or reporting tools where search results are presented to users.
Examples:
Search Boxes: Autocomplete or faceted search interfaces.
Dashboards: Visualization tools like Kibana to display data analytics.
Responsibilities:
Provide an intuitive search and filtering experience.
Display aggregated or detailed search results.
Monitoring and Maintenance Layer

Description: Tools and systems to monitor the health and performance of the Elasticsearch cluster.
Examples:
Elasticsearch Monitoring APIs:
Monitor cluster health, shard allocation, and node status.
Tools:
Kibana, Elastic APM, or external tools like Prometheus and Grafana.
Responsibilities:
Detect and address cluster issues like node failures or performance bottlenecks.
Optimize indexing and querying for better performance.
Detailed Data Flow
Data Ingestion:

Raw data is extracted from the source (e.g., database or logs).
The data is transformed (e.g., tokenized, cleaned) using tools like Logstash, Kafka, or custom scripts.
Transformed data is sent to Elasticsearch for indexing via REST APIs.
Data Storage and Indexing:

Elasticsearch indexes the incoming data.
Data is distributed across shards for scalability.
Metadata and positional information are stored in the inverted index for efficient search.
Search Queries:

Users or applications send search requests to the application layer.
The application translates user requests into Elasticsearch queries.
Elasticsearch retrieves relevant documents using the inverted index and scoring mechanisms like BM25.
Aggregations are performed if necessary (e.g., faceted search).
Search Results:

Elasticsearch returns results to the application.
The application processes and formats the results for the UI.
The UI displays search results to the user.
Key Considerations
Scalability:

Use shards to horizontally scale as the data size grows.
Use replicas to handle high query loads and ensure availability.
Fault Tolerance:

Ensure replicas are configured for data redundancy.
Design the system to handle node failures gracefully.
Performance Optimization:

Use appropriate analyzers and mapping configurations for indexing.
Optimize query performance by designing efficient queries and minimizing the use of expensive operations.
Security:

Secure Elasticsearch with features like API keys, IP whitelisting, and encryption.
Use the Elastic Stack’s security features (if available).
Integration:

Design the ingestion pipeline to handle incremental updates and deletions.
Integrate Elasticsearch with other tools like Kibana for visualization.
Example Use Case: E-commerce Search
Data Source: Product catalog stored in a relational database.
Data Ingestion: Logstash fetches product data, transforms it (e.g., tokenizes product descriptions), and sends it to Elasticsearch.
Elasticsearch Cluster: Stores indexed product data.
Application Layer: Backend APIs use Elasticsearch to power search features like product suggestions, filters, and sorting.
UI Layer: Displays search results and filters in a user-friendly manner on the e-commerce site.
Monitoring: Kibana dashboards monitor cluster health and search performance.



While injecting data in the elastic search we need to follow below steps to process the data -

1) Remove stop words.
2) Stemming or finding the root words - like for word "Running" root word will be "run".
3) Generate tokens - in this the words will be joined to form the meaning ful phrases. if formed phrase has only one word then we can call it Unigrams, and if it has two words then we will call it Bigrams and for three words we will call it trigrams.

4) store these words in the elastic search with their document id.

Google uses the Term frequency and inverse document frequency statistical methods in NLP and information retrieval to measure how important a term is within a document relative to a collection of docuements.

in Elastic search sharding should not be done based on the words.


In original database the documents are stored and sharded based on the document id.

if we search something on the elastic search then we will get the document ids in which our serach is present, which are present on the different machines, so we will have to fetch the documents from all the shards of the database.


so to avoid this fetching the documents from different shards google does below -

It will store the indexing of the word and documents in the same database in which the actual documents are present, in this way we will reduce the no of queries between different shards.

and for each popular query it will pre process the results and store in on the cache. as we do not go to the second page of the google search results, it will only store first few results on the cache.

Elastic search is also used for logging.

---------------------------------

System Design - Design S3 

Design file storage -

S3, HDFS, BLOB storage and object storage is the same thing and uses the file storage architecture internally.

Images, Videos, media content, log files are stored on the s3 storage.

and the url of above contents will be stored in the database mapped with the post id and user id in case of the facebook.

file storage system is not optimized for search and other operations.

and we do not consider this file storage system in back of the envelop calculation.

 
Properties the file storage should have - 

store big files.

data should be durable and not lost.

upload and download performance should be good - when connection breaks while uploading and downloading then it should start from the last point not from start.

There are two ways to store the big files on the machine -

1) Upload complete file in a single machine.
2) Or Upload the file and distribute it across multiple machines.

 To solve the problem of Single point of failure we will use replication method.
 
 and sharding is used only when we need to distibute the data across multiple machines, when data is too much in size to store on single machine.
 
 
 if we store the file as a single unit in single machine then pros and cons are below -
 
 cons - 
 
1) file size is limited by the machine size.
2) Parallelism is not possible.

Pros - 
No need to maintain multiple entries for each chunk of the file.

we do not have to collate the chunks at the time of downloading.

or if we divide the file into multiple chunks and store them into multiple machines then pros and cons will be -

Pros - 
1) We can store the file of any size.
2) Parallelism is possible.

cons -

Need to maintain multiple entries for each chunk of the file.

we have to collate the chunks at the time of downloading.

Chunk size should not be very small and it should not be very large as well.


HDFS - Hadoop distributed file storage -

in there are two types of machines -

one stores the data of file in chunks and these are called as data nodes.

and other stores the mapping of the chunks of the file, like file one chunks are stored on this machine, we need to also maintain the metadata of all the files and their chunks. this machines is called as Name node.



Name node maintains which chunks of which file is present on which machines.

and data nodes are sharded but there is no need to shard the named nodes as they have small metadata.

 and if data in the named nodes is more than the capacity of the nodes then create multiple HDFS clustures instead of creating new name node. 
 
 What is HDFS?
HDFS (Hadoop Distributed File System) is a distributed file system designed for storing large datasets across a cluster of commodity hardware. It is a core component of Apache Hadoop and provides high throughput access to application data. HDFS is fault-tolerant, scalable, and designed to work efficiently with very large datasets.

Key Features of HDFS:
Distributed Storage: Data is distributed across multiple nodes in the cluster.
Fault Tolerance: Provides data redundancy by replicating data blocks across different nodes.
Scalability: Can scale horizontally by adding more nodes to the cluster.
High Throughput: Optimized for high throughput of data access, rather than low latency.
Write Once, Read Many (WORM): Files are typically written once and read many times.
Large Block Size: HDFS uses large block sizes (default is 128MB or 256MB) to minimize the number of metadata entries.
HDFS Internal Architecture
HDFS consists of two main components:

1. NameNode
The NameNode is the master node responsible for managing the metadata of the file system.
It maintains information about:
The directory structure
File permissions
Locations of data blocks
Stores metadata in memory for fast access.
The metadata is also persisted to disk (e.g., fsimage and edit logs).
2. DataNodes
The DataNodes are the worker nodes responsible for storing the actual data blocks.
Each DataNode manages storage attached to it and periodically reports back to the NameNode with:
The list of data blocks it is storing.
Health status (via heartbeats).
3. Secondary NameNode (Checkpoint Node)
Often misunderstood, the Secondary NameNode is not a backup NameNode.
It periodically merges the fsimage and edit logs from the NameNode to create a new fsimage.
Helps reduce the load on the NameNode and provides a consistent state.
HDFS Block Storage
Block Concept:
Files in HDFS are split into fixed-size blocks (e.g., 128MB).
Blocks are stored independently across different DataNodes.
Replication:
Each block is replicated (default replication factor: 3) to ensure fault tolerance.
The NameNode decides which DataNodes will store replicas.
HDFS Architecture and Data Flow
1. File Write Operation
A client interacts with the NameNode to request file creation.
The NameNode checks permissions and provides the client with a list of DataNodes to write blocks.
The client writes blocks directly to the chosen DataNodes in a pipeline (writes to the first, which writes to the second, and so on).
2. File Read Operation
The client requests the NameNode for the locations of the blocks of a file.
The NameNode provides the block locations.
The client reads the blocks directly from the DataNodes in parallel.
3. Heartbeat and Block Reports
DataNodes send periodic heartbeats to the NameNode to signal that they are alive.
They also send block reports containing the list of blocks they are storing.
4. Replication Management
If a DataNode fails, the NameNode detects missing replicas and ensures new replicas are created on other DataNodes.
Advantages of HDFS:
Fault Tolerance: Automatic replication ensures data availability even if nodes fail.
Cost-Effective: Runs on commodity hardware.
Scalable: Can handle petabytes of data by simply adding more nodes.
Data Locality: Computation can be moved to where the data resides, reducing network bandwidth usage.

 
 
 Scenario: Uploading and Reading a File in HDFS
Example File:
File Name: example.txt
File Size: 256MB
Replication Factor: 3 (default)
Block Size: 128MB (default)
Step 1: Writing the File (example.txt)
Client Requests File Creation:

The user initiates an upload for example.txt via an HDFS client or a Hadoop application.
The client sends a request to the NameNode to create the file in the HDFS namespace.
NameNode Allocates Blocks:

The NameNode checks if the file can be created (e.g., sufficient permissions, namespace availability).
It splits the file into two blocks:
Block 1: 128MB
Block 2: 128MB
For each block, the NameNode determines three DataNodes (based on the replication factor) to store the replicas.
Client Writes Blocks to DataNodes:

The client writes Block 1 to the first DataNode in the pipeline. This DataNode forwards the block to the second DataNode, and so on until all replicas are written.
The same process occurs for Block 2.
Example:
Block 1:
Replica 1: DataNode A
Replica 2: DataNode B
Replica 3: DataNode C
Block 2:
Replica 1: DataNode D
Replica 2: DataNode E
Replica 3: DataNode F
Metadata Update:

The DataNodes notify the NameNode after successfully storing the blocks.
The NameNode updates its metadata to reflect:
Block-to-DataNode mapping.
Replication status.
Step 2: Reading the File (example.txt)
Client Requests File Access:

The user requests to read example.txt.
The client sends a request to the NameNode to retrieve the file's block locations.
NameNode Provides Block Locations:

The NameNode checks its metadata and provides the client with the locations of the replicas for each block.
Example:
Block 1: DataNode A, DataNode B, DataNode C
Block 2: DataNode D, DataNode E, DataNode F
Client Reads Blocks from DataNodes:

The client directly contacts the DataNodes to fetch the blocks.
Example:
Block 1: The client might read from DataNode A.
Block 2: The client might read from DataNode D.
If a DataNode is unavailable (e.g., DataNode A is down), the client fetches the block from another replica (e.g., DataNode B).
File Reconstruction:

The client combines Block 1 and Block 2 to reconstruct example.txt.
Fault Tolerance in Action
DataNode Failure During Write:
If a DataNode fails while writing Block 1:
The client stops writing to the failed DataNode.
The NameNode detects the failure and reallocates a new DataNode for replication.
DataNode Failure During Read:
If DataNode A is down when the client tries to read Block 1:
The client retrieves Block 1 from another replica (e.g., DataNode B).
The NameNode detects the missing replica and initiates replication on another healthy DataNode.
How HDFS Components Interact in This Example
NameNode Responsibilities:

Handles client requests for file creation and read operations.
Allocates blocks and selects DataNodes for storage.
Maintains metadata for block-to-DataNode mapping.
Monitors DataNodes through heartbeats.
DataNode Responsibilities:

Stores actual data blocks.
Sends heartbeats and block reports to the NameNode.
Notifies the NameNode of successful writes.
Client Responsibilities:

Communicates with the NameNode for metadata.
Reads/writes data blocks directly to/from DataNodes.
Combines blocks to reconstruct the file during reads.

Replication Strategy in HDFS
The replication strategy in HDFS is a carefully designed mechanism to ensure fault tolerance, data availability, and efficient data access in a distributed environment.

Goals of Replication Strategy
Fault Tolerance: Ensure data is not lost in case of node or hardware failures.
High Availability: Maintain data accessibility even during network partitioning or node downtimes.
Load Balancing: Distribute replicas across nodes to balance the workload.
Rack Awareness: Optimize replication placement to minimize cross-rack traffic and improve fault isolation.
Replication Factor
The replication factor determines how many copies of a block are stored in the cluster.
Default: 3
Configurable per file.
Replication Placement Strategy
HDFS uses a rack-aware replication strategy to determine where replicas are stored.

Steps in the Placement Strategy:
First Replica:

Placed on the same node where the client is running if the client is co-located in the cluster (data locality).
Otherwise, placed on a randomly selected node in the cluster.
Second Replica:

Placed on a different node in a different rack from the first replica.
This ensures fault tolerance at the rack level. If one rack fails, data is still available in another rack.
Third Replica:

Placed on a different node within the same rack as the second replica.
This reduces cross-rack traffic during replication.
Additional Replicas (if replication factor > 3):

Placed randomly, but balanced across the cluster.
Why Rack Awareness?
Fault Isolation: Ensures that data remains accessible even if an entire rack goes down.
Network Optimization: Placing most replicas within the same rack minimizes cross-rack communication, which is more expensive than intra-rack communication.
Replication Monitoring and Balancing
Heartbeat and Block Reports:

DataNodes send periodic heartbeats and block reports to the NameNode to indicate they are operational and to report the blocks they store.
Under-Replicated Blocks:

If a block's replication falls below the desired factor (e.g., due to a DataNode failure), the NameNode schedules replication tasks to restore the replication factor.
Over-Replicated Blocks:

If a block is over-replicated (e.g., after a DataNode rejoins the cluster), the NameNode reduces the replication factor by deleting excess replicas.
Balancing:

The HDFS Balancer tool redistributes replicas across DataNodes to achieve balanced storage utilization.
Example: Replication Strategy in Action
File Details:
File Name: example.txt
File Size: 128MB
Replication Factor: 3
Cluster Setup:
6 Nodes across 2 Racks:
Rack 1: Node A, Node B, Node C
Rack 2: Node D, Node E, Node F
Replication Steps:
Client Request:

The client writes the file example.txt to HDFS.
First Replica:

Placed on Node A in Rack 1 (local to the client if possible).
Second Replica:

Placed on Node D in Rack 2 to ensure fault tolerance.
Third Replica:

Placed on Node B in Rack 1, minimizing cross-rack traffic.
Final Placement:
Block 1: Node A (Rack 1), Node D (Rack 2), Node B (Rack 1)
Advantages of HDFS Replication Strategy
Data Availability: Ensures that even if two nodes or one rack fails, data remains accessible.
Fault Tolerance: With replication, no single point of failure can lead to data loss.
Optimized Network Usage: Limits cross-rack traffic while maintaining fault tolerance.
Load Balancing: Distributes replicas across nodes to avoid hotspots.
Limitations
Storage Overhead: Higher replication factors consume more storage.
Latency: Writing and replicating blocks to multiple nodes can introduce slight delays.

 Scenario: Uploading a File to HDFS
We’re uploading a file named example.jpg (256MB) to HDFS through a system with an API Gateway and an Application Server.

Components Involved
Browser (Client): User uploads the file via a web interface.
API Gateway: Acts as an entry point, validating requests and forwarding them to the backend.
Application Server: Processes the incoming file and handles block formation from the input stream.
HDFS Client (on the Application Server): Manages communication with HDFS for block placement and replication.
HDFS Components:
NameNode: Stores metadata, such as file paths, block locations, and replication info.
DataNodes: Physically store the file blocks and replicate them.
Detailed Explanation of the Flow
1. File Upload via Browser
The user uploads example.jpg via a file upload form in the browser.

The browser sends an HTTP POST request to the API Gateway.
The file data is streamed as an input stream to the backend for processing.
2. API Gateway
The API Gateway:
Validates the incoming request (e.g., authentication, file size limits).
Ensures the request is routed to the appropriate Application Server.
The API Gateway doesn’t process the file data itself; it forwards the stream to the next component.

3. Application Server
Receiving the Input Stream
The Application Server receives the input stream of the file.
Instead of immediately sending the data to HDFS, the server:
Temporarily buffers the data.
Constructs blocks (128MB each) from the incoming stream.
Block Formation
The Application Server writes the file data into a local cache or memory buffer (e.g., using Java’s BufferedInputStream and ByteArrayOutputStream).
Once the buffer size reaches the configured block size (128MB), the block is considered complete.
4. HDFS Client Interaction
Communicating with the NameNode
The HDFS client (running on the Application Server) contacts the NameNode to request block allocation.
The NameNode:
Allocates storage for the block on three DataNodes (based on the replication factor).
Returns the list of allocated DataNodes to the HDFS client.
Streaming the Completed Block
The Application Server sends the completed block to the first DataNode in the pipeline.

The DataNode acknowledges receipt and forwards the block to the second DataNode.
The second DataNode forwards it to the third DataNode.
While the first block is being streamed, the Application Server continues processing the next part of the input stream to form the second block.

5. DataNodes
Storing Blocks
Each DataNode stores its copy of the block locally on disk.
The DataNodes periodically send heartbeats and block reports to the NameNode to confirm they are operational and storing the blocks.
6. Completing the File Upload
Finalizing Metadata
After all blocks are successfully written and replicated, the HDFS client sends a file close request to the NameNode.
The NameNode updates its metadata to include:
The file’s name and path.
Block IDs and their locations.
User Confirmation
The Application Server notifies the API Gateway of the successful upload, which informs the browser (user).
How Blocks Are Managed and Stored in Detail
Formation at Application Server
The Application Server ensures efficient utilization of network resources by buffering the file data and sending it block by block.
Example:
File Size: 256MB
Blocks Formed: 2
Block 1: First 128MB
Block 2: Remaining 128MB
Pipeline Replication
Block 1 (128MB) is sent to:
DataNode A (Primary)
DataNode B (Replica 1)
DataNode C (Replica 2)
Block 2 (128MB) follows the same process but with different DataNodes (e.g., D, E, F).
Why This Flow is Efficient
Block-Based Processing:

Large files are broken into smaller, manageable blocks.
Allows parallel processing across multiple DataNodes.
Replication:

Ensures fault tolerance by storing multiple copies of each block.
Streamlined Communication:

The API Gateway and Application Server decouple user interaction from backend processing, improving scalability and user experience.
Reduced Latency:

Data streaming and pipeline replication minimize delays.

Scenario: Downloading a File from HDFS
We’re downloading a file named example.jpg (256MB) stored in HDFS.

Replication Factor: 3.
Block Size: 128MB.
The file is split into two blocks:
Block 1: 128MB (stored on DataNodes A, B, and C).
Block 2: 128MB (stored on DataNodes D, E, and F).
Components Involved
Browser (Client): The user initiates the download request.
API Gateway: Acts as the entry point, validating and forwarding the request.
Application Server: Processes the request, communicates with HDFS, and assembles the file from blocks.
HDFS Client (on the Application Server): Handles communication with the HDFS NameNode and DataNodes.
HDFS Components:
NameNode: Provides metadata, such as block locations for the requested file.
DataNodes: Store the actual file blocks.
Flow for Downloading a File
1. User Initiates Download Request
The user clicks the "Download" button in their browser for the file example.jpg.
The browser sends an HTTP GET request to the API Gateway.
2. API Gateway
The API Gateway:
Validates the incoming request (e.g., authentication, file existence).
Forwards the request to the Application Server responsible for HDFS interactions.
3. Application Server
Requesting Metadata from HDFS
The Application Server’s HDFS Client communicates with the NameNode to get the metadata for the file example.jpg.
The NameNode responds with the following details:
Block IDs: Block_1 and Block_2.
DataNode locations for each block:
Block 1: DataNodes A, B, C.
Block 2: DataNodes D, E, F.
Selecting Optimal DataNodes
The HDFS client selects the closest (or least loaded) DataNodes for each block. For example:
Block 1: DataNode A.
Block 2: DataNode D.
Downloading Blocks
The Application Server retrieves each block from the selected DataNodes.
The blocks are streamed directly to the server in chunks.
Reassembling the File
As the blocks are received, the Application Server:
Writes them to a local buffer or temporary storage.
Reassembles the blocks into the complete file example.jpg.
4. Returning the File to the User
Streaming File to the API Gateway
The Application Server streams the reassembled file to the API Gateway.
Sending File to Browser
The API Gateway streams the file to the browser over HTTP.
File Download Complete
The user’s browser prompts them to save the file locally.
How HDFS Handles Data Retrieval in Detail
Block Retrieval from DataNodes
Parallel Fetching:

The HDFS client downloads multiple blocks in parallel for efficiency.
Each block is retrieved from its assigned DataNode.
Fault Tolerance:

If a DataNode (e.g., A for Block 1) is unavailable, the client retries with another replica (e.g., B or C).
Chunk Streaming:

Blocks are streamed in chunks to minimize memory usage.
Data Flow Overview
1. Browser to API Gateway
The user sends an HTTP GET request to download the file.
2. API Gateway to Application Server
The API Gateway forwards the request to the backend for processing.
3. Application Server to HDFS
The Application Server retrieves file metadata from the NameNode.
The blocks are fetched from DataNodes and reassembled into the file.
4. Application Server to API Gateway
The Application Server streams the file back to the API Gateway.
5. API Gateway to Browser
The API Gateway streams the file to the browser for download.
Why This Flow is Efficient
Parallel Block Fetching:

Multiple blocks are retrieved simultaneously, reducing latency.
Fault Tolerance:

DataNode failures don’t affect file availability due to replication.
Streaming:

Blocks and chunks are streamed, ensuring minimal memory overhead.
Seamless Integration:

The API Gateway and Application Server abstract HDFS complexities from the user.


HLD notes on github - 

https://github.com/KingsGambitLab/Lecture_Notes/tree/master/Non-DSA%20Notes/HLD%20Notes


---------------------------------------

System Design - Microservices 1 


Flipkart - 

Product service
Order service
Payment service
Tracking Service
Search Service
Authentication Service



Initially they started with monolith architecture. all above listed service were module in that project.

Monolith was single point of failure whenever they have to deploy code and other code related things.

and same code was running on all the machines. 

as the traffic grew, they scaled by increasing machines but all the machines are running the same code.

and it was very hard for all the teams to work together on this project and even small single deployment was restarting all the machines one by one.

In monolith all the components are part of the same application.

 Strangler fig pattern - in this monolith and microservices architecture is combined to create the projects.
 
  Basically this pattern is used when converting very big monolith into microservices. They identify the less depedent modules and starts converting them to microservices.
  
  Most of the companies are using this pattern.
  
Monolith - 

Pros - Single deployment, Services can talk to each other via a simple function call (low latency), 

Cons - No tech stack flxibility, a small issue can bring the entire application down, huge deployment time, Understanding codebase is very hard, the traffic on one service might have more traffic than another service,  No selective scalling is possible, not cost effectice in most cases.

Microservices architecture - It says that divide the code base into smaller and individual services.

In there will be multiple executable files and small independant codebases.

most of the projects has microservice and monolith combined to achieve the efficiency.

pros - Individual deployments are feasible, No cascading failures, tech stack, Selective scalling, Developer onboarding is relatively easy, less cascading failure.

Cons -  Managing so many microservices can be very challenging, debugging can be difficult as request tracking is difficult, more latency due to communication between microservices via network, infracture cost can increase. 


API gateway and Load balancer - 

we can use api gateways provided by cloud providers we just have to configure these. 

API Gateway - used to route the traffic to the correct microservice. it can also be used for rate limiting. and it can be used for authentication. 

Mostly rate limiter are placed at the API gateway level .

mostly Authentication is placed at API gateway level.

 
In monolith architecture we can keep the load balancer and api gateway on the same machine and when there is a microservice architecture we need the seperate load balancer for each microservice as each microservice will have more than one machine in it.  


Only api gateway is on public ip address and all the other microservices are on the private ip address, so inside microservices will only accept the traffic from API gateway.

Inbound rules - this is used to configure from where we want to allow the traffic, in this ip addresses are added to allow the traffic. 

Outbound rule - This is used to configure where we will send the response. in this ip addresses are added to allow the traffic.


https://github.com/arpitbbhayani/system-design-questions

https://github.com/KingsGambitLab/Lecture_Notes/blob/master/Non-DSA%20Notes/HLD%20Notes/System%20Design%20-%20Microservices.md

--------------------------------------

System Design : Microservices 2 


Communication between multiple microservices - 

1) HTTP request response model - In this api call is made by one service to another service. 

This is the synchronous way of communication and json object is sent as part of the response. 

On sender side the object is converted to the json it is called as Serialization and on receivers end the object is converted from json to object is called as deserilization.

this converting process takes time. 

2) RPC - remote procedure call - 

gRPC + protobuf -> 

gRPC - google remote procedure call

Protobuff - Data in binary format. data is converted from object to binary and vice versa is done by using proto library. 

This is more optimized than http request response model. 


3) Asynchronous or event driven architecture - queue is used to perform this communication. publisher publish the event and consumer consumes the event . 

Consistency between microservices - 

1) Two phase commit - 

phase one  - are you ready phase or also called as voting phase. - in this loack will be taken on both the entries. 

phase two - just commit the data changes.  if both the services respond with yes then commit will happen at both the places and if any service respond with no then roll back both transaction on both the machines.

This provides highest level of consistency.

This has high latency.  

but in case of swiggy we will have to take a lock on atleast 6 different services and this is not efficient. 

like in swiggy we can not confirm order if payment is not confirm same we can not assign delivery partner before order is prepared by restaurant etc.

in this example two phase commit is not good to use.

for such situations SAGA pattern is used.

SAGA pattern - it is also referred as orderly chaos. 

Different services are going to talk to each other via message queues in between. Service will become decoupled. 

To reduce the delay we need to have more consumers to consume the events, as orders are increasing the consumers are also increased by scaling the consumer services. 


Ex - Order service will put the event in the queue which will read by payment service and once payment is successfull it will put the event in another queue and order service will read from second queue to confirm the order once order is confirmed it will put that event in another queue and that event will be read by the restaurant service. and once restaurant accepts the order it will put this event in another queue and this event will be read by delivery service.



and in case of failure from any service like restaurant declined the order. For failure scenarios we will have another queue and if any service fails it will push the event in particular failure queue of the service. ex if restaurant declines the order then restaurant service will add that event in the failure queue of  restaurant service and and order service will read the event from this queue and process the requests. like this failures will be handled for all the services and each service will have seperate failure or cancelletion queue.   


In this way we can avoid the lock and inefficiency. 

Every service will have the SLA ( service level agreement), so for every api call lot of metrics are monitored once the response time of the service is increases the server will be scaled accordingly to reduce the latency.

Saga pattern is of two types - 

1) Orchestrated
2) Choreoghraph


Service discovery - Eureka service - This maintains how many servers of particular service is running and their ip addresses.


Observability in microservice - This is related to logging and monitoring. 

In microservice architecture the reuqest travels from one service to another service, in this case it becomes challenging to track the request. 


To debug an issue effectively, we need to keep a track of the entire request journey in the right order.  
 
for this unique trace id or correlation id or tracking id is used, this is a unique id that gets associated with the request at begining itself and stays with the request till the end. 


So the log of these requests is stored on the seperate server, and each log will be stored with its trace id.


 ELK - elastic search , log stash , kibana.  
 
 Make sure downtime caused issues are not repeated and make sure to have some automated process to make sure that the code we are pushing has low latency. 
 
 read about CQRS, Circuit breaker pattern. 
   

Microservices 1 - https://github.com/KingsGambitLab/Lecture_Notes/blob/non-dsa/non_dsa/hld/System%20Design%20-%20Microservices%201.pdf


Microservices 2 - https://github.com/KingsGambitLab/Lecture_Notes/blob/non-dsa/non_dsa/hld/System%20Design%20-%20Microservices%202.pdf



--------------------------------------

CrewAI Multi agent course - 

A Large Language Model (LLM) is an advanced deep learning model trained on vast amounts of text data to generate human-like text, understand language, and perform tasks such as text summarization, translation, question-answering, and code generation. LLMs rely on transformer architectures, like GPT (Generative Pre-trained Transformer), to process and generate coherent responses based on input prompts.

Examples of LLMs
GPT-4 (OpenAI)
Claude (Anthropic)
LLaMA (Meta)
Gemini (Google DeepMind)
Mistral (Mistral AI)
Key Features of LLMs
Text-based Processing – Understands and generates natural language.
Pattern Recognition – Learns from large-scale datasets to predict the next words.
No Autonomous Decision-Making – Does not independently take actions beyond text generation.
Context-Limited Memory – Retains input only within a session (unless integrated with external memory).
Definition of AI Agents
An AI Agent is an autonomous system that perceives its environment, makes decisions, and takes actions to achieve a goal. AI agents can interact with users, software, APIs, and physical devices, leveraging LLMs, rule-based logic, and reinforcement learning to complete tasks dynamically.

Examples of AI Agents
AutoGPT – A self-improving agent using GPT to perform multi-step tasks.
BabyAGI – A task-driven AI that autonomously completes objectives.
Virtual Assistants (e.g., Siri, Google Assistant, Alexa) – Respond to voice commands and execute tasks.
AI-powered Chatbots – Can call APIs, fetch data, and provide intelligent responses.
Key Features of AI Agents
Goal-Oriented – Acts towards achieving a specific task.
Interacts with Systems – Calls APIs, executes code, and performs web searches.
Can Use LLMs – May integrate with LLMs for natural language processing.
Decision-Making Ability – Uses logic, learning models, and memory to act autonomously.
Key Difference: LLMs vs. AI Agents
LLMs generate text-based responses but do not act independently.
AI Agents can use LLMs but also perform actions, call APIs, and automate workflows.

----------------------------------

Udacity
Codecrafters
Kaggle

---------------------------------------

Version control systems and how git helps in development flow 

What is VCS -

A Version Control System (VCS) is a tool that helps track and manage changes to code, documents, or any digital files over time. It allows multiple people to collaborate efficiently, maintain different versions, and revert to previous versions if needed.

Production servers runs the linux operating system.

VCS will helps in rollback.

Types of VCS - 

1) Centralised - all versions will be saved on one central server.

This is single point of failure.
But in this system we have to stay connected to the server continuously and not suitable for high latency.

ex - perfoce, sub version.

Google uses perforce.
 
2) Distributed - 

Git is an example distributed vcs.

In this mulitple copies of code or versions present at multiple places, and we can work in offline fashion.

-----------------------------------

Lecture : Learning git commands 

git clone <repo url>

In each repository of the code the will be .git directory, if we delete this repo then we will not be able to run the git commands on the repository.

git branch -

to create branch - git branch new_branch_name

to checkout in branch - git checkout branch_name

to get the available branch names - git branch

when you create a new branch called "test" from master branch, the new branch will inherit the commit history of master branch up to that point. 

to create new branch and checkout in it right away - git checkout -b test4
above will create a new branch and checkout in it.


git status - this command gives modified files and not yet staged, staged files ( ready to commit), untracked files (new files not yet added to git), branch information ( current branch, ahead or behind status)


unstaging the file - git restore --staged file_name_to_unstage

git log - this command gives us commit history with commit hash id, author name, date and time of commit and commit message.

To push your test branch to the remote repository in Git, follow these steps:

Step 1: Switch to the test branch
If you're not already on the test branch:

git checkout test
Step 2: Push the test branch to the remote

git push origin test
 origin → The default name for your remote repository.
 test → The branch you want to push.

Step 3: Set the test branch to track the remote branch (optional but recommended)
If this is the first time you're pushing the branch, you can set it to track the remote branch using:

git push -u origin test
 The -u flag sets the test branch to track the remote branch automatically.
 After this, you can simply run git push without specifying the branch name for future pushes.

Step 4: Verify the branch on the remote

To confirm the branch was pushed successfully:

git branch -r

This lists all remote branches.

Example Output

origin/master
origin/test

Pull Request - 

A pull request (PR) is a process in Git that lets you notify team members about changes you've made in a branch. It's commonly used in collaborative development to review, discuss, and merge changes into the main codebase.

🔹 What is a Pull Request (PR)?
A pull request is a way to propose changes from one branch (e.g., feature/test) into another branch (e.g., master or main).

In simple terms:
➡️ "I have made some changes. Please review them and merge them if everything looks good."

🔹 How Does a Pull Request Work?
Step 1: Create a Branch
Suppose you're adding a new feature or fixing a bug.
Create a new branch (e.g., feature/test) from the main branch.
bash
Copy
Edit
git checkout -b feature/test
Step 2: Make Changes & Commit
Add your code changes and commit them.
bash
Copy
Edit
git add .
git commit -m "Added feature X"
Step 3: Push the Branch to Remote
bash
Copy
Edit
git push origin feature/test
Step 4: Create a Pull Request
Go to your repository on platforms like GitHub, GitLab, or Bitbucket.
Navigate to the "Pull Requests" section.
Click on "New Pull Request".
Select:
Base branch → The branch where changes will be merged (e.g., main or master).
Compare branch → The branch with your changes (e.g., feature/test).
Step 5: Code Review
Other developers will review your code.
They may add comments, request changes, or approve the PR.
Step 6: Merge the PR
Once approved, your changes will be merged into the base branch.

🔹 Important Concepts in Pull Requests
✅ Base Branch

The branch where changes will be merged (commonly main or master).
✅ Compare Branch

The branch that contains your new changes (e.g., feature/test).
✅ Code Review

Team members review your code to ensure quality and standards are met.
✅ Merge Conflict

Happens when changes in both branches affect the same lines of code. You'll need to resolve these conflicts before merging.
✅ Draft Pull Request

Used for unfinished work. It signals that the PR is not yet ready for review.
✅ Close Pull Request

If the proposed changes are no longer needed, you can close the PR without merging.
✅ Squash and Merge / Rebase and Merge

Different merging strategies for cleaner commit history.
🔹 Best Practices for Pull Requests
✅ Keep PRs small and focused on a single task or feature.
✅ Write clear titles and detailed descriptions for your PRs.
✅ Request specific reviewers based on their expertise.
✅ Use meaningful commit messages to explain your changes.
✅ Regularly fetch changes from the base branch to avoid conflicts.

🔹 Example Scenario
Imagine your repo has a master branch. You create a new branch called feature/login.

Create a branch:


git checkout -b feature/login
Add changes, commit, and push:

git add .
git commit -m "Added login feature"
git push origin feature/login
Create a pull request:

Base branch: master
Compare branch: feature/login
After review and approval, the feature/login branch is merged into master.


git pull - to get the remote changes to local.

by default it will fetch and merge changes from the remote branch linked to current branch at local. and it will not fetch the other branches unless specified.

to reset the latest commit -> git reset --hard HEAD~1

This will remove the commit and the changes are also removed from file.

git reset --soft HEAD~1

above command will only remove the commit and changes in file will remain as it is.

git reset --mixed HEAD~1

git reset --mixed HEAD~1
This command does the following:

✅ Moves the HEAD pointer back by one commit (reverts the last commit).
✅ Keeps your changes in the working directory (untracked but not staged).
✅ Unstages the changes that were in the last commit.

🔹 Example Scenario
Suppose your commit history looks like this:


commit A (HEAD -> master)
commit B
commit C

If you run:

git reset --mixed HEAD~1
Result:

commit A is removed from the commit history.
The changes from commit A are still in your working directory but unstaged.
State after the command:


Unstaged changes (previous commit A's changes)
commit B (HEAD -> master)
commit C

When to Use Each Version?

git reset --mixed HEAD~1
→ Removes the last commit, unstages changes, and keeps files in your working directory.

git reset --soft HEAD~1
→ Removes the last commit but keeps all changes staged, ready for a new commit.

git reset --hard HEAD~1
→ ⚠️ Completely deletes the last commit and all its changes — no recovery.


Merge and Merge conflict - 

Recommendation - before merging anything into main branch rebase your branch in main.

steps - 

checkout to your branch.

git rebase main_branch_name

above steps will rebase your branch with main branch.


1. Git Merge
Merging in Git is used to combine changes from different branches into a single branch. It is commonly used in collaborative development where multiple developers work on different features or fixes.

Example Scenario:
You have two branches:

main (stable branch)
feature-branch (development branch)
To merge feature-branch into main, you use:

sh
Copy
Edit
git checkout main  
git merge feature-branch  
If there are no conflicting changes, Git will automatically merge the changes.

2. Merge Conflict
A merge conflict occurs when Git cannot automatically resolve differences between two branches because the same lines in the same file were modified differently in each branch.

Example:

In main:
java
Copy
Edit
System.out.println("Hello, World!");
In feature-branch:
java
Copy
Edit
System.out.println("Hello, Git!");
If you merge feature-branch into main, Git cannot decide which version to keep, so it creates a merge conflict.

Resolving Merge Conflict:

Git marks conflicts in the file:
java
Copy
Edit
<<<<<<< HEAD
System.out.println("Hello, World!");
=======
System.out.println("Hello, Git!");
>>>>>>> feature-branch
Manually edit the file to keep the correct version.
Stage the resolved file:
sh
Copy
Edit
git add filename
Complete the merge:
sh
Copy
Edit
git commit -m "Resolved merge conflict"
Git Rebase and When to Use It
1. What is Git Rebase?
Rebasing is an alternative to merging that moves a branch to a new base commit, replaying its commits on top of another branch. This keeps the commit history cleaner by avoiding unnecessary merge commits.

Example Scenario:

You created a feature-branch from main.
main has new commits since you started working.
Instead of merging, you want to apply your changes on top of the latest main.
Steps to Rebase:

sh
Copy
Edit
git checkout feature-branch  
git rebase main  
This replays all commits of feature-branch on top of main, making the history linear.

2. When to Use Rebase?
✅ Before merging a feature branch to ensure it includes the latest changes from main without creating a merge commit.
✅ To keep a clean history without unnecessary merge commits.
✅ When working alone and you want to maintain a linear commit history.

⚠️ When NOT to use Rebase:
❌ If the branch is shared with others, avoid rebasing because it rewrites history, making collaboration confusing.

Merge vs. Rebase: Key Differences
Feature	Merge	Rebase
History	Creates a merge commit	Rewrites commit history
Conflict Resolution	During merge	During rebase
Use Case	Preserves commit history	Keeps history linear
Safe for Collaboration?	Yes	No (if branch is shared)
👍 Use merge for collaboration.
👍 Use rebase for clean history before merging.

1. Rebase Before Merge (Recommended for Clean History)
Pros:
✅ Keeps a linear, clean history without extra merge commits.
✅ Ensures the feature branch is up to date before merging.
✅ Makes git log easier to read.

Cons:
⚠️ Rewrites commit history, which can be risky in shared branches.
⚠️ If conflicts occur, you must resolve them during the rebase.

When to Use:
✔ When working alone or on a private branch.
✔ Before merging a feature branch into main to keep history clean.

Steps to Follow:

sh
Copy
Edit
git checkout feature-branch  
git rebase main  # Rebase feature branch onto main  
git checkout main  
git merge feature-branch  # Fast-forward merge (no extra merge commit)
git push origin main  
2. Merge Directly (Recommended for Team Collaboration)
Pros:
✅ Safer for shared branches (no history rewriting).
✅ Keeps track of exactly when branches were merged.
✅ Easier to debug since merge commits show where changes came from.

Cons:
⚠️ Creates extra merge commits, making history slightly messier.
⚠️ Merge conflicts may happen if the branches diverge.

When to Use:
✔ When working with a team on a shared branch.
✔ When you want to preserve a full history of merges.

Steps to Follow:

git checkout main  
git merge feature-branch  
git push origin main  

My Recommendation:

If working alone → Rebase before merging for a clean history.
If working with a team → Merge directly to avoid rewriting history.



Git Stash -


git stash is a command in Git that allows you to temporarily save changes that are not yet committed, so you can switch branches, pull updates, or perform other operations without losing your work. The stashed changes can be reapplied later.

Real-World Scenario Where git stash is Useful
Scenario 1: Interruptions for Urgent Fixes
Imagine you are working on a new feature (feature-branch), but suddenly, an urgent bug needs to be fixed in the main branch. However, your current working directory has uncommitted changes. Instead of committing half-done work, you can stash your changes, switch to main, fix the bug, push the fix, then come back and restore your work.

Scenario 2: Syncing with Latest Code
You're working on a branch, and before pushing your changes, you need to pull the latest updates from develop. But Git warns about merge conflicts because of local uncommitted changes. You can stash them, pull the latest code, then apply your stashed changes back.

Common git stash Commands
Save Uncommitted Changes

sh
Copy
Edit
git stash
or with a custom message:

sh
Copy
Edit
git stash push -m "Work in progress on login feature"
View Stashed Items

sh
Copy
Edit
git stash list
Output:

pgsql
Copy
Edit
stash@{0}: WIP on feature-branch: 123abc Login feature work
stash@{1}: WIP on main: 456def Bug fix
Apply Stashed Changes

sh
Copy
Edit
git stash apply
or apply a specific stash:

sh
Copy
Edit
git stash apply stash@{1}
Remove Stash After Applying

sh
Copy
Edit
git stash pop
This restores the last stashed changes and removes them from the stash list.

Delete a Specific Stash

git stash drop stash@{0}
Clear All Stashed Changes


git stash clear
Best Practices
✅ Use stash when you need to switch branches but don’t want to commit incomplete work.
✅ Always provide a meaningful message when stashing (git stash push -m "WIP on feature X") for better tracking.
✅ Use git stash apply if you want to keep the stash for future use, and git stash pop if you don’t need it anymore.
✅ Avoid excessive stashing without applying, as it can clutter your stash list.

------------------------------------

Lecture - APIs and REST and how to code APIs.

API - application programming interface - API is a contract to connect to web application. 

API (Application Programming Interface) is a set of rules, protocols, and tools that allow different software applications to communicate with each other. It defines how requests and responses should be structured between systems.

Key Components of an API -

Endpoints – URLs where the API is accessible.

Methods (HTTP Verbs) – Define actions like:
GET → Retrieve data
POST → Create new data
PUT/PATCH → Update data, put is for replacement and patch is for update.
DELETE → Remove data

Request & Response Format – Typically JSON or XML.

Authentication – API keys, OAuth, JWT, etc., for secure access.

REST APIS - 

Best practices - 
 
REST APIS should be stateless.

REST APIs should have relation with N tables instead of only one. create seperate table for each entity instead of storing all the data in the one single table.

REST APIs should talk in terms of JSON or XML.

Resource based endpoints.

What is REST?

REST (Representational State Transfer) is an architectural style for designing networked applications. It is based on a set of principles that define how web services should work by using standard HTTP methods (GET, POST, PUT, DELETE, etc.) and stateless communication. RESTful APIs allow clients to interact with a server using standard web technologies.

Best Practices for RESTful APIs
1. Use Meaningful Resource URIs
Use nouns instead of verbs in URLs.
✅ GET /users (Good)
❌ GET /getUsers (Bad)
Use plural nouns for collections.
✅ GET /products (List all products)
✅ GET /products/123 (Retrieve product with ID 123)
2. Use the Correct HTTP Methods
GET → Retrieve data (Safe and Idempotent)
POST → Create a new resource
PUT → Update a resource (Replace existing data)
PATCH → Partially update a resource
DELETE → Remove a resource
3. Use Proper Status Codes
200 OK → Successful request
201 Created → Resource successfully created
204 No Content → Request successful but no response body
400 Bad Request → Invalid request from client
401 Unauthorized → Authentication required
403 Forbidden → Client has no access rights
404 Not Found → Resource does not exist
500 Internal Server Error → Unexpected server failure
4. Use Query Parameters for Filtering, Sorting, and Pagination
Filtering: /products?category=electronics
Sorting: /products?sort=price,desc
Pagination: /products?page=2&size=10
5. Use JSON as the Response Format
JSON is widely used because it is lightweight and easy to parse.
Always include Content-Type: application/json in responses.
6. Implement Authentication and Authorization
Use JWT (JSON Web Tokens) for authentication.
Use OAuth 2.0 for API security when dealing with third-party integrations.
7. Handle Errors Properly with Clear Messages
Send structured error responses:


{
  "error": "Invalid request",
  "message": "Product ID must be a number",
  "status": 400
}

8. Version Your API
Add versioning to API URLs to avoid breaking changes.
✅ /api/v1/products

9. Use HATEOAS (Optional for Advanced REST APIs)
Hypermedia as the Engine of Application State (HATEOAS) helps clients navigate APIs dynamically.

Build tools - 

It createes a graph of all the dependancies and maven creates linked list of all the dependancies.

Maven - 

Maven users XML based configuration to define project structure, dependencies and lifecycle.

fearures - dependencies management, Standardized directory structure, Lifecycle management. 

Gradle - 

this uses Groovy or kotlin instead of XML. This is more efficient than maven.

features - 

Performance optimization - uses incremental builds and caching.

We can configure in groovy or Kotlin.

Better suited for microservices and modulaer projects, and it supports the plugin system more customizable than maven.


Creating spring boot application - 

Once spring boot project is created then first create models, then controllers, 

@RestController - for controllers.

@Service - for service.

@Component - for models.


@RestController - it is advance version of the @Controller annotation. IT is used to create restful web services and automatically serializes the response data into JSON or XML format.

It combines @Controller and @ResponseBody.

It tells spring that this class will handle restful api requests.

It is used to define controller that handles HTTP requests and return data instead of a view and data is automatically serialized into JSON or XML using jackson ( for json) or JAXB (for XML).

without @ResponseBody, spring assumes you want to return view and @RestController serializes objects to JSON and sends them in the response body.

example - 

@RestController  // Marks this class as a REST Controller
@RequestMapping("/users") // Base path for all endpoints in this controller
public class UserController {

    @GetMapping("/hello")
    public String sayHello() {
        return "Hello, Welcome to Spring Boot REST API!";
    }
}


@RequestMapping("/users") - this defines the base path for all endpoints inside this controller.

@GetMApping("/hello") maps the /users/hello endpoint to the sayHello() method.

when we call GET for /users/hello we will get below output - 

"Hello, Welcome to Spring Boot REST API!"



user controller which returns JSON - 

@RestController
@RequestMapping("/users")
public class UserController {

    @GetMapping("/profile")
    public User getUserProfile() {
        return new User(1, "John Doe", "john.doe@example.com");
    }
}

User class - 


public class User {
    private int id;
    private String name;
    private String email;

    // Constructor
    public User(int id, String name, String email) {
        this.id = id;
        this.name = name;
        this.email = email;
    }

    // Getters
    public int getId() { return id; }
    public String getName() { return name; }
    public String getEmail() { return email; }
}


output while calling GET /users/profile - 

{
    "id": 1,
    "name": "John Doe",
    "email": "john.doe@example.com"
}


Spring automatically converts Java objects into JSON.

GetMapping("/hello") - maps http get request to a specific method.

@PostMapping("/hello") - maps that post request to a specific method.

@PutMapping("/update") - maps HTTP put requests to a method.

@DeleteMapping("/delete/{id}") - Maps http delete request to a method.

CRUD example - 

@RestController
@RequestMapping("/users")
public class UserController {

    private List<User> users = new ArrayList<>();

    // CREATE (POST)
    @PostMapping("/add")
    public String addUser(@RequestBody User user) {
        users.add(user);
        return "User added successfully!";
    }

    // READ (GET)
    @GetMapping
    public List<User> getAllUsers() {
        return users;
    }

    // UPDATE (PUT)
    @PutMapping("/update/{id}")
    public String updateUser(@PathVariable int id, @RequestBody User updatedUser) {
        for (User user : users) {
            if (user.getId() == id) {
                users.remove(user);
                users.add(updatedUser);
                return "User updated successfully!";
            }
        }
        return "User not found!";
    }

    // DELETE (DELETE)
    @DeleteMapping("/delete/{id}")
    public String deleteUser(@PathVariable int id) {
        users.removeIf(user -> user.getId() == id);
        return "User deleted successfully!";
    }
}

@Component - 

This is used for dependency injection and automatic bean management, this tells spring that class is spring managed bean and should be automatically detected and registered in spring application context.

This marks the class as spring managed component, it allows component scanning mechanism to automatically detect and register the bean, Then the registered beans can be autowired into other spring managed beans. 

@Component
public class MyComponent {
    public void showMessage() {
        System.out.println("Hello from MyComponent!");
    }
}

In above spring automatically detects MyComponent and registers it as a bean.

Spring scans the package where @Component is used and it registers the class as a bean in the spring IoC container ( Application context), the bean can be injected into other component using @Autowired annotation.

example - 

@SpringBootApplication  // Enables component scanning
public class ComponentExampleApplication {
    public static void main(String[] args) {
        SpringApplication.run(ComponentExampleApplication.class, args);
    }
}

The @SpringBootApplication annotation will enable component scanning in the main application class. It has @ComponentScan so it will automatically detects all @Component classes.

create component - 

@Component  // Marking this class as a Spring component (bean)
public class MessageService {
    public String getMessage() {
        return "Hello from MessageService!";
    }
}

use component  -

@Component
public class UserService {
    private final MessageService messageService;

    @Autowired  // Injecting MessageService
    public UserService(MessageService messageService) {
        this.messageService = messageService;
    }

    public void displayMessage() {
        System.out.println(messageService.getMessage());
    }
}

@ComponentScan to define custom package - 

@SpringBootApplication
@ComponentScan(basePackages = "com.example.custompackage") // Custom package to scan
public class ComponentExampleApplication {
    public static void main(String[] args) {
        SpringApplication.run(ComponentExampleApplication.class, args);
    }
}

@ComponentScan tells the spring to scan the provided package for beans.

there are two ways define the spring beans - 

1) @Component
2) using @Bean manual bean registration in a @Configuration class - 

@Configuration
public class AppConfig {
    @Bean
    public MyComponent myComponent() {
        return new MyComponent();
    }
}

when manual control is needed to create the objects @Bean is used.

follow the baeldung documentation.


------------------

lecture - Create product controller and APIs.

PRD document link of the project - https://docs.google.com/document/d/1Gn2ib5YhhpcFUiWGAUbCpg0ZPh3m_wSA-9IolGMjkIE/edit?tab=t.0#heading=h.hteovoit9b96



DTO - 

DTO (Data Transfer Object) in a Spring Application - Detailed Explanation
1️⃣ What is a DTO (Data Transfer Object)?
A DTO (Data Transfer Object) is a Java class used to transfer data between different layers of a Spring Boot application (e.g., Controller → Service → Repository).

📌 Why use a DTO?

Encapsulation – Hides database entity details.
Security – Prevents exposing unnecessary fields (e.g., passwords).
Performance – Reduces unnecessary data transfer over the network.
Validation – Enables custom validation for input data.
2️⃣ Example Without DTO (Directly Exposing Entity)
Without a DTO, we expose the Entity class directly in our REST API.

Entity Class (Exposed Directly)


import jakarta.persistence.*;

@Entity
public class User {
    @Id
    @GeneratedValue(strategy = GenerationType.IDENTITY)
    private Long id;
    private String username;
    private String email;
    private String password; // ⚠ Exposing sensitive data

    // Getters and Setters
}
Controller (Returning Entity Directly)


import org.springframework.web.bind.annotation.*;

@RestController
@RequestMapping("/users")
public class UserController {
    private final UserRepository userRepository;

    public UserController(UserRepository userRepository) {
        this.userRepository = userRepository;
    }

    @GetMapping("/{id}")
    public User getUser(@PathVariable Long id) {
        return userRepository.findById(id).orElseThrow();
    }
}
✅ The problem: This exposes passwords and other internal details to API consumers.

3️⃣ Using DTO to Solve the Problem
Instead of returning the Entity, we create a DTO that contains only the required fields.

Step 1: Create a DTO Class
The DTO contains only safe and necessary fields.



public class UserDTO {
    private String username;
    private String email;

    public UserDTO(String username, String email) {
        this.username = username;
        this.email = email;
    }

    // Getters and Setters
}
Step 2: Convert Entity to DTO
Modify the Service Layer to return a DTO instead of an Entity.



import org.springframework.stereotype.Service;
import java.util.Optional;

@Service
public class UserService {
    private final UserRepository userRepository;

    public UserService(UserRepository userRepository) {
        this.userRepository = userRepository;
    }

    public UserDTO getUserById(Long id) {
        User user = userRepository.findById(id).orElseThrow();
        return new UserDTO(user.getUsername(), user.getEmail()); // Convert Entity to DTO
    }
}
Step 3: Update the Controller to Return DTO Instead of Entity


import org.springframework.web.bind.annotation.*;

@RestController
@RequestMapping("/users")
public class UserController {
    private final UserService userService;

    public UserController(UserService userService) {
        this.userService = userService;
    }

    @GetMapping("/{id}")
    public UserDTO getUser(@PathVariable Long id) {
        return userService.getUserById(id);
    }
}
✅ Now, passwords and unnecessary fields are not exposed in the API response.

4️⃣ Automating Entity-to-DTO Conversion with ModelMapper
Manually converting entities to DTOs can be repetitive. ModelMapper automates this.

📌 Add ModelMapper dependency (for Maven users):


<dependency>
    <groupId>org.modelmapper</groupId>
    <artifactId>modelmapper</artifactId>
    <version>3.1.0</version>
</dependency>
📌 Spring Configuration for ModelMapper:



import org.modelmapper.ModelMapper;
import org.springframework.context.annotation.Bean;
import org.springframework.context.annotation.Configuration;

@Configuration
public class ModelMapperConfig {
    @Bean
    public ModelMapper modelMapper() {
        return new ModelMapper();
    }
}
📌 Using ModelMapper in the Service Layer:



import org.modelmapper.ModelMapper;
import org.springframework.stereotype.Service;

@Service
public class UserService {
    private final UserRepository userRepository;
    private final ModelMapper modelMapper;

    public UserService(UserRepository userRepository, ModelMapper modelMapper) {
        this.userRepository = userRepository;
        this.modelMapper = modelMapper;
    }

    public UserDTO getUserById(Long id) {
        User user = userRepository.findById(id).orElseThrow();
        return modelMapper.map(user, UserDTO.class); // Automatic mapping
    }
}
✅ Now, entity-to-DTO conversion happens automatically!

5️⃣ Two-Way Conversion (DTO to Entity)
If you need to convert DTOs back to Entities (e.g., for saving user data):

📌 Create a UserDTO with an additional constructor


public class UserDTO {
    private String username;
    private String email;
    
    public UserDTO() {} // Default constructor

    public UserDTO(User user) { // Convert Entity to DTO
        this.username = user.getUsername();
        this.email = user.getEmail();
    }

    // Getters and Setters
}
📌 Convert DTO to Entity in the Service Layer


public User convertToEntity(UserDTO userDTO) {
    return modelMapper.map(userDTO, User.class);
}
6️⃣ Validating DTOs with @Valid
To ensure input data is valid, use Spring Validation with @Valid.

📌 Add Validation Dependency (Maven)



<dependency>
    <groupId>org.springframework.boot</groupId>
    <artifactId>spring-boot-starter-validation</artifactId>
</dependency>
📌 Modify DTO to Include Validation Annotations


import jakarta.validation.constraints.Email;
import jakarta.validation.constraints.NotBlank;

public class UserDTO {
    @NotBlank(message = "Username is required")
    private String username;

    @Email(message = "Invalid email format")
    private String email;

    // Getters and Setters
}
📌 Modify Controller to Validate Input DTO


@PostMapping("/create")
public ResponseEntity<String> createUser(@Valid @RequestBody UserDTO userDTO) {
    userService.saveUser(userDTO);
    return ResponseEntity.ok("User created successfully");
}
✅ If invalid data is provided, Spring automatically throws validation errors.

7️⃣ Summary
DTO (Data Transfer Object) is used to transfer data between layers without exposing entity details.
Prevents exposing sensitive information (e.g., passwords).
Reduces unnecessary data transfer and improves API performance.
ModelMapper automates entity-to-DTO conversion.
Validation using @Valid ensures correct input data.

ObjectMapper - 


ObjectMapper in DTO Conversion (Spring Boot)
ObjectMapper is a powerful class from the Jackson library used for converting Java objects to JSON and vice versa. In the case of DTO (Data Transfer Object) conversion, it helps in mapping Entity → DTO and DTO → Entity efficiently.

1. Adding Jackson Dependency (If Not Already Added)
If you are using Spring Boot, Jackson is included by default. Otherwise, add it manually:

xml
Copy
Edit
<dependency>
    <groupId>com.fasterxml.jackson.core</groupId>
    <artifactId>jackson-databind</artifactId>
    <version>2.15.0</version>
</dependency>
2. DTO vs. Entity
Before using ObjectMapper, let's assume we have an Entity and a corresponding DTO.

Entity Class (UserEntity.java)
java
Copy
Edit
@Entity
@Getter
@Setter
@NoArgsConstructor
@AllArgsConstructor
public class UserEntity {
    @Id
    @GeneratedValue(strategy = GenerationType.IDENTITY)
    private Long id;
    
    private String name;
    
    private String email;
    
    @Enumerated(EnumType.STRING)
    private Role role; // ENUM field
}
DTO Class (UserDTO.java)
java
Copy
Edit
@Getter
@Setter
@NoArgsConstructor
@AllArgsConstructor
public class UserDTO {
    private Long id;
    private String name;
    private String email;
    private String role; // Keeping it as String for flexibility
}
3. Using ObjectMapper for DTO Conversion
Jackson's ObjectMapper helps convert between UserEntity and UserDTO.

Convert Entity → DTO
java
Copy
Edit
import com.fasterxml.jackson.databind.ObjectMapper;

public UserDTO convertToDTO(UserEntity user) {
    ObjectMapper objectMapper = new ObjectMapper();
    return objectMapper.convertValue(user, UserDTO.class);
}
💡 How it works?

convertValue(user, UserDTO.class) automatically maps matching fields.
The role enum is automatically converted to a String in UserDTO.
Convert DTO → Entity
java
Copy
Edit
public UserEntity convertToEntity(UserDTO userDTO) {
    ObjectMapper objectMapper = new ObjectMapper();
    return objectMapper.convertValue(userDTO, UserEntity.class);
}
💡 How it works?

Fields are mapped automatically.
Since role is a String in DTO, but Role (Enum) in UserEntity, it needs special handling.
4. Handling Enum Conversion in DTO
Since role is a String in UserDTO, but an Enum in UserEntity, you should handle the conversion manually:

java
Copy
Edit
public UserEntity convertToEntity(UserDTO userDTO) {
    ObjectMapper objectMapper = new ObjectMapper();
    UserEntity user = objectMapper.convertValue(userDTO, UserEntity.class);
    user.setRole(Role.valueOf(userDTO.getRole())); // Convert String to Enum
    return user;
}
🚀 This ensures the role is correctly mapped from String to Enum!

5. Using @Autowired ObjectMapper in a Service
Instead of creating a new ObjectMapper instance every time, you can inject it in Spring Boot:

Service Class Example
java
Copy
Edit
import com.fasterxml.jackson.databind.ObjectMapper;
import org.springframework.stereotype.Service;

@Service
public class UserService {
    
    private final ObjectMapper objectMapper;

    public UserService(ObjectMapper objectMapper) {
        this.objectMapper = objectMapper;
    }

    public UserDTO convertToDTO(UserEntity user) {
        return objectMapper.convertValue(user, UserDTO.class);
    }

    public UserEntity convertToEntity(UserDTO userDTO) {
        UserEntity user = objectMapper.convertValue(userDTO, UserEntity.class);
        user.setRole(Role.valueOf(userDTO.getRole())); // Convert String to Enum
        return user;
    }
}
✅ Now ObjectMapper is managed by Spring Boot and is reusable.

6. Alternative: Using ModelMapper (Better for Complex Mappings)
If you have deeply nested DTOs, consider using ModelMapper instead of ObjectMapper:

java
Copy
Edit
<dependency>
    <groupId>org.modelmapper</groupId>
    <artifactId>modelmapper</artifactId>
    <version>3.1.1</version>
</dependency>
Then use:

java
Copy
Edit
ModelMapper modelMapper = new ModelMapper();
UserDTO userDTO = modelMapper.map(userEntity, UserDTO.class);
But for simple cases, ObjectMapper is sufficient! 🚀

Final Thoughts
✅ Use ObjectMapper.convertValue() for simple DTO mappings.
✅ Manually handle Enums (Role.valueOf(userDTO.getRole())).
✅ Inject ObjectMapper in services instead of creating new instances.
✅ For deeply nested DTOs, consider ModelMapper.


-------------------------------

Lecture - Introduction to SpringBoot, MVC , RestTemplate and Exceptions


DAO - Data Access Object.


Web Server
A web server is responsible for handling HTTP requests from clients (browsers, mobile apps, etc.) and serving static content like HTML, CSS, JavaScript, and images. It can also act as a reverse proxy to forward dynamic requests to an application server.

Common Web Servers:
Apache HTTP Server

Nginx

Microsoft IIS

LiteSpeed

Functions of a Web Server:
Handles static content (HTML, CSS, JavaScript, images, etc.)

Supports HTTP protocol for communication

Implements load balancing and reverse proxying

Can serve as a gateway to application servers

Example Usage:
If you visit www.example.com, the web server fetches and serves the HTML page to your browser.

2. Application Server
An application server is responsible for executing business logic and handling dynamic content (e.g., database interactions, user authentication, API responses). It processes requests from the web server and generates dynamic responses.

Common Application Servers:
Apache Tomcat (Java-based)

JBoss/WildFly (Java EE)

WebLogic (Oracle)

WebSphere (IBM)

Node.js (JavaScript runtime)

Functions of an Application Server:
Executes business logic (e.g., processing orders, user authentication)

Handles dynamic content generation (e.g., JSP, Servlets, ASP.NET)

Manages database connections and APIs

Supports transactions and session management

Provides enterprise-level services like security and clustering

Example Usage:
When a user logs in, the web server forwards the request to the application server, which checks credentials in the database and returns a session token.


How They Work Together
In a typical three-tier architecture:

Web Server receives the request (GET /products)

If it's a static file (CSS, JS), the web server serves it.

If it's a dynamic request (/login), the web server forwards it to the Application Server.

The Application Server processes the request (e.g., queries a database) and sends a response back.

The Web Server delivers the final response to the client.

Example Stack in Java:
Web Server: Nginx (serves React frontend)

Application Server: Apache Tomcat (handles Java backend)

Database: PostgreSQL/MySQL (stores user data)

------------------------------------------------------

Lecture | Integrating 3rd Party APIs


Rest template - 

RestTemplate in Spring Boot
RestTemplate is a synchronous HTTP client provided by Spring for making REST API calls from a Spring application. It allows you to send HTTP requests and handle responses in various formats.

Note: RestTemplate is now deprecated in favor of WebClient (introduced in Spring WebFlux). However, it's still widely used in legacy projects.

Basic Methods in RestTemplate
getForObject(url, responseType): Fetch an object from the given URL.

getForEntity(url, responseType): Fetch an entity with HTTP response details.

postForObject(url, request, responseType): Send a POST request and get an object response.

postForEntity(url, request, responseType): Send a POST request and get full response details.

put(url, request): Update a resource.

delete(url): Delete a resource.

exchange(url, method, request, responseType): More customizable, allowing different HTTP methods.

CRUD Operations Using RestTemplate
1. Setup RestTemplate Bean
java
Copy
Edit
import org.springframework.context.annotation.Bean;
import org.springframework.context.annotation.Configuration;
import org.springframework.web.client.RestTemplate;

@Configuration
public class RestTemplateConfig {
    @Bean
    public RestTemplate restTemplate() {
        return new RestTemplate();
    }
}
2. Example Model (User)
java
Copy
Edit
public class User {
    private Long id;
    private String name;
    private String email;

    // Getters and Setters
}
3. Service for CRUD Operations
java
Copy
Edit
import org.springframework.http.*;
import org.springframework.stereotype.Service;
import org.springframework.web.client.RestTemplate;

import java.util.Arrays;
import java.util.List;

@Service
public class UserService {
    private final RestTemplate restTemplate;
    private final String BASE_URL = "http://localhost:8080/api/users";

    public UserService(RestTemplate restTemplate) {
        this.restTemplate = restTemplate;
    }

    // 1. Create a User (POST)
    public User createUser(User user) {
        return restTemplate.postForObject(BASE_URL, user, User.class);
    }

    // 2. Get a User by ID (GET)
    public User getUserById(Long id) {
        return restTemplate.getForObject(BASE_URL + "/" + id, User.class);
    }

    // 3. Get All Users (GET)
    public List<User> getAllUsers() {
        User[] users = restTemplate.getForObject(BASE_URL, User[].class);
        return Arrays.asList(users);
    }

    // 4. Update a User (PUT)
    public void updateUser(Long id, User user) {
        restTemplate.put(BASE_URL + "/" + id, user);
    }

    // 5. Delete a User (DELETE)
    public void deleteUser(Long id) {
        restTemplate.delete(BASE_URL + "/" + id);
    }

    // 6. Using exchange() method
    public ResponseEntity<User> getUserUsingExchange(Long id) {
        HttpHeaders headers = new HttpHeaders();
        headers.set("Accept", MediaType.APPLICATION_JSON_VALUE);
        HttpEntity<String> entity = new HttpEntity<>(headers);
        
        return restTemplate.exchange(BASE_URL + "/" + id, HttpMethod.GET, entity, User.class);
    }
}

Spring’s RestTemplate provides exchange(), getForEntity(), postForEntity(), and similar methods that return a ResponseEntity<T>. These methods provide more details about the HTTP response, such as status codes and headers.

1. Setup RestTemplate Bean
java
Copy
Edit
import org.springframework.context.annotation.Bean;
import org.springframework.context.annotation.Configuration;
import org.springframework.web.client.RestTemplate;

@Configuration
public class RestTemplateConfig {
    @Bean
    public RestTemplate restTemplate() {
        return new RestTemplate();
    }
}
2. Example Model (User)
java
Copy
Edit
public class User {
    private Long id;
    private String name;
    private String email;

    // Constructors
    public User() {}

    public User(String name, String email) {
        this.name = name;
        this.email = email;
    }

    // Getters and Setters
}
3. Service with Entity Methods
java
Copy
Edit
import org.springframework.http.*;
import org.springframework.stereotype.Service;
import org.springframework.web.client.RestTemplate;

import java.util.Arrays;
import java.util.List;

@Service
public class UserService {
    private final RestTemplate restTemplate;
    private final String BASE_URL = "http://localhost:8080/api/users";

    public UserService(RestTemplate restTemplate) {
        this.restTemplate = restTemplate;
    }

    // 1. Create a User (POST)
    public ResponseEntity<User> createUser(User user) {
        HttpHeaders headers = new HttpHeaders();
        headers.setContentType(MediaType.APPLICATION_JSON);
        HttpEntity<User> requestEntity = new HttpEntity<>(user, headers);

        return restTemplate.exchange(BASE_URL, HttpMethod.POST, requestEntity, User.class);
    }

    // 2. Get a User by ID (GET)
    public ResponseEntity<User> getUserById(Long id) {
        HttpHeaders headers = new HttpHeaders();
        headers.setAccept(Arrays.asList(MediaType.APPLICATION_JSON));
        HttpEntity<String> requestEntity = new HttpEntity<>(headers);

        return restTemplate.exchange(BASE_URL + "/" + id, HttpMethod.GET, requestEntity, User.class);
    }

    // 3. Get All Users (GET)
    public ResponseEntity<List<User>> getAllUsers() {
        HttpHeaders headers = new HttpHeaders();
        headers.setAccept(Arrays.asList(MediaType.APPLICATION_JSON));
        HttpEntity<String> requestEntity = new HttpEntity<>(headers);

        ResponseEntity<User[]> responseEntity = restTemplate.exchange(BASE_URL, HttpMethod.GET, requestEntity, User[].class);
        List<User> users = Arrays.asList(responseEntity.getBody());

        return new ResponseEntity<>(users, responseEntity.getStatusCode());
    }

    // 4. Update a User (PUT)
    public ResponseEntity<Void> updateUser(Long id, User user) {
        HttpHeaders headers = new HttpHeaders();
        headers.setContentType(MediaType.APPLICATION_JSON);
        HttpEntity<User> requestEntity = new HttpEntity<>(user, headers);

        return restTemplate.exchange(BASE_URL + "/" + id, HttpMethod.PUT, requestEntity, Void.class);
    }

    // 5. Delete a User (DELETE)
    public ResponseEntity<Void> deleteUser(Long id) {
        HttpHeaders headers = new HttpHeaders();
        HttpEntity<String> requestEntity = new HttpEntity<>(headers);

        return restTemplate.exchange(BASE_URL + "/" + id, HttpMethod.DELETE, requestEntity, Void.class);
    }
}

----------------------------------

Lecture | Introduction to JPA and connecting Db

JPA connects to the database by using driver called JDBC.

JDBC is an interface and with help of its implementations we can connect to different types of databases like mysql connector, postgres connector etc.



ORM - 

What is ORM?
ORM (Object-Relational Mapping) is a programming technique that allows developers to interact with a relational database using object-oriented programming concepts instead of SQL queries. It maps database tables to Java classes, making database interactions more intuitive and reducing the need for complex SQL statements.

Benefits of ORM
Abstraction – Hides SQL queries behind method calls.

Productivity – Reduces boilerplate code for database interactions.

Portability – Makes applications database-independent.

Performance Optimization – Uses caching and lazy loading to improve performance.

Automatic Schema Generation – Can generate database tables automatically.

JPA (Java Persistence API) in Detail
What is JPA?
JPA (Java Persistence API) is a specification for ORM in Java. It provides a standard way to manage relational data using Java objects. JPA itself is just a specification; it requires an implementation like Hibernate, EclipseLink, or OpenJPA to work.

Key Components of JPA
Entity – Represents a table in the database.

Entity Manager – Manages entity objects and provides CRUD operations.

Persistence Context – A set of managed entity instances associated with a persistence unit.

JPQL (Java Persistence Query Language) – Used to query database entities in an object-oriented way.

Transactions – Handled using the @Transactional annotation or EntityTransaction in JPA.

JPA Annotations with Examples
1. Entity Class
Each entity maps to a database table.

java
Copy
Edit
import jakarta.persistence.*;

@Entity
@Table(name = "users")
public class User {
    @Id
    @GeneratedValue(strategy = GenerationType.IDENTITY)
    private Long id;

    @Column(name = "username", nullable = false, unique = true)
    private String username;

    @Column(name = "email", nullable = false)
    private String email;

    // Getters and setters
}
2. Repository Layer (Spring Data JPA Example)
Spring Data JPA simplifies CRUD operations.

java
Copy
Edit
import org.springframework.data.jpa.repository.JpaRepository;

public interface UserRepository extends JpaRepository<User, Long> {
    User findByUsername(String username);
}
3. Service Layer
Handles business logic.

java
Copy
Edit
import org.springframework.stereotype.Service;
import java.util.List;

@Service
public class UserService {
    private final UserRepository userRepository;

    public UserService(UserRepository userRepository) {
        this.userRepository = userRepository;
    }

    public List<User> getAllUsers() {
        return userRepository.findAll();
    }
}
4. Using EntityManager for Custom Queries
java
Copy
Edit
import jakarta.persistence.EntityManager;
import jakarta.persistence.PersistenceContext;
import org.springframework.stereotype.Repository;

@Repository
public class UserDao {
    @PersistenceContext
    private EntityManager entityManager;

    public User getUserByEmail(String email) {
        return entityManager.createQuery("SELECT u FROM User u WHERE u.email = :email", User.class)
                            .setParameter("email", email)
                            .getSingleResult();
    }
}
Lifecycle of JPA Entities
New (Transient) – Object created but not associated with the database.

Managed (Persistent) – Object is managed by JPA and associated with a database entry.

Detached – Object is no longer managed but still exists in memory.

Removed – Object is marked for deletion.

Hibernate - 

What is Hibernate?
Hibernate is an Object-Relational Mapping (ORM) framework for Java that simplifies database interactions by mapping Java objects to database tables. It eliminates the need for writing complex SQL queries and provides an abstraction layer over JDBC.

Key Features of Hibernate
✔ ORM Support – Maps Java classes to database tables.
✔ HQL (Hibernate Query Language) – Allows querying using an object-oriented approach.
✔ Automatic Schema Generation – Can create, update, and validate database tables automatically.
✔ Caching – Provides first-level and second-level caching to improve performance.
✔ Lazy & Eager Loading – Optimizes data fetching strategies.
✔ Transaction Management – Ensures data consistency with ACID compliance.
✔ Database Independence – Supports multiple databases without code changes.

How Hibernate Works?
Configuration – Load Hibernate configurations (hibernate.cfg.xml or persistence.xml in JPA).

Session Factory – A heavyweight object that manages database connections.

Session – Lightweight, used for performing CRUD operations.

Transaction Management – Ensures data consistency.

Query Execution – Uses HQL or Criteria API for retrieving data.

Architecture of Hibernate
SessionFactory – A factory for Session objects, created once per application.

Session – Represents a unit of work with the database (like JDBC Connection).

Transaction – Manages database transactions.

Query & Criteria API – Provides ways to query data.

Cache – Stores frequently used objects to enhance performance.

Hibernate Configuration
1. Configure Hibernate (hibernate.cfg.xml)
xml
Copy
Edit
<hibernate-configuration>
    <session-factory>
        <property name="hibernate.connection.driver_class">com.mysql.cj.jdbc.Driver</property>
        <property name="hibernate.connection.url">jdbc:mysql://localhost:3306/mydb</property>
        <property name="hibernate.connection.username">root</property>
        <property name="hibernate.connection.password">password</property>
        <property name="hibernate.dialect">org.hibernate.dialect.MySQL8Dialect</property>
        <property name="hibernate.show_sql">true</property>
        <property name="hibernate.hbm2ddl.auto">update</property>
    </session-factory>
</hibernate-configuration>
✅ hibernate.hbm2ddl.auto options:

create – Drops and creates the schema on startup.

update – Updates schema without deleting existing data.

validate – Ensures schema matches but doesn’t modify it.

none – Disables automatic schema management.

Mapping Java Class to Database Table
2. Create an Entity Class
java
Copy
Edit
import jakarta.persistence.*;

@Entity
@Table(name = "users")
public class User {
    @Id
    @GeneratedValue(strategy = GenerationType.IDENTITY)
    private Long id;

    @Column(name = "username", nullable = false, unique = true)
    private String username;

    @Column(name = "email", nullable = false)
    private String email;

    // Getters and setters
}
Perform CRUD Operations with Hibernate
3. Create SessionFactory & Session
java
Copy
Edit
import org.hibernate.Session;
import org.hibernate.SessionFactory;
import org.hibernate.Transaction;
import org.hibernate.cfg.Configuration;

public class HibernateUtil {
    private static final SessionFactory sessionFactory = new Configuration().configure().buildSessionFactory();

    public static Session getSession() {
        return sessionFactory.openSession();
    }
}
4. Insert Data (Save a User)
java
Copy
Edit
public class UserDao {
    public void saveUser(User user) {
        Session session = HibernateUtil.getSession();
        Transaction transaction = session.beginTransaction();
        
        session.save(user);
        
        transaction.commit();
        session.close();
    }
}
5. Retrieve Data (Get a User by ID)
java
Copy
Edit
public class UserDao {
    public User getUserById(Long id) {
        Session session = HibernateUtil.getSession();
        User user = session.get(User.class, id);
        session.close();
        return user;
    }
}
6. Update Data
java
Copy
Edit
public void updateUser(Long id, String newEmail) {
    Session session = HibernateUtil.getSession();
    Transaction transaction = session.beginTransaction();

    User user = session.get(User.class, id);
    if (user != null) {
        user.setEmail(newEmail);
        session.update(user);
    }

    transaction.commit();
    session.close();
}
7. Delete Data
java
Copy
Edit
public void deleteUser(Long id) {
    Session session = HibernateUtil.getSession();
    Transaction transaction = session.beginTransaction();

    User user = session.get(User.class, id);
    if (user != null) {
        session.delete(user);
    }

    transaction.commit();
    session.close();
}
Hibernate Query Language (HQL)
HQL is an object-oriented query language similar to SQL but operates on entity classes instead of tables.

Example HQL Queries
java
Copy
Edit
import org.hibernate.query.Query;

public List<User> getAllUsers() {
    Session session = HibernateUtil.getSession();
    Query<User> query = session.createQuery("FROM User", User.class);
    List<User> users = query.list();
    session.close();
    return users;
}
java
Copy
Edit
public User getUserByUsername(String username) {
    Session session = HibernateUtil.getSession();
    Query<User> query = session.createQuery("FROM User WHERE username = :username", User.class);
    query.setParameter("username", username);
    User user = query.uniqueResult();
    session.close();
    return user;
}
Hibernate Caching
1. First-Level Cache (Enabled by Default)
Each session maintains a cache to store retrieved objects.

2. Second-Level Cache (Shared Across Sessions)
Hibernate supports caching using providers like EhCache, Infinispan, or Redis.

Example configuration in hibernate.cfg.xml:

xml
Copy
Edit
<property name="hibernate.cache.use_second_level_cache">true</property>
<property name="hibernate.cache.region.factory_class">org.hibernate.cache.ehcache.EhCacheRegionFactory</property>
Lazy vs Eager Loading
Lazy Loading (Default): Data is loaded only when accessed.

java
Copy
Edit
@OneToMany(mappedBy = "user", fetch = FetchType.LAZY)
private List<Order> orders;
Eager Loading: Data is loaded immediately.

java
Copy
Edit
@OneToMany(mappedBy = "user", fetch = FetchType.EAGER)
private List<Order> orders;
Spring Boot Integration
Spring Boot simplifies Hibernate configuration using Spring Data JPA.

Steps to Integrate Hibernate with Spring Boot
Add dependencies in pom.xml:



<dependency>
    <groupId>org.springframework.boot</groupId>
    <artifactId>spring-boot-starter-data-jpa</artifactId>
</dependency>
<dependency>
    <groupId>mysql</groupId>
    <artifactId>mysql-connector-java</artifactId>
</dependency>
Configure application.properties:


spring.datasource.url=jdbc:mysql://localhost:3306/mydb
spring.datasource.username=root
spring.datasource.password=password
spring.jpa.hibernate.ddl-auto=update
spring.jpa.show-sql=true
Use JpaRepository for easy data access:

java
Copy
Edit
public interface UserRepository extends JpaRepository<User, Long> {
    User findByUsername(String username);
}



Add two depedencies to the POM - 

Spring data starter JPA and MySQL Connector.


Create database.

add url, hibernate ddl auto, username, password, driver class properties to the application.properties file with its values.

@MappedSuperClass - 

What is @MappedSuperclass?
The @MappedSuperclass annotation in JPA is used to create a base class that provides common attributes and mappings for multiple entity classes. However, this base class is not mapped to a database table on its own.

Why Use @MappedSuperclass?
✅ Code reusability – Define common fields (e.g., id, createdAt, updatedAt) in one place.
✅ Avoid duplication – Prevent repeating the same fields in multiple entity classes.
✅ Cleaner structure – Keeps entities more readable and maintainable.

How @MappedSuperclass Works?
A class annotated with @MappedSuperclass is not an entity itself.

Subclasses inherit its fields and mappings as if they were defined in each subclass.

It does not have a corresponding table in the database.

Example: Using @MappedSuperclass for Auditing Fields
Step 1: Create the Base Entity Class
java
Copy
Edit
import jakarta.persistence.*;
import java.time.LocalDateTime;

@MappedSuperclass
public abstract class BaseEntity {
    
    @Id
    @GeneratedValue(strategy = GenerationType.IDENTITY)
    private Long id;

    @Column(name = "created_at", nullable = false, updatable = false)
    private LocalDateTime createdAt;

    @Column(name = "updated_at")
    private LocalDateTime updatedAt;

    @PrePersist
    protected void onCreate() {
        createdAt = LocalDateTime.now();
    }

    @PreUpdate
    protected void onUpdate() {
        updatedAt = LocalDateTime.now();
    }

    // Getters and Setters
}
💡 What Happens Here?

@MappedSuperclass: Indicates this class should be a base class but not mapped to a table.

@Id, @GeneratedValue: Common ID field for all child entities.

@Column: Specifies column names and constraints.

@PrePersist, @PreUpdate: Automatically set timestamps when saving/updating an entity.

Step 2: Create an Entity That Extends BaseEntity
java
Copy
Edit
import jakarta.persistence.*;

@Entity
@Table(name = "users")
public class User extends BaseEntity {
    
    @Column(name = "username", nullable = false, unique = true)
    private String username;

    @Column(name = "email", nullable = false)
    private String email;

    // Getters and Setters
}
💡 Key Points:

User inherits id, createdAt, and updatedAt from BaseEntity.

Hibernate treats BaseEntity fields as part of User, but BaseEntity itself has no database table.

Step 3: Create Another Entity That Also Extends BaseEntity
java
Copy
Edit
import jakarta.persistence.*;

@Entity
@Table(name = "products")
public class Product extends BaseEntity {
    
    @Column(name = "name", nullable = false)
    private String name;

    @Column(name = "price", nullable = false)
    private Double price;

    // Getters and Setters
}
💡 Now both User and Product entities have id, createdAt, and updatedAt fields without duplicating code.



 Use @MappedSuperclass when you only want to reuse fields.
🔹 Use @Inheritance when you want a separate table for the parent class.


Relationships between the entities - 

In JPA, relationships between entities are managed using annotations like @OneToMany, @ManyToOne, @ManyToMany, and @OneToOne. These define how tables are linked in the database.

1. @OneToMany and @ManyToOne (Bidirectional Relationship)
✅ @OneToMany: One entity has a collection of another entity.
✅ @ManyToOne: Many entities reference one entity.

Example: One User can have multiple Orders
Step 1: Create User Entity (@OneToMany)
java
Copy
Edit
import jakarta.persistence.*;
import java.util.List;

@Entity
@Table(name = "users")
public class User {
    
    @Id
    @GeneratedValue(strategy = GenerationType.IDENTITY)
    private Long id;
    
    @Column(nullable = false)
    private String username;

    // One user can have multiple orders
    @OneToMany(mappedBy = "user", cascade = CascadeType.ALL, fetch = FetchType.LAZY)
    private List<Order> orders;

    // Getters and Setters
}
@OneToMany(mappedBy = "user"): Defines the one-to-many relationship.

mappedBy = "user" means the user field in Order owns the relationship.

cascade = CascadeType.ALL ensures deleting a user also deletes their orders.

fetch = FetchType.LAZY improves performance by loading orders only when needed.

Step 2: Create Order Entity (@ManyToOne)
java
Copy
Edit
import jakarta.persistence.*;

@Entity
@Table(name = "orders")
public class Order {
    
    @Id
    @GeneratedValue(strategy = GenerationType.IDENTITY)
    private Long id;
    
    @Column(nullable = false)
    private String product;

    // Many orders belong to one user
    @ManyToOne
    @JoinColumn(name = "user_id", nullable = false)
    private User user;

    // Getters and Setters
}
@ManyToOne: Multiple orders reference one user (User entity).

@JoinColumn(name = "user_id"): Creates a foreign key (user_id) in orders table.

nullable = false means an Order must be linked to a User.

2. @ManyToMany (Bidirectional Relationship)
✅ Used when both entities have a many-to-many relationship.
✅ Requires a join table to link both entities.

Example: Many Users can enroll in many Courses
Step 1: Create User Entity (@ManyToMany)
java
Copy
Edit
import jakarta.persistence.*;
import java.util.List;

@Entity
@Table(name = "users")
public class User {
    
    @Id
    @GeneratedValue(strategy = GenerationType.IDENTITY)
    private Long id;
    
    @Column(nullable = false)
    private String username;

    // Many users can enroll in many courses
    @ManyToMany
    @JoinTable(
        name = "user_courses",
        joinColumns = @JoinColumn(name = "user_id"),
        inverseJoinColumns = @JoinColumn(name = "course_id")
    )
    private List<Course> courses;

    // Getters and Setters
}
@ManyToMany with @JoinTable: Creates a join table (user_courses) with user_id and course_id.

A user can enroll in many courses, and a course can have many users.

Step 2: Create Course Entity (@ManyToMany, mappedBy)
java
Copy
Edit
import jakarta.persistence.*;
import java.util.List;

@Entity
@Table(name = "courses")
public class Course {
    
    @Id
    @GeneratedValue(strategy = GenerationType.IDENTITY)
    private Long id;
    
    @Column(nullable = false)
    private String courseName;

    @ManyToMany(mappedBy = "courses")
    private List<User> users;

    // Getters and Setters
}
@ManyToMany(mappedBy = "courses"): User owns the relationship, so Course uses mappedBy.

Hibernate does not create another join table.

3. @OneToOne (Bidirectional Relationship)
✅ One-to-One mapping means each record in one table is linked to only one record in another table.
✅ Example: A User has only one Profile.

Example: One User has One Profile
Step 1: Create User Entity (@OneToOne)
java
Copy
Edit
import jakarta.persistence.*;

@Entity
@Table(name = "users")
public class User {
    
    @Id
    @GeneratedValue(strategy = GenerationType.IDENTITY)
    private Long id;
    
    @Column(nullable = false)
    private String username;

    // One-to-One relationship
    @OneToOne(mappedBy = "user", cascade = CascadeType.ALL)
    private Profile profile;

    // Getters and Setters
}
@OneToOne(mappedBy = "user"): User does not own the relationship.

Profile owns it, so mappedBy = "user" points to user in Profile.

cascade = CascadeType.ALL: Deleting User also deletes Profile.

Step 2: Create Profile Entity (@OneToOne)
java
Copy
Edit
import jakarta.persistence.*;

@Entity
@Table(name = "profiles")
public class Profile {
    
    @Id
    @GeneratedValue(strategy = GenerationType.IDENTITY)
    private Long id;
    
    @Column(nullable = false)
    private String bio;

    // Foreign key
    @OneToOne
    @JoinColumn(name = "user_id", unique = true)
    private User user;

    // Getters and Setters
}
@OneToOne with @JoinColumn(name = "user_id"):

Creates a foreign key (user_id) in profiles table.

Unique constraint ensures one-to-one mapping.

When to Use mappedBy?
@OneToMany(mappedBy = "user"): Used on the parent side when the child owns the relationship.

@ManyToMany(mappedBy = "courses"): Used on one side to indicate the other side owns the relationship.

@OneToOne(mappedBy = "user"): Used when the other entity owns the foreign key.

Final Thoughts
✅ @OneToMany – One parent, multiple children (User -> Orders).
✅ @ManyToOne – Multiple children reference one parent (Orders -> User).
✅ @ManyToMany – Many-to-many with a join table (Users <-> Courses).
✅ @OneToOne – One entity links to exactly one other entity (User -> Profile).

 
------------------------------


Lecture | Implement StorageProductService


@Qualifier Annotation in Spring Boot
In Spring Boot, the @Qualifier annotation is used to resolve ambiguity when multiple beans of the same type are present in the application context. It tells Spring which specific bean to inject when multiple beans of the same type exist.

🔹 Why is @Qualifier Needed?
Spring uses dependency injection to autowire beans into a class. By default, it resolves dependencies by type (@Autowired). However, when multiple beans of the same type are available, Spring gets confused about which one to inject. This is where @Qualifier helps.

🔹 How @Qualifier Works?
It is used along with @Autowired to specify which bean to inject.

The value inside @Qualifier("beanName") must match the name of the bean defined in the configuration.

🛠 Example 1: Without @Qualifier (Ambiguity Issue)
java
Copy
Edit
@Component
public class Dog implements Animal {
    @Override
    public void speak() {
        System.out.println("Woof! Woof!");
    }
}

@Component
public class Cat implements Animal {
    @Override
    public void speak() {
        System.out.println("Meow! Meow!");
    }
}

@Service
public class AnimalService {
    @Autowired
    private Animal animal;  // 🚨 Ambiguity! Spring doesn't know which one to inject.

    public void makeSound() {
        animal.speak();
    }
}
❌ Error: NoUniqueBeanDefinitionException because Spring finds two Animal beans (Dog and Cat) and doesn't know which one to inject.

🛠 Example 2: Using @Qualifier to Resolve Ambiguity
java
Copy
Edit
@Component("dog")
public class Dog implements Animal {
    @Override
    public void speak() {
        System.out.println("Woof! Woof!");
    }
}

@Component("cat")
public class Cat implements Animal {
    @Override
    public void speak() {
        System.out.println("Meow! Meow!");
    }
}

@Service
public class AnimalService {
    @Autowired
    @Qualifier("dog")  // ✅ Explicitly specify which bean to inject
    private Animal animal;

    public void makeSound() {
        animal.speak();
    }
}
✅ Now, Spring will inject the dog bean, and the output will be:

Copy
Edit
Woof! Woof!
🛠 Example 3: Using @Qualifier in Constructor Injection
@Qualifier also works with constructor-based dependency injection:

java
Copy
Edit
@Service
public class AnimalService {
    private final Animal animal;

    @Autowired
    public AnimalService(@Qualifier("cat") Animal animal) {
        this.animal = animal;
    }

    public void makeSound() {
        animal.speak();
    }
}
✅ Now, Spring will inject the cat bean, and the output will be:

Copy
Edit
Meow! Meow!
🛠 Example 4: Using @Qualifier in Method Injection
You can also use @Qualifier in setter methods:

java
Copy
Edit
@Service
public class AnimalService {
    private Animal animal;

    @Autowired
    public void setAnimal(@Qualifier("dog") Animal animal) {
        this.animal = animal;
    }

    public void makeSound() {
        animal.speak();
    }
}
✅ This will inject the dog bean.

🛠 Example 5: @Qualifier with @Bean in Configuration Class
If beans are defined using @Bean in a configuration class, @Qualifier can still be used.

java
Copy
Edit
@Configuration
public class AnimalConfig {
    @Bean(name = "dogBean")
    public Animal getDog() {
        return new Dog();
    }

    @Bean(name = "catBean")
    public Animal getCat() {
        return new Cat();
    }
}

@Service
public class AnimalService {
    private final Animal animal;

    @Autowired
    public AnimalService(@Qualifier("dogBean") Animal animal) {
        this.animal = animal;
    }

    public void makeSound() {
        animal.speak();
    }
}
✅ This explicitly injects the dogBean.

🔹 Difference Between @Primary and @Qualifier
Feature	@Primary	@Qualifier
Purpose	Sets a default bean when multiple beans exist	Specifies exactly which bean to inject
Scope	Works at the class level	Works at the injection point
Priority	Lower priority (overridden by @Qualifier)	Higher priority
Example:
java
Copy
Edit
@Component
@Primary
public class Dog implements Animal { ... }

@Component
public class Cat implements Animal { ... }

@Service
public class AnimalService {
    @Autowired
    private Animal animal; // ✅ Dog is injected by default due to @Primary
}
If @Qualifier("cat") is used, it overrides @Primary.

🎯 Key Takeaways
✅ @Qualifier is used to resolve ambiguity when multiple beans of the same type exist.
✅ Works with field injection, constructor injection, and setter injection.
✅ It overrides @Primary if both are present.
✅ Used with @Component, @Bean, and other Spring stereotypes.




----------------------------------



Lecture | UUIDs and Representing Inheritance in Database


What is UUID?
UUID (Universally Unique Identifier) is a 128-bit unique identifier used to ensure uniqueness across distributed systems. It is widely used in databases, applications, and APIs for generating unique keys.

UUID Format
A UUID is represented as a 36-character string in hexadecimal format, usually divided into 5 groups like this:

Copy
Edit
xxxxxxxx-xxxx-Mxxx-Nxxx-xxxxxxxxxxxx
M: Represents the UUID version.

N: Defines the UUID variant.

Example UUID:
550e8400-e29b-41d4-a716-446655440000

Generating UUID in Java
Java provides the java.util.UUID class to generate UUIDs. You can create different types of UUIDs using built-in methods.

1. Random UUID
java
Copy
Edit
import java.util.UUID;

public class UUIDExample {
    public static void main(String[] args) {
        UUID uuid = UUID.randomUUID();
        System.out.println("Generated UUID: " + uuid);
    }
}
💡 How it Works?

Uses random numbers and cryptographic algorithms.

Ensures uniqueness with very low chances of collision.

No external input required.

2. Name-based UUID (MD5 - Version 3 & SHA-1 - Version 5)
If you need consistent UUIDs for the same input, you can use a name-based UUID.

java
Copy
Edit
import java.util.UUID;
import java.nio.charset.StandardCharsets;
import java.security.MessageDigest;

public class NameBasedUUID {
    public static void main(String[] args) {
        String name = "agripulse";
        UUID uuid = generateNameBasedUUID(name);
        System.out.println("Name-Based UUID: " + uuid);
    }

    public static UUID generateNameBasedUUID(String name) {
        try {
            MessageDigest md = MessageDigest.getInstance("SHA-1");
            byte[] hash = md.digest(name.getBytes(StandardCharsets.UTF_8));
            hash[6] &= 0x0F; // Set version (SHA-1 = version 5)
            hash[6] |= 0x50;
            hash[8] &= 0x3F; // Set variant
            hash[8] |= 0x80;
            return UUID.nameUUIDFromBytes(hash);
        } catch (Exception e) {
            throw new RuntimeException(e);
        }
    }
}
💡 How it Works?

Converts a string input into a UUID.

Uses MD5 (v3) or SHA-1 (v5) hashing.

The same input always generates the same UUID.

3. Time-based UUID (Version 1)
Uses timestamp, MAC address, and sequence number.

Spring Boot does not provide built-in support for v1 UUIDs, but you can use an external library like UUID Creator:

xml
Copy
Edit
<dependency>
    <groupId>com.github.f4b6a3</groupId>
    <artifactId>uuid-creator</artifactId>
    <version>5.2.0</version>
</dependency>
java
Copy
Edit
import com.github.f4b6a3.uuid.UuidCreator;

public class TimeBasedUUID {
    public static void main(String[] args) {
        UUID uuid = UuidCreator.getTimeBased();
        System.out.println("Time-Based UUID: " + uuid);
    }
}
💡 How it Works?

Uses current timestamp for uniqueness.

Avoids randomness, ensuring uniqueness in distributed systems.

UUID in Spring Boot
In Spring Boot, UUIDs are often used in databases, request IDs, and distributed applications.

1. Using UUID as Primary Key in JPA
java
Copy
Edit
import jakarta.persistence.*;
import java.util.UUID;

@Entity
public class Farmer {
    @Id
    @GeneratedValue(strategy = GenerationType.AUTO)
    private UUID id;

    private String name;

    // Getters and Setters
}
💡 How it Works?

Uses Hibernate to auto-generate UUIDs.

Prevents duplicate keys across distributed databases.

2. Generating UUID in Spring Boot Service
java
Copy
Edit
import org.springframework.stereotype.Service;
import java.util.UUID;

@Service
public class UUIDService {
    public String generateUUID() {
        return UUID.randomUUID().toString();
    }
}
💡 Use Case?

Generate unique identifiers for API responses.

UUID Versions and Parameters Used
Version	Type	Parameters Used	Use Case
1	Time-based	Timestamp + MAC Address + Sequence	Distributed Systems
2	DCE Security	User ID + Timestamp	Rarely Used
3	Name-based (MD5)	Namespace + Name	Same input = Same UUID
4	Random	Random Numbers	General Purpose
5	Name-based (SHA-1)	Namespace + Name	Secure name-based UUID
When to Use UUID?
✅ When working with distributed systems.
✅ When needing globally unique identifiers.
✅ When using NoSQL databases (e.g., MongoDB).
✅ When requiring secure, unpredictable IDs.

🚫 Avoid UUID if performance is a concern, as it takes more storage (16 bytes) compared to integers.

Conclusion
UUID is a powerful way to generate unique identifiers in Java and Spring Boot. Depending on your use case, you can choose random (v4), name-based (v3/v5), or time-based (v1) UUIDs.


Storing inheritance relationships in the database - 

In relational databases, storing inheritance relationships is challenging because relational databases do not natively support inheritance like object-oriented programming languages do. However, there are three main strategies to map inheritance hierarchies to a relational database:

Single Table Inheritance (Table Per Hierarchy - TPH)

Table Per Subclass (Table Per Concrete Class - TPC)

Table Per Concrete Class (Table Per Type - TPT)

Each of these approaches has its own advantages and disadvantages.

1. Single Table Inheritance (TPH)
In this approach, all classes in the inheritance hierarchy are stored in a single table. A discriminator column is used to differentiate between different types.

Example
Consider an inheritance hierarchy with Vehicle as the parent class and Car and Bike as subclasses. The table will have all the fields from the subclasses, and a Type column will indicate whether a row is for a Car or a Bike. However, some columns may be NULL for certain types.

Pros
✔ Simple Schema – Only one table is needed, making it easier to query and manage.
✔ Better Performance – Since all data is stored in one table, queries don’t require complex joins.
✔ Easy to Implement – No need for multiple foreign keys or joins between tables.

Cons
❌ Data Sparsity – Some columns are irrelevant for certain subclasses, leading to many NULL values.
❌ Schema Rigidity – Adding a new subclass may require schema changes (new columns).
❌ Data Integrity Issues – Constraints may not be enforceable efficiently (e.g., Max_Speed should only apply to Car and Bike, but it exists for all records).

2. Table Per Subclass (TPC)
Each subclass has its own table, and the base class stores only common attributes. The subclass tables reference the base class using a foreign key.

Example
A Vehicle table will store common attributes like ID and Wheels. Separate tables for Car and Bike will store specific attributes like Engine_Type and Max_Speed. Each subclass table will have a foreign key referencing Vehicle.

Pros
✔ Better Data Integrity – Each table contains only relevant data, enforcing constraints at the table level.
✔ Efficient Storage – No NULL values, as each table contains only necessary columns.
✔ Extensibility – Adding new subclasses is straightforward (create a new table without modifying existing ones).

Cons
❌ Complex Queries – Retrieving all Vehicles requires JOIN operations across multiple tables.
❌ Performance Issues – Joins can slow down queries, especially when retrieving polymorphic results.
❌ Harder to Maintain – More tables mean more complexity in database schema and management.

3. Table Per Concrete Class (TPT)
Each subclass has its own table, containing all fields (including those of the parent class). No shared table exists.

Example
Instead of a shared Vehicle table, each subclass (Car and Bike) has its own table, including both common (Wheels) and specific (Engine_Type, Max_Speed) fields. There are no foreign key references between them.

Pros
✔ Fast Queries – No joins are needed to get subclass-specific data.
✔ Simple Reads/Writes – Each table corresponds to a single class, making queries efficient.
✔ Data Integrity – Each subclass has its constraints properly defined.

Cons
❌ Data Duplication – Common attributes (e.g., Wheels) are repeated across tables.
❌ Hard to Maintain – If the parent class structure changes, every table needs to be updated.
❌ Inflexibility – Queries spanning multiple types (e.g., fetching all Vehicles) require UNION operations.

Comparison Summary
Single Table (TPH): Simple and fast, but leads to NULL values and schema rigidity.

Table Per Subclass (TPC): Structured and extensible, but requires JOINs and adds complexity.

Table Per Concrete Class (TPT): Optimized for subclass-specific queries but causes data duplication.

Which One Should You Use?
Use Single Table (TPH) if:

Your hierarchy is shallow (few subclasses).

You prioritize performance and simple queries.

NULL values are not a concern.

Use Table Per Subclass (TPC) if:

Your hierarchy is deep and evolving.

You need strong referential integrity.

Joins are acceptable in queries.

Use Table Per Concrete Class (TPT) if:

You want fast reads with no joins.

Data duplication is not a major concern.

You don’t frequently query across all subclasses.

Final Thoughts
Each approach has trade-offs, and the best choice depends on your application’s needs.
For example:

In an e-commerce system, where performance is key, TPH may be best.

In an enterprise application, where integrity and flexibility matter, TPC is a better choice.

For analytics-heavy systems, where reads are optimized, TPT might work well.

MappedSuperClass - 

@MappedSuperclass in JPA does not map inheritance into a table structure like the other three strategies (TPH, TPC, TPT). Instead, it is used only for code reuse. It does not create a separate table for the parent class; instead, all the fields of the parent class are copied into each subclass's table.

How @MappedSuperclass Works
The superclass does not have its own table.

All fields of the superclass are directly inherited by the subclass.

Each subclass has its own table, containing both the subclass's attributes and the inherited attributes.

There is no polymorphism support (i.e., you cannot query the superclass and get all its subclass instances).

Example
Superclass
java
Copy
Edit
@MappedSuperclass
public abstract class Vehicle {
    @Id
    @GeneratedValue(strategy = GenerationType.IDENTITY)
    private Long id;
    private int wheels;
}
Subclass - Car
java
Copy
Edit
@Entity
public class Car extends Vehicle {
    private String engineType;
}
Subclass - Bike
java
Copy
Edit
@Entity
public class Bike extends Vehicle {
    private int maxSpeed;
}
Resulting Database Tables
There is no Vehicle table.

The Car table has id, wheels, and engineType.

The Bike table has id, wheels, and maxSpeed.

Comparison with Other Approaches
Similar to Table Per Concrete Class (TPT) because each subclass has its own independent table.

Unlike TPT, there is no parent class table, meaning no shared relationships.

Best suited for code reuse rather than actual inheritance modeling.

Pros and Cons of @MappedSuperclass
✔ Encourages Code Reuse – Avoids redundant fields in subclasses.
✔ No Joins Required – Each entity has all necessary fields in its own table.
✔ Simple Structure – No discriminator columns or foreign keys.

❌ No Polymorphic Queries – Cannot fetch all Vehicle types in a single query.
❌ Data Duplication – If many subclasses exist, the common fields are repeated in each table.
❌ Hard to Maintain Schema – Changing the superclass requires altering every subclass's table.

When to Use @MappedSuperclass?
When you need code reuse but do not need polymorphic queries.

When you don’t want a base table and want each subclass to have a self-contained schema.

When subclass queries are independent and there’s no need to query all subclass types together.

Examples of implementations - 

1. Single Table Inheritance (@Inheritance(strategy = InheritanceType.SINGLE_TABLE))
✅ Uses a single table for all entities with a discriminator column.
✅ Best for performance but may have NULL values.

Entity Classes
java
Copy
Edit
import jakarta.persistence.*;

@Entity
@Inheritance(strategy = InheritanceType.SINGLE_TABLE)
@DiscriminatorColumn(name = "vehicle_type", discriminatorType = DiscriminatorType.STRING)
public abstract class Vehicle {
    @Id
    @GeneratedValue(strategy = GenerationType.IDENTITY)
    private Long id;
    private int wheels;
}

@Entity
@DiscriminatorValue("Car")
public class Car extends Vehicle {
    private String engineType;
}

@Entity
@DiscriminatorValue("Bike")
public class Bike extends Vehicle {
    private int maxSpeed;
}
Repositories
java
Copy
Edit
import org.springframework.data.jpa.repository.JpaRepository;

public interface VehicleRepository extends JpaRepository<Vehicle, Long> {}
public interface CarRepository extends JpaRepository<Car, Long> {}
public interface BikeRepository extends JpaRepository<Bike, Long> {}
2. Table Per Subclass (@Inheritance(strategy = InheritanceType.JOINED))
✅ Uses separate tables for each subclass but inherits primary key from the base table.
✅ Avoids NULL values but requires JOINs when querying the base entity.

Entity Classes
java
Copy
Edit
import jakarta.persistence.*;

@Entity
@Inheritance(strategy = InheritanceType.JOINED)
public abstract class Vehicle {
    @Id
    @GeneratedValue(strategy = GenerationType.IDENTITY)
    private Long id;
    private int wheels;
}

@Entity
public class Car extends Vehicle {
    private String engineType;
}

@Entity
public class Bike extends Vehicle {
    private int maxSpeed;
}
Repositories
java
Copy
Edit
import org.springframework.data.jpa.repository.JpaRepository;

public interface VehicleRepository extends JpaRepository<Vehicle, Long> {}
public interface CarRepository extends JpaRepository<Car, Long> {}
public interface BikeRepository extends JpaRepository<Bike, Long> {}
3. Table Per Concrete Class (@Inheritance(strategy = InheritanceType.TABLE_PER_CLASS))
✅ Each subclass has its own independent table (no shared parent table).
✅ Fast for querying single entities, but data duplication occurs.

Entity Classes
java
Copy
Edit
import jakarta.persistence.*;

@Entity
@Inheritance(strategy = InheritanceType.TABLE_PER_CLASS)
public abstract class Vehicle {
    @Id
    @GeneratedValue(strategy = GenerationType.AUTO)
    private Long id;
    private int wheels;
}

@Entity
public class Car extends Vehicle {
    private String engineType;
}

@Entity
public class Bike extends Vehicle {
    private int maxSpeed;
}
Repositories
java
Copy
Edit
import org.springframework.data.jpa.repository.JpaRepository;

public interface VehicleRepository extends JpaRepository<Vehicle, Long> {}
public interface CarRepository extends JpaRepository<Car, Long> {}
public interface BikeRepository extends JpaRepository<Bike, Long> {}
4. Mapped Superclass (@MappedSuperclass)
✅ Superclass is NOT an entity and no table is created for it.
✅ Fields are copied into each subclass's table.

Entity Classes


import jakarta.persistence.*;

@MappedSuperclass
public abstract class Vehicle {
    @Id
    @GeneratedValue(strategy = GenerationType.IDENTITY)
    private Long id;
    private int wheels;
}

@Entity
public class Car extends Vehicle {
    private String engineType;
}

@Entity
public class Bike extends Vehicle {
    private int maxSpeed;
}
Repositories


import org.springframework.data.jpa.repository.JpaRepository;

public interface CarRepository extends JpaRepository<Car, Long> {}
public interface BikeRepository extends JpaRepository<Bike, Long> {}
⚠ No VehicleRepository because Vehicle is not an entity!

Example Service Layer
For each approach, you can use the following service layer for CRUD operations.



import org.springframework.stereotype.Service;
import java.util.List;

@Service
public class VehicleService {
    private final VehicleRepository vehicleRepository;
    private final CarRepository carRepository;
    private final BikeRepository bikeRepository;

    public VehicleService(VehicleRepository vehicleRepository, CarRepository carRepository, BikeRepository bikeRepository) {
        this.vehicleRepository = vehicleRepository;
        this.carRepository = carRepository;
        this.bikeRepository = bikeRepository;
    }

    public Car saveCar(Car car) {
        return carRepository.save(car);
    }

    public Bike saveBike(Bike bike) {
        return bikeRepository.save(bike);
    }

    public List<Vehicle> getAllVehicles() {
        return vehicleRepository.findAll();
    }

    public List<Car> getAllCars() {
        return carRepository.findAll();
    }

    public List<Bike> getAllBikes() {
        return bikeRepository.findAll();
    }
}

Which One Should You Use?
Use SINGLE_TABLE if performance is a priority, and you don’t mind some NULL values.

Use JOINED if data integrity and normalization are important.

Use TABLE_PER_CLASS if you need fast subclass-specific queries and don't query across all types often.


example - 

1. What is @PrimaryKeyJoinColumn?
By default, when using InheritanceType.JOINED, JPA uses Foreign Key Join to link subclass tables to the parent table. However, we can explicitly specify which column in the subclass table should be used as the Primary Key & Foreign Key by using @PrimaryKeyJoinColumn.

How it Works
The parent class (Vehicle) has a primary key (id).

The subclass tables (Car and Bike) inherit id as a primary key and foreign key referencing the Vehicle table.

The @PrimaryKeyJoinColumn(name = "id") ensures that the id column in Car and Bike is both:

The Primary Key of the subclass table.

The Foreign Key referencing Vehicle.id.

2. Database Structure
Using @PrimaryKeyJoinColumn, the tables will look like this:
Vehicle Table (Parent Table)
id	wheels
1	4
2	2
Car Table (Subclass Table)
id	engine_type
1	Petrol
Bike Table (Subclass Table)
id	max_speed
2	120
Notice:

Car.id and Bike.id are Primary Keys but also Foreign Keys referencing Vehicle.id.

No redundant columns—only relevant attributes are stored in each subclass.

3. Implementation in Spring Boot with JPA
Step 1: Define the Parent Class (Vehicle)
java
Copy
Edit
import jakarta.persistence.*;

@Entity
@Inheritance(strategy = InheritanceType.JOINED)
public abstract class Vehicle {
    @Id
    @GeneratedValue(strategy = GenerationType.IDENTITY)
    private Long id;

    private int wheels;

    // Getters and Setters
    public Long getId() {
        return id;
    }

    public void setId(Long id) {
        this.id = id;
    }

    public int getWheels() {
        return wheels;
    }

    public void setWheels(int wheels) {
        this.wheels = wheels;
    }
}
Step 2: Define the Subclass (Car)
java
Copy
Edit
import jakarta.persistence.*;

@Entity
@PrimaryKeyJoinColumn(name = "id") // This column is both PRIMARY KEY & FOREIGN KEY
public class Car extends Vehicle {
    private String engineType;

    // Getters and Setters
    public String getEngineType() {
        return engineType;
    }

    public void setEngineType(String engineType) {
        this.engineType = engineType;
    }
}
Step 3: Define Another Subclass (Bike)
java
Copy
Edit
import jakarta.persistence.*;

@Entity
@PrimaryKeyJoinColumn(name = "id") // This column is both PRIMARY KEY & FOREIGN KEY
public class Bike extends Vehicle {
    private int maxSpeed;

    // Getters and Setters
    public int getMaxSpeed() {
        return maxSpeed;
    }

    public void setMaxSpeed(int maxSpeed) {
        this.maxSpeed = maxSpeed;
    }
}
Step 4: Create JPA Repositories
java
Copy
Edit
import org.springframework.data.jpa.repository.JpaRepository;

public interface VehicleRepository extends JpaRepository<Vehicle, Long> {}
public interface CarRepository extends JpaRepository<Car, Long> {}
public interface BikeRepository extends JpaRepository<Bike, Long> {}
Step 5: Create Service Layer
java
Copy
Edit
import org.springframework.stereotype.Service;
import java.util.List;

@Service
public class VehicleService {
    private final VehicleRepository vehicleRepository;
    private final CarRepository carRepository;
    private final BikeRepository bikeRepository;

    public VehicleService(VehicleRepository vehicleRepository, CarRepository carRepository, BikeRepository bikeRepository) {
        this.vehicleRepository = vehicleRepository;
        this.carRepository = carRepository;
        this.bikeRepository = bikeRepository;
    }

    public Car saveCar(Car car) {
        return carRepository.save(car);
    }

    public Bike saveBike(Bike bike) {
        return bikeRepository.save(bike);
    }

    public List<Vehicle> getAllVehicles() {
        return vehicleRepository.findAll();
    }
}
Step 6: Create Controller Layer
java
Copy
Edit
import org.springframework.web.bind.annotation.*;

import java.util.List;

@RestController
@RequestMapping("/vehicles")
public class VehicleController {
    private final VehicleService vehicleService;

    public VehicleController(VehicleService vehicleService) {
        this.vehicleService = vehicleService;
    }

    @PostMapping("/car")
    public Car addCar(@RequestBody Car car) {
        return vehicleService.saveCar(car);
    }

    @PostMapping("/bike")
    public Bike addBike(@RequestBody Bike bike) {
        return vehicleService.saveBike(bike);
    }

    @GetMapping
    public List<Vehicle> getAllVehicles() {
        return vehicleService.getAllVehicles();
    }
}
4. Testing the API
1. Create a Car
POST Request:
bash
Copy
Edit
POST /vehicles/car
Content-Type: application/json
{
    "wheels": 4,
    "engineType": "Diesel"
}
✅ Response:

json
Copy
Edit
{
    "id": 1,
    "wheels": 4,
    "engineType": "Diesel"
}
2. Create a Bike
POST Request:
bash
Copy
Edit
POST /vehicles/bike
Content-Type: application/json
{
    "wheels": 2,
    "maxSpeed": 150
}
✅ Response:

json
Copy
Edit
{
    "id": 2,
    "wheels": 2,
    "maxSpeed": 150
}
3. Get All Vehicles
GET Request:
bash
Copy
Edit
GET /vehicles
✅ Response:

json
Copy
Edit
[
    {
        "id": 1,
        "wheels": 4
    },
    {
        "id": 2,
        "wheels": 2
    }
]
(Note: Since Vehicle is abstract, only common fields appear in the JSON response.)

5. Advantages of Using @PrimaryKeyJoinColumn
✔ Efficient Storage: No duplicate parent fields in subclass tables.
✔ Better Normalization: Parent and child data are linked efficiently.
✔ Flexible Queries:

Fetch all vehicles (SELECT * FROM Vehicle)

Fetch only cars (SELECT * FROM Car)

Fetch only bikes (SELECT * FROM Bike)

6. When to Use @PrimaryKeyJoinColumn?
✅ Use it when:

You want better normalization (no redundant data).

You expect frequent queries on the base class (Vehicle).

You want to enforce strong foreign key constraints.

❌ Avoid if:

You want fewer joins (because queries require JOIN operations).

Your hierarchy is shallow, and you prefer a single table (SINGLE_TABLE).

Final Thoughts
🔹 @PrimaryKeyJoinColumn is useful when mapping normalized inheritance relationships in databases.
🔹 It ensures that subclass tables inherit the primary key from the parent while maintaining separate tables.
🔹 Although JOINs may slightly impact performance, they help in maintaining data integrity.


--------------------------------


Lecture | DbMigration, Queries and Query Methods


Db Migration is the process of adding or undoing changes to the db. Changes in this case is the structure of the tables not about the data.

It is the process of modifying a database structure, schema, or data while ensuring the system continues to function correctly. 

DB schema version - this is similar to version control which helps in keeping track of DB schema ( What all the fields and tables we have).


It is required to maintain database intefrity, version control for database, consistency across environments.

Tools used for this - 

Flyway - It uses versioned SQL scripts, it automatically applies pending migrations. 

Flyway follows a simple convention based migration strategy. It tracks database versions using versioned migration scripts stored in a specific directory. When the application starts, fllyway checks for any pending migrations and applies the in the correct order.

When Flyway starts, it creates a metadata table called flyway_schema_history in the database (if it doesn't already exist), This table tracks all applied migrations to prevent duplicate execution.

Flyway scans the migration directory ( src/main/resources/db/migration/) and compares available scripts with the ones already recorded in flyway_schema_history, if a script is missing in the history table, flyway runs it.

Each migration script is executed sequentially based on its versio number.

when flyway executes these scripts - it records their execution in flyway_schema_history, if the scripts are already executed, flyway skips them. This ensures that each script runs only once.

For we need add jpa plugin to the intellij called JPA buddy. ( the one with dog icon ).

every time restart the application if using flyway.

click on the jpa Explorer ( on the left side panel of intellij ).

then click on + icon in opened tab then select generate DDL by entities, then select DDL type as DB schema initialization then click on Ok button.

Above steps used for generating the schema files for previously generated tables and schema changes.

all the migration files will be created in resources folder -> db.migration folder. 

Whenever there is change in schema again click on jpa explorer, ( on the left side panel of intellij ).

then click on + icon in opened tab then select Existing DB updates (Diff), then select DDL type as DB schema initialization then click on Ok button, then check the queries and click on save button.

these migration files will be helpful to recreate the schema changes and get the latest schema in the database and these are used by the developers.


------------------------------


Lecture | FetchTypes and Modes

Fetch types - 

1. Eager Loading
Definition
In Eager Loading, related entities are fetched immediately along with the main entity.

It reduces the number of queries but may load unnecessary data, affecting performance.

Example
Consider a system where we have:

User entity

Profile entity (One-to-One relationship with User)

java
Copy
Edit
@Entity
public class User {
    @Id
    @GeneratedValue(strategy = GenerationType.IDENTITY)
    private Long id;
    
    private String name;

    @OneToOne(fetch = FetchType.EAGER)
    @JoinColumn(name = "profile_id")
    private Profile profile;

    // Getters and Setters
}
java
Copy
Edit
@Entity
public class Profile {
    @Id
    @GeneratedValue(strategy = GenerationType.IDENTITY)
    private Long id;

    private String bio;
    
    // Getters and Setters
}
Behavior
When we fetch a User, the Profile is also fetched immediately, even if we don’t need it.

java
Copy
Edit
User user = userRepository.findById(1L).get();
System.out.println(user.getName());  // Loads user
System.out.println(user.getProfile().getBio()); // Already loaded
Drawbacks
If Profile is large and not always needed, it increases memory usage and reduces performance.

Fetching large collections eagerly can lead to performance bottlenecks.

2. Lazy Loading
Definition
In Lazy Loading, related entities are fetched only when explicitly accessed.

This is the default behavior in JPA.

Example
Modify the @OneToOne relationship:

java
Copy
Edit
@OneToOne(fetch = FetchType.LAZY)
@JoinColumn(name = "profile_id")
private Profile profile;
Behavior
java
Copy
Edit
User user = userRepository.findById(1L).get();
System.out.println(user.getName()); // Loads only User

// Profile is fetched only when accessed
System.out.println(user.getProfile().getBio());
Benefits
Improves performance by fetching related data only when needed.

Reduces initial query size.

Problems
Can cause LazyInitializationException if accessed outside a transaction:

java
Copy
Edit
@Transactional
public void fetchUser() {
    User user = userRepository.findById(1L).get();
    // Transaction is open, so lazy loading works
    System.out.println(user.getProfile().getBio());
}
Without @Transactional, the session might close before fetching the related entity.

When to Use What?
Use Eager Loading when related data is always required (e.g., fetching a User with its Roles).

Use Lazy Loading when related data is optional or large, reducing memory usage.

Fetch types eventually helps in getting when data will be fetched.

while fetching there are multiple ways like by using select, subquery and join.

Fetch modes - 

Fetch Modes in Spring Data JPA: JOIN, SELECT, and SUBSELECT
When using JPA relationships (@OneToOne, @OneToMany, @ManyToOne, @ManyToMany), the way related entities are fetched can significantly impact performance. Fetch modes control how related entities are retrieved.

JPA provides three fetch strategies under @Fetch annotation from Hibernate-specific FetchMode:

FetchMode.JOIN → Uses JOIN queries.

FetchMode.SELECT → Uses separate SELECT queries.

FetchMode.SUBSELECT → Uses a subquery.

These fetch modes work alongside FetchType.EAGER and FetchType.LAZY. Let’s explore in detail.

1. FetchMode.JOIN (Immediate Fetching with JOIN Query)
Definition
Uses an INNER JOIN or LEFT JOIN to fetch related entities in a single query.

Works only with EAGER loading.

Reduces the number of queries but can lead to cartesian product issues (N+1 problem for collections).

Example
Consider User and Profile entities with a @OneToOne relationship.

java
Copy
Edit
@Entity
public class User {
    @Id
    @GeneratedValue(strategy = GenerationType.IDENTITY)
    private Long id;
    
    private String name;

    @OneToOne(fetch = FetchType.EAGER)
    @JoinColumn(name = "profile_id")
    @Fetch(FetchMode.JOIN)  // Uses JOIN fetching
    private Profile profile;
}
Generated SQL Query
sql
Copy
Edit
SELECT u.id, u.name, p.id, p.bio 
FROM user u 
JOIN profile p ON u.profile_id = p.id 
WHERE u.id = 1;
Advantages
✔ Efficient for one-to-one or many-to-one relationships.
✔ Reduces extra queries for related entities.

Disadvantages
❌ Not good for one-to-many relationships, as it can cause data duplication.
❌ If you have a large collection, it can load unnecessary data upfront.

2. FetchMode.SELECT (Lazy Loading with Separate Queries)
Definition
Fetches the related entities using separate queries.

Works with both EAGER and LAZY loading.

Default behavior when using FetchType.LAZY.

Example
Modify @OneToOne with FetchMode.SELECT:

java
Copy
Edit
@Entity
public class User {
    @Id
    @GeneratedValue(strategy = GenerationType.IDENTITY)
    private Long id;
    
    private String name;

    @OneToOne(fetch = FetchType.LAZY)
    @JoinColumn(name = "profile_id")
    @Fetch(FetchMode.SELECT) // Uses separate SELECT queries
    private Profile profile;
}
Generated SQL Queries
When fetching the user:

sql
Copy
Edit
SELECT id, name FROM user WHERE id = 1;
When accessing profile:

sql
Copy
Edit
SELECT id, bio FROM profile WHERE id = 2;
Advantages
✔ Best for one-to-many or many-to-one relationships (avoids duplicate rows).
✔ Reduces initial load time by fetching only what is needed.

Disadvantages
❌ N+1 Query Problem → If we fetch multiple users, each Profile is fetched separately.
❌ LazyInitializationException → If accessed outside a transaction, Hibernate cannot fetch the related entity.

3. FetchMode.SUBSELECT (Batch Fetching with Subqueries)
Definition
Fetches collections using a subquery when accessed.

Works only with lazy loading (FetchType.LAZY).

Useful for avoiding N+1 problem in @OneToMany relationships.

Example
Let's say a User can have multiple Posts (@OneToMany relationship).

java
Copy
Edit
@Entity
public class User {
    @Id
    @GeneratedValue(strategy = GenerationType.IDENTITY)
    private Long id;
    
    private String name;

    @OneToMany(mappedBy = "user", fetch = FetchType.LAZY)
    @Fetch(FetchMode.SUBSELECT)  // Uses a subquery to fetch related posts
    private List<Post> posts;
}
Generated SQL Queries
When fetching all users:

sql
Copy
Edit
SELECT id, name FROM user;
When accessing posts:

sql
Copy
Edit
SELECT id, title, user_id FROM post 
WHERE user_id IN (SELECT id FROM user);
🔹 This avoids multiple queries and fetches all related posts in one go.

Advantages
✔ Efficient for One-To-Many relationships (batch fetches related entities).
✔ Prevents the N+1 problem.

Disadvantages
❌ Not good for single record retrievals.
❌ Not useful for one-to-one relationships.

Comparison of Fetch Modes
Fetch Mode	Works with	Query Type	Pros	Cons
JOIN	EAGER	Single JOIN query	Efficient for one-to-one/many-to-one, reduces query count	Causes data duplication in one-to-many
SELECT	LAZY & EAGER	Multiple SELECT queries	Avoids data duplication, good for one-to-many	N+1 problem, can cause LazyInitializationException
SUBSELECT	LAZY	Single SELECT query with subquery	Prevents N+1 problem, batch-fetches collections	Not good for one-to-one or small datasets
Which Fetch Mode to Use?
Use JOIN for one-to-one or many-to-one relationships when data is frequently used.

Use SELECT for lazy loading when the related data is not always required.

Use SUBSELECT for one-to-many when you want to batch-fetch related entities efficiently.

Final Thoughts
FetchType.LAZY with FetchMode.SUBSELECT is best for collections.

FetchType.EAGER with FetchMode.JOIN is best for small relationships.

FetchType.LAZY with FetchMode.SELECT is best when related data is rarely needed.

Observations from class - 

FetchType        FetchMode          Result

 LAZY             SELECT             Asked For Products - Queries - 2 select
                                     Not Asked For Products - Queries - 1 select


 EAGER            SELECT             Asked For Products - Queries - 2 select
                                     Not Asked For Products - Queries - 2 select



 LAZY             JOIN               Asked For Products - Queries - 1 join
                                     Not Asked For Products - Queries - 1 join



 EAGER            JOIN               Asked For Products - Queries - 1 join
                                     Not Asked For Products - Queries - 1 join



 LAZY             SUBSELECT           Asked For Products - Queries - 2 select
                                      Not Asked For Products - Queries - 1 select



 EAGER            SUBSELECT          Asked For Products - Queries - 2 select
                                     Not Asked For Products - Queries - 2 select


 Summary ->

 FetchMode JOIN is dominant over all cases. It doesn't honor any fetchtype

 In case of FetchMode Select and Subselect, FetchType is honored, that means we will not get child
 entity in case of lazy loading.

 In  case of FetchMode Select and Subselect , 2 select queries from product and category table, even though join would
 have ran.
 
 Batch size - 
 
 Understanding @BatchSize in Spring Data JPA
The @BatchSize annotation in Hibernate helps improve performance when dealing with lazy-loaded collections or associations. It controls how many related entities are fetched in a single query, reducing the N+1 problem in many-to-one and one-to-many relationships.

Why @BatchSize?
The N+1 Problem
When fetching an entity with lazy-loaded relationships, Hibernate executes an extra query for each related entity.

Example: If we fetch 10 Users, and each User has 1 Profile, Hibernate will execute 1 query for Users + 10 queries for Profiles (N+1 problem).

Solution: @BatchSize
Instead of fetching one entity at a time, Hibernate fetches multiple related entities in batches.

This reduces the number of queries.

How @BatchSize Works
Syntax
java
Copy
Edit
@BatchSize(size = 5)
size = 5 → Hibernate fetches 5 entities at once instead of querying each separately.

Example 1: @BatchSize in @OneToMany (User → Posts)
Entities
User (One User has many Posts)
java
Copy
Edit
@Entity
public class User {
    @Id
    @GeneratedValue(strategy = GenerationType.IDENTITY)
    private Long id;
    
    private String name;

    @OneToMany(mappedBy = "user", fetch = FetchType.LAZY)
    @BatchSize(size = 5) // Fetch 5 Users' Posts at once
    private List<Post> posts;
}
Post (Many Posts belong to one User)
java
Copy
Edit
@Entity
public class Post {
    @Id
    @GeneratedValue(strategy = GenerationType.IDENTITY)
    private Long id;
    
    private String title;

    @ManyToOne(fetch = FetchType.LAZY)
    @JoinColumn(name = "user_id")
    private User user;
}
Without @BatchSize: N+1 Problem
java
Copy
Edit
List<User> users = userRepository.findAll(); 
for (User user : users) {
    System.out.println(user.getPosts().size());
}
Generated SQL Queries (Bad Performance)
sql
Copy
Edit
SELECT * FROM user;  -- Fetch all Users (1 query)
SELECT * FROM post WHERE user_id = 1;  -- Fetch Posts for User 1
SELECT * FROM post WHERE user_id = 2;  -- Fetch Posts for User 2
SELECT * FROM post WHERE user_id = 3;  -- Fetch Posts for User 3
-- If we have 10 Users, this results in 11 queries (1 + 10)
🔥 Problem: Too many queries = Performance issues.

With @BatchSize(size = 5): Optimized Querying
sql
Copy
Edit
SELECT * FROM user;  -- Fetch all Users (1 query)
SELECT * FROM post WHERE user_id IN (1, 2, 3, 4, 5);  -- Batch fetch 5 Users' Posts
SELECT * FROM post WHERE user_id IN (6, 7, 8, 9, 10); -- Next batch of 5
🔥 Optimization: Now only 3 queries instead of 11!

Example 2: @BatchSize in @ManyToOne (Post → User)
Post with Many-to-One Relationship
java
Copy
Edit
@Entity
public class Post {
    @Id
    @GeneratedValue(strategy = GenerationType.IDENTITY)
    private Long id;
    
    private String title;

    @ManyToOne(fetch = FetchType.LAZY)
    @JoinColumn(name = "user_id")
    @BatchSize(size = 10)  // Fetch 10 Users at once when accessed
    private User user;
}
Without @BatchSize
Fetching 10 Posts causes 10 separate queries to fetch their User.

With @BatchSize(size = 10)
Hibernate groups the requests and fetches 10 Users at once.

Example 3: @BatchSize in @OneToOne (User → Profile)
java
Copy
Edit
@Entity
public class User {
    @Id
    @GeneratedValue(strategy = GenerationType.IDENTITY)
    private Long id;

    private String name;

    @OneToOne(fetch = FetchType.LAZY)
    @JoinColumn(name = "profile_id")
    @BatchSize(size = 3) // Fetch 3 Profiles at a time
    private Profile profile;
}
Query Optimization
sql
Copy
Edit
SELECT * FROM user; -- Fetch all Users (1 query)
SELECT * FROM profile WHERE id IN (1, 2, 3); -- Fetch 3 Profiles at a time
When to Use @BatchSize?
Relationship	Use @BatchSize?	Why?
One-to-Many (@OneToMany)	✅ Yes	Prevents N+1 problem by fetching multiple child records in a batch.
Many-to-One (@ManyToOne)	✅ Yes	Fetches multiple parent entities efficiently.
One-to-One (@OneToOne)	✅ Yes	Helps in lazy loading of related entities in batches.
Many-to-Many (@ManyToMany)	✅ Yes	Reduces excessive join queries.
@BatchSize vs FetchMode.SUBSELECT
Feature	@BatchSize	FetchMode.SUBSELECT
Query Strategy	Multiple queries, but batched	Single query using a subquery
Works with	LAZY loading	LAZY loading
Best for	Fetching parent-child relationships efficiently	Fetching all child entities for multiple parents
Performance	Better than SELECT (default)	Best for batch fetching collections
Best Practices
✔ Use @BatchSize(size = X) instead of FetchType.EAGER to avoid unnecessary eager loading.
✔ Use FetchMode.SUBSELECT for large collections, but @BatchSize for better flexibility.
✔ Set the right size based on expected usage (too large may cause memory issues).

Conclusion
@BatchSize solves the N+1 problem by batching queries for lazy-loaded relationships.

Works best with Many-to-One and One-to-Many associations.

Helps reduce query overhead while avoiding excessive eager loading.


Batchsize with select mode is always better.


-------------------------------


Lecture | Introduction to Testing and Best Practices of writing Unit Tests


Different types of testing - 

Unit testing, Integration testing, functional testing.

Unit - is something very small which can not be broken further.

in testing we make sure whether our code handles all scenarios correctly or not.

Unit Testing: A Complete Guide
What is Unit Testing?
Unit testing is a software testing technique where individual units (functions, methods, or components) of a program are tested in isolation to verify their correctness.

✅ Goals of Unit Testing:

Ensure each unit behaves as expected.

Identify and fix bugs early in development.

Improve code reliability and maintainability.

Types of Unit Tests
1. Positive Test Cases
✅ Definition:

Tests that check if the system behaves correctly with valid inputs.

Ensures the function returns expected outputs under normal conditions.

✅ Example (Java, JUnit 5):

java
Copy
Edit
@Test
void testAdditionPositive() {
    int result = Calculator.add(2, 3);
    assertEquals(5, result); // Expected output: 5
}
✔ If the function returns 5, the test passes.

2. Negative Test Cases
❌ Definition:

Tests that check how the system handles invalid inputs or errors.

Ensures proper error handling and validation.

❌ Example:

java
Copy
Edit
@Test
void testAdditionNegative() {
    assertThrows(IllegalArgumentException.class, () -> Calculator.add(null, 3));
}
✔ The test passes if the method throws an IllegalArgumentException when given null.

3. Edge Cases
⚡ Definition:

Tests that verify behavior at boundaries or extreme conditions.

Ensures the system does not fail under rare, extreme, or special inputs.

⚡ Examples:

java
Copy
Edit
@Test
void testAdditionEdgeCases() {
    assertEquals(0, Calculator.add(0, 0)); // Zero inputs
    assertEquals(Integer.MAX_VALUE, Calculator.add(Integer.MAX_VALUE, 0)); // Upper limit
    assertEquals(Integer.MIN_VALUE, Calculator.add(Integer.MIN_VALUE, 0)); // Lower limit
}
✔ The function must handle edge conditions correctly.

Code Coverage in Unit Testing
📊 What is Code Coverage?

Code coverage is a measure of how much of your code is tested by unit tests.

Higher coverage = More tested code = Better reliability.

📊 Types of Code Coverage:

Type	Description	Example
Statement Coverage	Ensures each line of code is executed at least once.	Every if-else and loop is hit.
Branch Coverage	Ensures each decision point (if, switch) covers all possible branches.	Both if and else are tested.
Path Coverage	Ensures all possible execution paths are tested.	Every function call path is executed.
Condition Coverage	Ensures all boolean conditions (`&&,	
🔹 Tools for Code Coverage:

JaCoCo (Java Code Coverage)

Cobertura

SonarQube

📊 Example JaCoCo Report (After Running Tests)

yaml
Copy
Edit
Class: Calculator.java
Statements Coverage: 90%
Branch Coverage: 75%
👉 90% coverage means 90% of code was tested.
👉 75% branch coverage means some conditions were not fully tested.

How to Write Effective Unit Tests?
✅ Follow the AAA Pattern (Arrange, Act, Assert)

java
Copy
Edit
@Test
void testMultiplication() {
    // Arrange - Setup test data
    int a = 5, b = 4;

    // Act - Call the method
    int result = Calculator.multiply(a, b);

    // Assert - Verify the output
    assertEquals(20, result);
}
✅ Use Mocks for Dependencies (Mockito Example)

java
Copy
Edit
@Mock
DatabaseService databaseService; // Mock database

@Test
void testUserLogin() {
    when(databaseService.findUser("admin")).thenReturn(new User("admin", "pass123"));

    User user = authService.login("admin", "pass123");

    assertNotNull(user);
    assertEquals("admin", user.getUsername());
}
✅ Test All Scenarios (Positive, Negative, Edge Cases)

✅ Ensure High Code Coverage (>80%)

Final Thoughts
Unit tests improve code quality and catch bugs early.

Always write positive, negative, and edge test cases.

Use mocking frameworks for testing dependencies.

Code coverage should be high but meaningful (aim for >80%).


Cucumber can be used to perform integration testing.


Integration Testing vs Functional Testing
Both Integration Testing and Functional Testing are essential in ensuring that an application works correctly. Let’s break them down.

1️⃣ Integration Testing
🔹 What is Integration Testing?
Integration testing verifies how different modules or components work together. It ensures that the data flow between modules is correct.

🔹 Goal:

Test interactions between components (APIs, databases, services).

Identify bugs in communication between different parts of the system.

🔹 Example Scenario:

In an e-commerce app, a PaymentService calls an OrderService. Integration tests ensure the payment is processed and updated correctly.

🔹 How to Perform Integration Testing?
✅ Use Real or Mock Databases (H2, PostgreSQL, MySQL)
✅ Test API Endpoints (Spring Boot + REST Assured, Postman)
✅ Verify Communication Between Services (MockMVC, WireMock)

🔹 Example: Spring Boot Integration Test
java
Copy
Edit
@SpringBootTest
@AutoConfigureMockMvc
public class OrderServiceIntegrationTest {

    @Autowired
    private MockMvc mockMvc;

    @Test
    void testPlaceOrder() throws Exception {
        mockMvc.perform(post("/orders")
                .contentType(MediaType.APPLICATION_JSON)
                .content("{ \"productId\": 1, \"quantity\": 2 }"))
                .andExpect(status().isOk())
                .andExpect(jsonPath("$.orderId").exists());
    }
}
🔥 This test checks if the /orders API works end-to-end.

2️⃣ Functional Testing
🔹 What is Functional Testing?
Functional testing ensures that the application behaves correctly based on the requirements.

🔹 Goal:

Validate that the system performs the expected business logic.

Ensure each feature meets the functional specifications.

🔹 Example Scenarios:

Login should work with correct credentials.

An item added to the cart should appear in checkout.

A user should get an error message when entering an invalid email.

🔹 Functional Testing Includes:
✅ Unit Testing – Testing individual functions/methods.
✅ Integration Testing – Testing interactions between modules.
✅ System Testing – Testing the entire system.
✅ Acceptance Testing – Ensuring the system meets user requirements.

🔹 Example: Functional Test for Login API
java
Copy
Edit
@Test
void testLoginSuccessful() throws Exception {
    mockMvc.perform(post("/login")
            .contentType(MediaType.APPLICATION_JSON)
            .content("{ \"username\": \"admin\", \"password\": \"pass123\" }"))
            .andExpect(status().isOk())
            .andExpect(jsonPath("$.token").exists());
}

@Test
void testLoginFailure() throws Exception {
    mockMvc.perform(post("/login")
            .contentType(MediaType.APPLICATION_JSON)
            .content("{ \"username\": \"admin\", \"password\": \"wrongpass\" }"))
            .andExpect(status().isUnauthorized());
}
✔ The first test checks successful login.
✔ The second test checks invalid password failure.

Best Practices for Writing Unit Tests
✅ 1️⃣ Follow the AAA Pattern (Arrange, Act, Assert)

java
Copy
Edit
@Test
void testMultiplication() {
    // Arrange - Setup test data
    int a = 5, b = 4;

    // Act - Call the method
    int result = Calculator.multiply(a, b);

    // Assert - Verify the output
    assertEquals(20, result);
}
✔ This ensures clean and structured tests.

✅ 2️⃣ Write Positive, Negative & Edge Cases

Test Type	Example
Positive Case	Valid login returns token.
Negative Case	Wrong password returns error.
Edge Case	Empty username returns validation error.
✅ 3️⃣ Keep Tests Independent

Avoid shared state between tests.

Reset test data before each test.

✔ Use @BeforeEach in JUnit:

java
Copy
Edit
@BeforeEach
void setup() {
    userRepository.deleteAll(); // Clean database before each test
}
✅ 4️⃣ Use Mocks for Dependencies
Use Mockito to mock dependencies and avoid testing real DBs or APIs.

java
Copy
Edit
@Mock
UserRepository userRepository;

@Test
void testFindUser() {
    when(userRepository.findByUsername("admin"))
        .thenReturn(new User("admin", "pass123"));

    User user = userService.findByUsername("admin");

    assertNotNull(user);
}
✔ This ensures fast, independent tests.

✅ 5️⃣ Ensure High Code Coverage (>80%)

Use JaCoCo to measure coverage.

Cover branches, conditions, and edge cases.

Final Thoughts
Aspect	Integration Testing	Functional Testing
Focus	Interaction between components	Validating business logic
Scope	APIs, databases, services	User flows, feature behavior
Tools	MockMvc, WireMock, TestContainers	Selenium, Postman, Rest Assured
Example	Order Service calling Payment Service	User login and checkout flow


Unit testing best practices - 

We should write UT quickly.

Arrange - create
ACt - Call
Assert - Check

Arrange means create objec of a class whose methods we are going to test.

Create mock of external depedencies and other things whatever required.

Act means calling the function which we want to test and store result in some variable. 	

Assert means checking if received output matches the expected result or not.

Unit tests should be repeatable means if we run it at any time it should run and give the same response.



Unit tests should be automatic. Inputs are always hard coded.

Unit test should always test the behivour and not implementation.

-------------------------------

Lecture | Let's write Unit Tests


AAA (Arrange-Act-Assert) Method for Writing Unit Test Cases
The AAA (Arrange-Act-Assert) method is a structured way of writing unit tests that improves readability and maintainability. It consists of three main steps:

Arrange – Set up the necessary objects, mock dependencies, and initialize the required state.

Act – Perform the actual method call that needs to be tested.

Assert – Verify the expected results using assertions.

Example: Writing Unit Test Cases in Java Using JUnit
Let's assume we have a simple Calculator class with an add method.

Production Code (Calculator.java)
java
Copy
Edit
public class Calculator {
    public int add(int a, int b) {
        return a + b;
    }
}
Unit Test Using AAA (CalculatorTest.java)
java
Copy
Edit
import org.junit.jupiter.api.Test;
import static org.junit.jupiter.api.Assertions.*;

public class CalculatorTest {

    @Test
    void testAddition() {
        // Arrange: Set up the calculator instance and inputs
        Calculator calculator = new Calculator();
        int a = 5;
        int b = 10;

        // Act: Call the method under test
        int result = calculator.add(a, b);

        // Assert: Verify the expected result
        assertEquals(15, result, "Addition should return the correct sum");
    }
}
Best Practices for Writing Unit Tests
Follow the AAA Pattern

Clearly separate the Arrange, Act, and Assert steps.

Use Meaningful Test Method Names

Use descriptive names like testAddition_returnsCorrectSum() instead of testAdd().

Keep Tests Independent

Each test should be self-contained and should not depend on other tests.

Test Both Positive and Negative Cases

Example: Test for valid input, edge cases, and invalid input scenarios.

Use Mocks for External Dependencies

When testing services that interact with databases or APIs, use Mockito to mock dependencies.

Follow the Single Responsibility Principle (SRP)

Each test case should test one specific behavior of a method.

Ensure Tests Run Fast

Avoid unnecessary dependencies that slow down test execution.

Example with Mocking (Using Mockito)
Suppose we have a UserService class that depends on UserRepository. Instead of calling the real repository, we mock it.

Production Code (UserService.java)


public class UserService {
    private final UserRepository userRepository;

    public UserService(UserRepository userRepository) {
        this.userRepository = userRepository;
    }

    public boolean isUserActive(String username) {
        User user = userRepository.findByUsername(username);
        return user != null && user.isActive();
    }
}
Unit Test with Mocking (UserServiceTest.java)


import org.junit.jupiter.api.Test;
import org.mockito.Mockito;
import static org.junit.jupiter.api.Assertions.*;
import static org.mockito.Mockito.*;

public class UserServiceTest {

    @Test
    void testIsUserActive_whenUserExistsAndActive() {
        // Arrange
        UserRepository mockRepository = mock(UserRepository.class);
        UserService userService = new UserService(mockRepository);
        
        User mockUser = new User("john_doe", true);
        when(mockRepository.findByUsername("john_doe")).thenReturn(mockUser);

        // Act
        boolean isActive = userService.isUserActive("john_doe");

        // Assert
        assertTrue(isActive, "User should be active");
    }
}
Conclusion
The AAA (Arrange-Act-Assert) pattern helps in writing clean and well-structured unit tests. Following best practices like using meaningful test names, keeping tests independent, and leveraging mocking frameworks ensures robust and maintainable test cases.


 test method names for both positive and negative cases:

For a Calculator class with an add method:

Positive Case: testAdd_TwoPositiveNumbers_ReturnsCorrectSum()

Negative Case: testAdd_PositiveAndNegativeNumber_ReturnsCorrectSum()

Edge Case: testAdd_NumberAndZero_ReturnsSameNumber()

Edge Case: testAdd_LargeNumbers_ReturnsCorrectSum()

For a UserService class with an isUserActive method:

Positive Case: testIsUserActive_UserExistsAndActive_ReturnsTrue()

Negative Case: testIsUserActive_UserDoesNotExist_ReturnsFalse()

Negative Case: testIsUserActive_UserExistsButInactive_ReturnsFalse()

For an AuthenticationService class with a login method:

Positive Case: testLogin_ValidCredentials_ReturnsToken()

Negative Case: testLogin_InvalidCredentials_ThrowsAuthenticationException()

Edge Case: testLogin_EmptyUsernameOrPassword_ThrowsValidationException()

Best practices for naming test methods:

Follow a structured format: test<MethodUnderTest>_<Condition>_<ExpectedOutcome>()

Use descriptive names instead of generic ones like testLogin().

Include edge cases such as testDivide_ByZero_ThrowsArithmeticException().

Use underscores for readability: testRegisterUser_AlreadyExists_ThrowsException().


Here's an example of a unit test using Mockito to mock an object. We'll test a UserService class that depends on a UserRepository.

Production Code (UserService.java)
java
Copy
Edit
public class UserService {
    private final UserRepository userRepository;

    public UserService(UserRepository userRepository) {
        this.userRepository = userRepository;
    }

    public boolean isUserActive(String username) {
        User user = userRepository.findByUsername(username);
        return user != null && user.isActive();
    }
}
Mocking Example: Unit Test Using Mockito (UserServiceTest.java)
java
Copy
Edit
import org.junit.jupiter.api.Test;
import org.mockito.Mockito;
import static org.junit.jupiter.api.Assertions.*;
import static org.mockito.Mockito.*;

public class UserServiceTest {

    @Test
    void testIsUserActive_UserExistsAndActive_ReturnsTrue() {
        // Arrange: Mock the UserRepository and User object
        UserRepository mockRepository = mock(UserRepository.class);
        UserService userService = new UserService(mockRepository);

        User mockUser = new User("john_doe", true);
        when(mockRepository.findByUsername("john_doe")).thenReturn(mockUser);

        // Act: Call the method under test
        boolean isActive = userService.isUserActive("john_doe");

        // Assert: Verify the expected result
        assertTrue(isActive, "User should be active");

        // Verify that the method was called with the correct argument
        verify(mockRepository, times(1)).findByUsername("john_doe");
    }

    @Test
    void testIsUserActive_UserDoesNotExist_ReturnsFalse() {
        // Arrange
        UserRepository mockRepository = mock(UserRepository.class);
        UserService userService = new UserService(mockRepository);

        when(mockRepository.findByUsername("unknown_user")).thenReturn(null);

        // Act
        boolean isActive = userService.isUserActive("unknown_user");

        // Assert
        assertFalse(isActive, "User should not be active");

        // Verify that the repository method was called once
        verify(mockRepository, times(1)).findByUsername("unknown_user");
    }
}
Explanation of Mocking in This Test
Mock the Dependency: We create a mock of UserRepository using mock(UserRepository.class).

Stub Method Calls: We use when(mockRepository.findByUsername("john_doe")).thenReturn(mockUser); to define the behavior of the mocked object.

Call the Method Under Test: The isUserActive method is called.

Assert the Result: We use assertTrue() or assertFalse() to verify the expected outcome.

Verify Method Calls: The verify(mockRepository, times(1)).findByUsername("john_doe"); ensures the mocked method was called exactly once.

This approach ensures that we isolate the service logic and do not interact with the actual database while testing.


We can call multiple assert statememnts in the same test method.

Argument Captor in Unit Testing (Mockito ArgumentCaptor)
When testing methods that interact with dependencies, we often want to verify which arguments were passed to a mocked method. Mockito’s ArgumentCaptor allows us to capture and inspect those arguments.

Why Use ArgumentCaptor?
To verify the exact parameters passed to a mocked method.

Useful when working with methods that do not return values (e.g., void methods).

Helps validate complex objects passed as arguments.

Example Scenario: Capturing Arguments in a Service Method
We have a UserService that saves user details via UserRepository.

Production Code (UserService.java)
java
Copy
Edit
public class UserService {
    private final UserRepository userRepository;

    public UserService(UserRepository userRepository) {
        this.userRepository = userRepository;
    }

    public void registerUser(String username) {
        User user = new User(username, true);
        userRepository.save(user);  // Calls a void method
    }
}
Unit Test Using ArgumentCaptor (UserServiceTest.java)
java
Copy
Edit
import org.junit.jupiter.api.Test;
import org.mockito.ArgumentCaptor;
import static org.mockito.Mockito.*;
import static org.junit.jupiter.api.Assertions.*;

public class UserServiceTest {

    @Test
    void testRegisterUser_CapturesUserObject() {
        // Arrange: Create a mock repository and service
        UserRepository mockRepository = mock(UserRepository.class);
        UserService userService = new UserService(mockRepository);

        // Create an ArgumentCaptor to capture User objects
        ArgumentCaptor<User> userCaptor = ArgumentCaptor.forClass(User.class);

        // Act: Call the method under test
        userService.registerUser("john_doe");

        // Assert: Verify that save() was called and capture the argument
        verify(mockRepository).save(userCaptor.capture());
        User capturedUser = userCaptor.getValue();

        // Validate the captured User object
        assertEquals("john_doe", capturedUser.getUsername(), "Username should be correct");
        assertTrue(capturedUser.isActive(), "User should be active by default");
    }
}
Explanation of How ArgumentCaptor Works
Create a mock object:
UserRepository mockRepository = mock(UserRepository.class);

Create an ArgumentCaptor:
ArgumentCaptor<User> userCaptor = ArgumentCaptor.forClass(User.class);

Invoke the method under test:
userService.registerUser("john_doe");

Capture the argument passed to save(user) method:
verify(mockRepository).save(userCaptor.capture());

Retrieve the captured argument:
User capturedUser = userCaptor.getValue();

Perform assertions on the captured object:
We check if the captured User object has the correct username and is active.

Use Cases for ArgumentCaptor
1️⃣ Capturing Primitive Values
For methods that take String, int, or other primitives, we can use:

java
Copy
Edit
ArgumentCaptor<String> stringCaptor = ArgumentCaptor.forClass(String.class);
verify(mockService).sendMessage(stringCaptor.capture());
assertEquals("Hello", stringCaptor.getValue());
2️⃣ Capturing Multiple Arguments
For methods that take multiple arguments:

java
Copy
Edit
ArgumentCaptor<String> arg1 = ArgumentCaptor.forClass(String.class);
ArgumentCaptor<Integer> arg2 = ArgumentCaptor.forClass(Integer.class);

verify(mockService).updateUser(arg1.capture(), arg2.capture());

assertEquals("john_doe", arg1.getValue());
assertEquals(25, arg2.getValue());
3️⃣ Capturing Arguments for Multiple Calls
If the method is called multiple times, we can capture all values:

java
Copy
Edit
verify(mockRepository, times(2)).save(userCaptor.capture());
List<User> capturedUsers = userCaptor.getAllValues();
assertEquals(2, capturedUsers.size());
When to Use ArgumentCaptor?
✔ Use when testing void methods (e.g., methods that don’t return a value).
✔ Use when verifying complex objects (e.g., when an object is modified before passing to a method).
✔ Use when ensuring the correct argument is passed to a mocked method.

------------------------


Lecture | Mock MVC , Asserting in JSONs and Types of Doubles


 While testing controller we will mock the service.
 
 Json is converted into string and then sent to the front end.
 
  What is MockMvc?
MockMvc is a part of Spring Test module that allows you to test your Spring MVC controllers without starting the full HTTP server.

Instead of deploying your app in a servlet container like Tomcat, it lets you simulate HTTP requests (like GET, POST, PUT, DELETE) and assert the results, all within memory.

📦 Where does MockMvc belong?
MockMvc is part of:

xml
Copy
Edit
<dependency>
  <groupId>org.springframework.boot</groupId>
  <artifactId>spring-boot-starter-test</artifactId>
  <scope>test</scope>
</dependency>
You use it in unit or integration tests for your web layer.

✅ Why Use MockMvc?
No need to start the full server → faster tests

Simulate HTTP requests and responses easily

Test filters, interceptors, controller logic, and error handling

Verify JSON, status codes, headers, redirections, etc.

🏗️ How MockMvc Works
It loads the Spring MVC configuration (optionally).

It executes controller methods using mock HTTP requests.

It verifies the response content, status, headers, etc.

🧪 Basic Example
Say you have a controller like:

java
Copy
Edit
@RestController
@RequestMapping("/api")
public class HelloController {
    
    @GetMapping("/hello")
    public String sayHello() {
        return "Hello, World!";
    }
}
📋 Test using MockMvc
java
Copy
Edit
@SpringBootTest
@AutoConfigureMockMvc
public class HelloControllerTest {

    @Autowired
    private MockMvc mockMvc;

    @Test
    public void testSayHello() throws Exception {
        mockMvc.perform(get("/api/hello"))
               .andExpect(status().isOk())
               .andExpect(content().string("Hello, World!"));
    }
}
⚙️ Setup Options
There are two main ways to set up MockMvc:

1. Full Context – loads everything (used in integration tests)
java
Copy
Edit
@SpringBootTest
@AutoConfigureMockMvc
class MyControllerTest {
    @Autowired
    private MockMvc mockMvc;
}
2. Standalone Setup – loads only controller (used in unit tests)
java
Copy
Edit
@BeforeEach
void setup() {
    mockMvc = MockMvcBuilders.standaloneSetup(new MyController()).build();
}
🔧 Common Methods
Method	Description
.perform()	Performs a request (e.g. get(), post())
.andExpect()	Verifies the result
.andReturn()	Returns the MvcResult for further inspection
.param()	Adds query params
.content()	Sends request body (for POST/PUT)
.contentType()	Specifies request content type
.header()	Adds headers
📦 JSON Testing Example
java
Copy
Edit
@Test
void testPostJson() throws Exception {
    String json = "{\"name\":\"John\"}";

    mockMvc.perform(post("/api/user")
            .content(json)
            .contentType(MediaType.APPLICATION_JSON))
           .andExpect(status().isCreated())
           .andExpect(jsonPath("$.name").value("John"));
}
✅ When to Use MockMvc vs Other Approaches
Use Case	Suggested Tool
Testing controllers without full server	MockMvc
Testing full application with embedded server	@SpringBootTest(webEnvironment = WebEnvironment.RANDOM_PORT) + TestRestTemplate
Testing services without web layer	Plain JUnit + Mockito

Examples - 

 Example 1: GET request with query parameters
✅ Controller
java
Copy
Edit
@RestController
@RequestMapping("/api")
public class GreetController {

    @GetMapping("/greet")
    public String greet(@RequestParam String name) {
        return "Hello, " + name + "!";
    }
}
🧪 Test
java
Copy
Edit
@Test
void testGreetWithParam() throws Exception {
    mockMvc.perform(get("/api/greet").param("name", "Alice"))
           .andExpect(status().isOk())
           .andExpect(content().string("Hello, Alice!"));
}
🧪 Example 2: POST request with JSON
✅ Controller
java
Copy
Edit
@RestController
@RequestMapping("/api")
public class UserController {

    @PostMapping("/user")
    public ResponseEntity<User> createUser(@RequestBody User user) {
        user.setId(1L); // mock saving
        return ResponseEntity.status(HttpStatus.CREATED).body(user);
    }
}
java
Copy
Edit
@Data
@AllArgsConstructor
@NoArgsConstructor
class User {
    private Long id;
    private String name;
}
🧪 Test
java
Copy
Edit
@Test
void testCreateUser() throws Exception {
    String json = "{\"name\":\"John\"}";

    mockMvc.perform(post("/api/user")
            .content(json)
            .contentType(MediaType.APPLICATION_JSON))
           .andExpect(status().isCreated())
           .andExpect(jsonPath("$.id").value(1))
           .andExpect(jsonPath("$.name").value("John"));
}
🧪 Example 3: PUT request with headers
✅ Controller
java
Copy
Edit
@PutMapping("/user/{id}")
public ResponseEntity<String> updateUser(
        @PathVariable Long id,
        @RequestHeader("X-Auth-Token") String token,
        @RequestBody User user) {

    if (!"valid-token".equals(token)) {
        return ResponseEntity.status(HttpStatus.UNAUTHORIZED).body("Invalid token");
    }

    return ResponseEntity.ok("User updated: " + user.getName());
}
🧪 Test
java
Copy
Edit
@Test
void testUpdateUserWithHeader() throws Exception {
    String json = "{\"name\":\"Alice\"}";

    mockMvc.perform(put("/api/user/1")
            .header("X-Auth-Token", "valid-token")
            .content(json)
            .contentType(MediaType.APPLICATION_JSON))
           .andExpect(status().isOk())
           .andExpect(content().string("User updated: Alice"));
}
🧪 Example 4: DELETE request
✅ Controller
java
Copy
Edit
@DeleteMapping("/user/{id}")
public ResponseEntity<Void> deleteUser(@PathVariable Long id) {
    return ResponseEntity.noContent().build();
}
🧪 Test
java
Copy
Edit
@Test
void testDeleteUser() throws Exception {
    mockMvc.perform(delete("/api/user/1"))
           .andExpect(status().isNoContent());
}
🧪 Example 5: Error handling and exception
✅ Controller with exception
java
Copy
Edit
@GetMapping("/error-test")
public ResponseEntity<String> throwError() {
    throw new RuntimeException("Something went wrong!");
}
✅ Global Exception Handler
java
Copy
Edit
@RestControllerAdvice
public class GlobalExceptionHandler {

    @ExceptionHandler(RuntimeException.class)
    public ResponseEntity<String> handleRuntimeException(RuntimeException ex) {
        return ResponseEntity.status(HttpStatus.INTERNAL_SERVER_ERROR).body(ex.getMessage());
    }
}
🧪 Test
java
Copy
Edit
@Test
void testExceptionHandling() throws Exception {
    mockMvc.perform(get("/api/error-test"))
           .andExpect(status().isInternalServerError())
           .andExpect(content().string("Something went wrong!"));
}
🧪 Example 6: Validate request body (DTO + validation)
✅ DTO + Validation
java
Copy
Edit
@Data
class Product {
    @NotBlank
    private String name;

    @Min(1)
    private int quantity;
}
✅ Controller
java
Copy
Edit
@PostMapping("/product")
public ResponseEntity<String> addProduct(@Valid @RequestBody Product product) {
    return ResponseEntity.ok("Product added");
}
🧪 Test (Invalid input)
java
Copy
Edit
@Test
void testProductValidationError() throws Exception {
    String json = "{\"name\":\"\",\"quantity\":0}";

    mockMvc.perform(post("/api/product")
            .content(json)
            .contentType(MediaType.APPLICATION_JSON))
           .andExpect(status().isBadRequest());
}
🔁 Summary of Techniques Used
Feature	Method or Annotation
GET/POST/PUT/DELETE	mockMvc.perform(get(...)) etc.
Params	.param("key", "value")
JSON body	.content(json) + MediaType.JSON
Headers	.header("Header-Name", "value")
Status check	.andExpect(status().isOk()) etc.
Content check	.andExpect(content().string(...))
JSON path check	.andExpect(jsonPath("$.field"))
Validation	@Valid, @NotBlank, etc.
Error handling	@RestControllerAdvice + @ExceptionHandler


When jackson converts Json to Object it will always take all fields whose value is not null.

Doubles or Stubbing - 

What Are Test Doubles?
Think of a test double as a "stand-in" or "fake version" of a real object you use in production. It lets you test code in isolation — without needing the full database, network, or complex dependencies.

Just like a stunt double in movies takes the place of the real actor, test doubles take the place of real classes in unit testing.

🧩 Types of Test Doubles (according to Martin Fowler):
Type	What it does	Use case
Dummy	Just fills parameter slots	Not used, but required
Fake	Has working, simple implementation	In-memory DB
Stub	Returns pre-defined responses	Simulate specific behavior
Mock	Verifies method calls/interactions	Check if methods are called correctly
Spy	Partial mock + tracks interactions	Track behavior of real object
But in real-world Java testing, we mostly talk about Stubs, Mocks, and Spies using Mockito.

🧪 What Is Stubbing?
Stubbing is when you tell a test double (like a mock or spy):

"When this method is called with these arguments, return this specific value."

You’re basically programming fake behavior for your test object.

🔧 Example: Real Use Case
Let’s say you have a service like:

java
Copy
Edit
@Service
public class OrderService {

    @Autowired
    private PaymentGateway paymentGateway;

    public boolean processOrder(String orderId) {
        return paymentGateway.charge(orderId);
    }
}
Now PaymentGateway might be a complex class that talks to a real bank API — not something you want during unit tests.

✅ Step 1: Mock and Stub
In the test:

java
Copy
Edit
@ExtendWith(MockitoExtension.class)
class OrderServiceTest {

    @Mock
    private PaymentGateway paymentGateway;

    @InjectMocks
    private OrderService orderService;

    @Test
    void testProcessOrder_Successful() {
        // STUB: simulate method return
        when(paymentGateway.charge("order123")).thenReturn(true);

        boolean result = orderService.processOrder("order123");

        assertTrue(result);
    }
}
✔️ This is stubbing: you’re saying “when charge() is called with order123, return true”
No actual payment is made — it's just a stubbed response.

🔍 Mockito Cheatsheet for Stubbing
Mockito Syntax	What it does
when(obj.method()).thenReturn(value)	Returns value
when(obj.method()).thenThrow(new Exception())	Throws an exception
doReturn(value).when(mock).method()	Alternate syntax
doThrow(new Exception()).when(mock).method()	Throws exception
doNothing().when(mock).method()	For void methods
📦 Full Example: Service with Dependency
🤖 Real Classes
java
Copy
Edit
public class UserService {
    private UserRepository userRepository;

    public UserService(UserRepository repo) {
        this.userRepository = repo;
    }

    public User getUser(String id) {
        return userRepository.findById(id).orElse(null);
    }
}
🧪 Stubbed Test
java
Copy
Edit
@ExtendWith(MockitoExtension.class)
class UserServiceTest {

    @Mock
    private UserRepository userRepository;

    @InjectMocks
    private UserService userService;

    @Test
    void testGetUser() {
        User mockUser = new User("1", "Alice");
        when(userRepository.findById("1")).thenReturn(Optional.of(mockUser));

        User result = userService.getUser("1");

        assertEquals("Alice", result.getName());
    }
}
🧠 Why Stubbing Is Important
Avoid slow dependencies (DB, API, network)

Test edge cases (like nulls, errors)

Focus on one class at a time

Keep tests fast and reliable

✋ Gotchas with Stubbing
Over-stubbing can make tests too rigid

Don’t mock what you don’t own (prefer real objects for simple logic)

If method signatures change, stubs may silently break


-----------------------


Lecture | Authentication Vs Authorization, OAuth2 and JWT

Authentication - Authentication is the process of verifying the identity of a user or system.

Think of authentication like showing your ID card to enter a building.

Authorization - Authorization is the process of determining what a user is allowed to do after they have been authenticated.


Different types of Authentication - 

Password, Fingerprint, sessions limit, security questions, Authenticator apps.



OAuth - Open authentication 2.0 this was the first protocol came to market.

this is used for Authentication and Authorization. 

Sign in google is this feature.

OAuth (pronounced “oh-auth”) stands for Open Authorization.

It is a protocol (set of rules) that lets third-party applications access a user’s data without needing their password.

Why do we need OAuth?
Imagine you’re using an app like Canva, and it says:

"Do you want to sign in with your Google account?"

You click yes, and Canva gets access to your Google email and profile picture — but never sees your password.

This is made possible by OAuth.

Real-World Analogy
Let’s say you go to a hotel and want to access the gym.

You show your ID at the front desk (authentication).

The hotel gives you a keycard that allows gym access only (authorization).

You never gave the gym your ID directly.

The keycard works only for a limited time and only for certain areas.

That keycard = access token
The hotel = OAuth provider (like Google, Facebook, GitHub)


Actors in OAuth 2.0
Let’s define the key entities first:

1. Resource Owner
This is the end user (you or me) — the person who owns the data (e.g., Gmail inbox, profile info, GitHub repositories). They must authorize access to this data.

2. Client
This is the application trying to access the resource owner’s data on another service.
Example: A third-party app like Trello trying to access your Google Calendar.

The client is not the user — it’s your application.

3. Authorization Server
This is the server owned by the service provider (e.g., Google, GitHub) responsible for:

Authenticating the user

Asking the user for consent

Issuing access tokens and refresh tokens

Google's OAuth server is a typical Authorization Server.

4. Resource Server
This is where the user’s actual data lives. It is protected by the Authorization Server.

If you want to get a user’s Gmail inbox, Google’s Gmail API is the Resource Server.

It only gives access if a valid access token is presented.

🧭 Full Authorization Code Flow (Step-by-Step)
Let’s assume you’re building a web application that wants to fetch the user's email and profile picture from Google.

🔹 Step 1: Client Initiates Authorization Request
Your client app redirects the user to the authorization server (Google’s OAuth endpoint):

bash
Copy
Edit
GET https://accounts.google.com/o/oauth2/v2/auth?
  response_type=code
  &client_id=your-client-id
  &redirect_uri=https://yourapp.com/callback
  &scope=openid email profile
  &state=random-generated-string
response_type=code means we want an authorization code

client_id identifies your app

redirect_uri is where Google should send the user after login

scope defines what kind of access you want

state is used for CSRF protection

📍This is done in the user’s browser — they are redirected to Google.

🔹 Step 2: User Authenticates and Grants Consent
On the Authorization Server (Google), the user:

Logs in with their Google account

Sees a consent screen: “This app wants to access your email and profile”

If the user consents, the authorization server redirects back to the redirect_uri:

lua
Copy
Edit
https://yourapp.com/callback?code=abc123&state=random-generated-string
The code is a short-lived authorization code

The state value should match the one you sent to prevent CSRF

📍 Your frontend sends this code to your backend server for token exchange.

🔹 Step 3: Backend Exchanges Authorization Code for Access Token
Your backend server sends a POST request to the Authorization Server’s token endpoint:

bash
Copy
Edit
POST https://oauth2.googleapis.com/token

Headers:
  Content-Type: application/x-www-form-urlencoded

Body:
  code=abc123
  &client_id=your-client-id
  &client_secret=your-client-secret
  &redirect_uri=https://yourapp.com/callback
  &grant_type=authorization_code
If everything checks out, the authorization server responds:

json
Copy
Edit
{
  "access_token": "ya29.a0AfH6...",
  "expires_in": 3600,
  "refresh_token": "1//0gW...",
  "token_type": "Bearer",
  "id_token": "eyJhbGciOiJSUzI1..."
}
Access Token:
This is a bearer token that gives your backend access to the user’s data on the resource server.

It expires in a short time (e.g., 1 hour).

Refresh Token:
Optional — if included, allows your server to request a new access token later without user involvement.

🔹 Step 4: Access Protected Resource
Now your backend makes an API call to the Resource Server (Google API) with the access token:

bash
Copy
Edit
GET https://www.googleapis.com/oauth2/v2/userinfo
Authorization: Bearer ya29.a0AfH6...
Google responds:

json
Copy
Edit
{
  "id": "123456789",
  "email": "johndoe@gmail.com",
  "name": "John Doe",
  "picture": "https://..."
}
📍 Now your server has verified the user’s identity and can create or log in a user in your app.

🔹 Step 5: Optional — Refresh Token Flow
If the access_token expires, your server can use the refresh_token to get a new one:

arduino
Copy
Edit
POST https://oauth2.googleapis.com/token

Body:
  client_id=your-client-id
  &client_secret=your-client-secret
  &refresh_token=1//0gW...
  &grant_type=refresh_token
Google responds with a new access_token.

🔐 Security Considerations
Never expose access or refresh tokens to the frontend.

Always use HTTPS to protect token transmission.

Use the state parameter to defend against CSRF attacks.

Validate the ID token (JWT) if using OpenID Connect for authentication.

🧩 What's the Difference Between Authorization Server and Resource Server?
Authorization Server is responsible for:

Authenticating the user

Asking for user consent

Issuing access tokens

Resource Server:

Stores and protects the actual data

Accepts access tokens and returns data if the token is valid

In most systems (like Google), both are part of the same ecosystem, but conceptually and technically, they are different.

✅ Summary of Flow (Technically)
User is redirected to Authorization Server.

User logs in and consents.

Authorization Server sends an authorization code to your redirect URI.

Your server exchanges the code for an access token (and optionally refresh token).

Your server uses the access token to call the Resource Server and get user data.

You now know who the user is — securely.


JWT - 

JWT (JSON Web Token) is a compact, URL-safe token format used for securely transmitting information between parties as a JSON object. It is digitally signed using a secret (HMAC) or a public/private key pair (RSA or ECDSA).

In backend systems, especially with Spring Security or stateless microservices, JWT is widely used for authentication and authorization.

Structure of a JWT
A JWT consists of three parts, separated by dots:

php-template
Copy
Edit
<HEADER>.<PAYLOAD>.<SIGNATURE>
Each part is Base64URL encoded.

1. Header
The header typically looks like this:

json
Copy
Edit
{
  "alg": "HS256",
  "typ": "JWT"
}
alg: Algorithm used to sign the token (e.g., HS256 = HMAC with SHA-256).

typ: Indicates this is a JWT.

2. Payload (Claims)
This is the body of the token and contains claims: statements about the user and other data.

Example:

json
Copy
Edit
{
  "sub": "user123",
  "name": "John Doe",
  "roles": ["ROLE_USER"],
  "iat": 1712178000,
  "exp": 1712181600
}
Common claim fields:

sub: Subject (usually user ID)

iat: Issued at (timestamp)

exp: Expiration time

iss: Issuer

aud: Audience

Custom claims: like roles, email, etc.

3. Signature
The signature ensures that the token has not been tampered with.

If using HMAC SHA-256:

scss
Copy
Edit
HMACSHA256(
  base64UrlEncode(header) + "." + base64UrlEncode(payload),
  secret
)
This is what protects the integrity of the token.

Why Use JWT?
Stateless authentication: No session or server-side storage needed.

Self-contained: All required user information is embedded inside the token.

Portable: Can be passed in headers or as URL query parameters.

Widely supported: Standard format used across languages and platforms.

How JWT is Used in Java (Spring Security Context)
Authentication Flow with JWT:
User Logs In
User provides username and password to your /login endpoint.

Backend Validates Credentials
You authenticate using a UserDetailsService and AuthenticationManager.

JWT Is Generated
Once the user is authenticated, you create a JWT using a library like io.jsonwebtoken (JJWT):

java
Copy
Edit
String token = Jwts.builder()
    .setSubject(userId)
    .claim("roles", roles)
    .setIssuedAt(new Date())
    .setExpiration(new Date(System.currentTimeMillis() + 3600000))
    .signWith(SignatureAlgorithm.HS256, secretKey)
    .compact();
Token is Returned to Client
This token is returned to the frontend (in HTTP response or a Set-Cookie header).

Subsequent Requests
The client includes the token in the Authorization header:

makefile
Copy
Edit
Authorization: Bearer eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9...
Backend Validates Token
Every incoming request is filtered (e.g., using JwtAuthenticationFilter), where:

The token is extracted from the header.

Signature and expiration are validated.

User details are fetched and security context is populated.

This allows your controller endpoints to be protected by annotations like @PreAuthorize("hasRole('ADMIN')").

Validating JWT in Java
Typical JWT validation involves:

Verifying signature using the secret key

Verifying exp (expiration time)

Optionally checking iss, aud, and custom claims

If everything is valid, authentication is successful

Example using JJWT:

java
Copy
Edit
Claims claims = Jwts.parser()
    .setSigningKey(secretKey)
    .parseClaimsJws(token)
    .getBody();

String userId = claims.getSubject();
List<String> roles = claims.get("roles", List.class);
Security Best Practices
Always use HTTPS to avoid token leakage over the network.

Do not store tokens in localStorage (if using a frontend) — use HTTP-only cookies if possible.

Set short expiration (exp) and use refresh tokens if needed.

Use strong secrets or private/public keys depending on the algorithm.

Blacklist tokens on logout (optional but recommended for high-security systems).

Rotate signing keys periodically.

Use a secure, time-constant comparison method when comparing tokens or secrets to avoid timing attacks.

Common Algorithms
HS256: HMAC with SHA-256 (symmetric; same key for signing and verifying)

RS256: RSA with SHA-256 (asymmetric; private key to sign, public key to verify)

For microservices or distributed systems, asymmetric signing (RS256) is recommended. You can have a central auth server signing tokens with a private key and all services verifying them using the public key.

JWT vs Session-based Authentication
Feature	JWT	Session-Based Auth
Stateless	Yes	No (server must store sessions)
Scalability	High (good for microservices)	Harder to scale
Revocation	Hard to revoke	Easy (invalidate session)
Payload stored where	Inside the token	On server
Frontend storage	HTTP-only cookie / localStorage	Session ID in cookie
JWT is ideal for stateless APIs, especially RESTful or mobile applications.

Summary
A JWT is a signed token containing claims.

It can be used to authenticate and authorize users without maintaining server-side session state.

In Java, use jjwt, Spring Security filters, and custom providers to issue and validate tokens.

Proper handling of expiration, revocation, and signature validation is critical

---------------------



Lecture | Create UserAuthService, Implement Signup and Login API using BCrypt


What is BCrypt?
BCrypt is a password hashing algorithm designed to be:

Slow and computationally expensive (by design), which protects against brute-force attacks.

Salted — meaning each password is hashed with a random value (salt) to prevent rainbow table attacks.

Adaptive — you can configure how “expensive” (slow) it should be via a cost factor.

BCrypt was designed specifically for password hashing and is not reversible. You cannot "decrypt" a hashed password — you only verify if a plain password matches the hash.

Why use BCrypt in Java?
Spring Security includes built-in support for BCrypt via the BCryptPasswordEncoder class.

It hashes the password with a unique salt every time.

Even if the user enters the same password again, the hash will be different.

When verifying a login attempt, BCrypt will internally use the salt embedded in the hashed password to validate the plain input.

Internal Structure of BCrypt Hash
A sample BCrypt hash might look like this:

perl
Copy
Edit
$2a$10$Dow1HtH/JqtgN.yjqK1v9uUqKu3ybYfbt5o6IUGjbTxqLR96dxGx6
Breakdown:

$2a$: BCrypt version

10: Cost factor (log2 number of rounds, i.e., 2¹⁰ = 1024 rounds of hashing)

Dow1HtH/JqtgN.yjqK1v9u: 128-bit salt (base64 encoded)

qKu3ybYfbt5o6IUGjbTxqLR96dxGx6: Hashed password result

Using BCrypt in Java (Spring Boot Example)
1. Add the Encoder as a Bean
java
Copy
Edit
@Configuration
public class SecurityConfig {

    @Bean
    public PasswordEncoder passwordEncoder() {
        return new BCryptPasswordEncoder(); // default strength = 10
    }
}
You can also increase the strength:

java
Copy
Edit
new BCryptPasswordEncoder(12); // 2^12 = 4096 rounds
2. Hashing a Password During Signup
java
Copy
Edit
@Autowired
private PasswordEncoder passwordEncoder;

public void registerUser(String username, String plainPassword) {
    String hashedPassword = passwordEncoder.encode(plainPassword);

    User user = new User();
    user.setUsername(username);
    user.setPassword(hashedPassword);

    userRepository.save(user);
}
encode(...) hashes the password using a random salt and the configured cost factor.

The hash is stored in the database, not the original password.

3. Verifying Password During Login
java
Copy
Edit
public boolean login(String username, String inputPassword) {
    User user = userRepository.findByUsername(username);
    if (user == null) return false;

    String storedHashedPassword = user.getPassword();
    return passwordEncoder.matches(inputPassword, storedHashedPassword);
}
matches(...) will:

Extract the salt and cost from the stored hash

Hash the input password using the same salt and cost

Compare the resulting hash

Security Considerations
Always hash passwords — never store plain passwords.

BCrypt is slower on purpose — do not worry about performance for hashing; it’s meant to slow down brute-force attacks.

Never use raw == or .equals() to compare hashes; let BCrypt handle comparison via matches(...).

Consider higher cost factor (12–14) for more security, but benchmark based on your system.

Summary
BCryptPasswordEncoder is a secure, adaptive password hashing utility.

It’s ideal for use in authentication systems where passwords need to be stored securely.

Each password is hashed with a random salt.

You use encode() to hash and matches() to verify.

BCrypt is built-in and recommended by Spring Security.




--------------------------



Lecture | Generate and validate JWT in API


To generate JWT tokens -

add below dependancies - 

jjwt-api
jjwt-impl
jjwt-jackson

What is MacAlgorithm?
In the JJWT library, MacAlgorithm is an interface representing HMAC (Hash-based Message Authentication Code) algorithms.

These are symmetric algorithms, meaning:

The same secret key is used to sign and verify the JWT.

Common HMAC algorithms include:

HS256 → HMAC using SHA-256

HS384 → HMAC using SHA-384

HS512 → HMAC using SHA-512

So, MacAlgorithm defines the contract used internally to configure and apply these HMAC algorithms securely.

Where It Lives
In the newer versions of JJWT (0.11.0+), the API is now more type-safe and modular. Instead of passing a string like "HS256", you now work with algorithm types and instances.

MacAlgorithm is part of the JJWT crypto module:

java
Copy
Edit
import io.jsonwebtoken.security.MacAlgorithm;
import io.jsonwebtoken.security.Keys;
import io.jsonwebtoken.Jwts;
How It Works
You use a MacAlgorithm when creating or verifying JWTs signed with an HMAC algorithm.

Example: Creating a JWT with HS256
java
Copy
Edit
import io.jsonwebtoken.Jwts;
import io.jsonwebtoken.security.Keys;
import io.jsonwebtoken.security.MacAlgorithm;

import javax.crypto.SecretKey;
import java.util.Date;

public class JwtService {

    private final SecretKey secretKey = Keys.secretKeyFor(MacAlgorithm.HS256); // Generate secret key

    public String generateToken(String subject) {
        return Jwts.builder()
                .subject(subject)
                .issuedAt(new Date())
                .signWith(secretKey, MacAlgorithm.HS256)
                .compact();
    }
}
Explanation:
Keys.secretKeyFor(MacAlgorithm.HS256) generates a secure random key suitable for HS256.

signWith(secretKey, MacAlgorithm.HS256) explicitly tells JJWT what algorithm you're using.

This makes the JWT signing explicit and secure, avoiding the ambiguity or misuse of strings like "HS256".

Why MacAlgorithm Instead of Just a String?
Older versions used string names for algorithms:

java
Copy
Edit
signWith(SignatureAlgorithm.HS256, secretKey)
But:

You could accidentally use a mismatched key and algorithm.

You had no type-safety or clarity on requirements.

With MacAlgorithm:

It's type-safe

It's clearly scoped for symmetric (HMAC) algorithms

Enforces secure key usage

Common Algorithms You Can Use
All of these implement MacAlgorithm:

Algorithm	Purpose
MacAlgorithm.HS256	HMAC with SHA-256 (most common)
MacAlgorithm.HS384	HMAC with SHA-384
MacAlgorithm.HS512	HMAC with SHA-512
Use a stronger algorithm like HS512 if you want better security at the cost of larger tokens and slightly more CPU use.

Key Management (Important)
You must:

Use a secret key of sufficient length (e.g., 256-bit key for HS256).

Store it securely (e.g., in environment variables, config servers, or secrets manager).

Never expose the secret key publicly — it's symmetric.

java
Copy
Edit
SecretKey key = Keys.hmacShaKeyFor("your-256-bit-secret-goes-here".getBytes(StandardCharsets.UTF_8));
This should be exactly 256 bits (32 bytes) for HS256, or more for HS384/512.

Summary
MacAlgorithm is an interface in the JJWT crypto module used for HMAC-based signing.

It provides stronger typing and safer configuration than older string-based SignatureAlgorithm.

Use Keys.secretKeyFor(MacAlgorithm.XYZ) to generate secure keys for your JWTs.

Works with symmetric encryption — same key for both signing and verifying.

Use it when you're signing JWTs in stateless authentication systems (Spring Boot APIs, microservices, etc.).


Example of login and sign up -


 Dependencies (pom.xml)
xml
Copy
Edit
<dependencies>
    <dependency>
        <groupId>org.springframework.boot</groupId>
        <artifactId>spring-boot-starter-security</artifactId>
    </dependency>
    <dependency>
        <groupId>org.springframework.boot</groupId>
        <artifactId>spring-boot-starter-web</artifactId>
    </dependency>
    <dependency>
        <groupId>org.springframework.boot</groupId>
        <artifactId>spring-boot-starter-data-jpa</artifactId>
    </dependency>
    <dependency>
        <groupId>com.h2database</groupId>
        <artifactId>h2</artifactId>
        <scope>runtime</scope>
    </dependency>
    <dependency>
        <groupId>io.jsonwebtoken</groupId>
        <artifactId>jjwt-api</artifactId>
        <version>0.11.5</version>
    </dependency>
    <dependency>
        <groupId>io.jsonwebtoken</groupId>
        <artifactId>jjwt-impl</artifactId>
        <version>0.11.5</version>
        <scope>runtime</scope>
    </dependency>
    <dependency>
        <groupId>io.jsonwebtoken</groupId>
        <artifactId>jjwt-jackson</artifactId>
        <version>0.11.5</version>
        <scope>runtime</scope>
    </dependency>
</dependencies>
✅ Entity: User
java
Copy
Edit
@Entity
public class User {
    @Id
    @GeneratedValue(strategy = GenerationType.IDENTITY)
    private Long id;

    private String username;
    private String password;
    private String role;
}
✅ DTO: AuthRequest
java
Copy
Edit
public class AuthRequest {
    private String username;
    private String password;
}
✅ Repository
java
Copy
Edit
public interface UserRepository extends JpaRepository<User, Long> {
    Optional<User> findByUsername(String username);
}
✅ JWT Service
java
Copy
Edit
@Service
public class JwtService {

    private static final String SECRET = "VerySecureSecretKeyAtLeast32Chars!";
    private final SecretKey key = Keys.hmacShaKeyFor(SECRET.getBytes(StandardCharsets.UTF_8));
    private final MacAlgorithm algorithm = MacAlgorithm.HS256;

    public String generateToken(UserDetails userDetails) {
        return Jwts.builder()
                .subject(userDetails.getUsername())
                .claim("roles", userDetails.getAuthorities().stream()
                        .map(GrantedAuthority::getAuthority)
                        .toList())
                .issuedAt(new Date())
                .expiration(new Date(System.currentTimeMillis() + 3600000)) // 1 hour
                .signWith(key, algorithm)
                .compact();
    }

    public boolean isTokenValid(String token, UserDetails userDetails) {
        try {
            String username = extractUsername(token);
            Date expiration = extractExpiration(token);
            return username.equals(userDetails.getUsername()) && !isTokenExpired(expiration);
        } catch (JwtException | IllegalArgumentException e) {
            return false;
        }
    }

    public String extractUsername(String token) {
        return parseToken(token).getPayload().getSubject();
    }

    public Date extractExpiration(String token) {
        return parseToken(token).getPayload().getExpiration();
    }

    private boolean isTokenExpired(Date expiration) {
        return expiration.before(new Date());
    }

    private Jws<Claims> parseToken(String token) {
        return Jwts.parser()
                .verifyWith(key, algorithm)
                .build()
                .parseSignedClaims(token);
    }
}
✅ Security Config
java
Copy
Edit
@Configuration
@EnableWebSecurity
public class SecurityConfig {

    @Autowired
    private JwtAuthenticationFilter jwtFilter;

    @Autowired
    private UserDetailsServiceImpl userDetailsService;

    @Bean
    public SecurityFilterChain filterChain(HttpSecurity http) throws Exception {
        return http
                .csrf(csrf -> csrf.disable())
                .authorizeHttpRequests(auth -> auth
                        .requestMatchers("/api/auth/**").permitAll()
                        .anyRequest().authenticated()
                )
                .sessionManagement(sess -> sess.sessionCreationPolicy(SessionCreationPolicy.STATELESS))
                .addFilterBefore(jwtFilter, UsernamePasswordAuthenticationFilter.class)
                .userDetailsService(userDetailsService)
                .build();
    }

    @Bean
    public PasswordEncoder passwordEncoder() {
        return new BCryptPasswordEncoder();
    }

    @Bean
    public AuthenticationManager authManager(AuthenticationConfiguration config) throws Exception {
        return config.getAuthenticationManager();
    }
}
✅ JWT Filter
java
Copy
Edit
@Component
public class JwtAuthenticationFilter extends OncePerRequestFilter {

    @Autowired
    private JwtService jwtService;

    @Autowired
    private UserDetailsServiceImpl userDetailsService;

    @Override
    protected void doFilterInternal(HttpServletRequest request,
                                    HttpServletResponse response,
                                    FilterChain filterChain) throws ServletException, IOException {

        final String authHeader = request.getHeader("Authorization");
        final String jwt;
        final String username;

        if (authHeader == null || !authHeader.startsWith("Bearer ")) {
            filterChain.doFilter(request, response);
            return;
        }

        jwt = authHeader.substring(7);
        username = jwtService.extractUsername(jwt);

        if (username != null && SecurityContextHolder.getContext().getAuthentication() == null) {
            UserDetails userDetails = userDetailsService.loadUserByUsername(username);

            if (jwtService.isTokenValid(jwt, userDetails)) {
                UsernamePasswordAuthenticationToken authToken =
                        new UsernamePasswordAuthenticationToken(userDetails, null, userDetails.getAuthorities());

                authToken.setDetails(new WebAuthenticationDetailsSource().buildDetails(request));

                SecurityContextHolder.getContext().setAuthentication(authToken);
            }
        }

        filterChain.doFilter(request, response);
    }
}
✅ UserDetailsServiceImpl
java
Copy
Edit
@Service
public class UserDetailsServiceImpl implements UserDetailsService {

    @Autowired
    private UserRepository userRepository;

    @Override
    public UserDetails loadUserByUsername(String username) throws UsernameNotFoundException {
        User user = userRepository.findByUsername(username)
            .orElseThrow(() -> new UsernameNotFoundException("User not found"));

        return new org.springframework.security.core.userdetails.User(
            user.getUsername(),
            user.getPassword(),
            List.of(new SimpleGrantedAuthority(user.getRole()))
        );
    }
}
✅ Controller
java
Copy
Edit
@RestController
@RequestMapping("/api/auth")
public class AuthController {

    @Autowired
    private UserRepository userRepository;

    @Autowired
    private PasswordEncoder passwordEncoder;

    @Autowired
    private AuthenticationManager authenticationManager;

    @Autowired
    private JwtService jwtService;

    @PostMapping("/signup")
    public ResponseEntity<String> signup(@RequestBody AuthRequest request) {
        if (userRepository.findByUsername(request.getUsername()).isPresent()) {
            return ResponseEntity.badRequest().body("Username already exists");
        }

        User user = new User();
        user.setUsername(request.getUsername());
        user.setPassword(passwordEncoder.encode(request.getPassword()));
        user.setRole("ROLE_USER");

        userRepository.save(user);
        return ResponseEntity.ok("User registered successfully");
    }

    @PostMapping("/login")
    public ResponseEntity<Map<String, String>> login(@RequestBody AuthRequest request) {
        Authentication auth = authenticationManager.authenticate(
                new UsernamePasswordAuthenticationToken(request.getUsername(), request.getPassword())
        );

        UserDetails userDetails = (UserDetails) auth.getPrincipal();
        String token = jwtService.generateToken(userDetails);

        return ResponseEntity.ok(Map.of("token", token));
    }
}
✅ Sample Test
Signup

json
Copy
Edit
POST /api/auth/signup
{
  "username": "alice",
  "password": "alice123"
}
Login

json
Copy
Edit
POST /api/auth/login
{
  "username": "alice",
  "password": "alice123"
}
Use returned token:

makefile
Copy
Edit
Authorization: Bearer <token>


project repo

https://github.com/ak-s-0723/UserAuthenticationServiceFeb2025

-------------------------------

Lecture | Implement Login API using OAuth2 and Spring Security

docs - https://docs.spring.io/spring-authorization-server/reference/overview.html

OAuth doc - 

https://docs.spring.io/spring-authorization-server/reference/getting-started.html

Coding session.





----------------------------------


Lecture | What are cloud Providers , Overview of AWS and EC2, EBS

Cloud computing is a tech that allows computers to use and access compute resources like server storage, caches over internet.

------------------------

Lecture | Deployment on Elastic Bean Stalk

EC2 - 

What is EC2?
Amazon EC2 is a web service that provides resizable compute capacity—essentially, virtual machines called instances—in the cloud. It’s designed to make web-scale cloud computing easier for developers.

🧠 Key Concepts
1. Instances
Virtual servers for running applications.

You can launch different types of instances depending on your needs (compute-optimized, memory-optimized, GPU, etc.)

Instances are based on AMIs (Amazon Machine Images).

2. AMIs (Amazon Machine Images)
Pre-configured templates for EC2 instances.

Includes OS, application server, and applications.

You can use prebuilt AMIs (e.g., Ubuntu, Amazon Linux, Windows) or create your own custom AMIs.

3. Instance Types
Defined by CPU, memory, storage, and networking capacity.

Common families:

t2, t3, t4g – General purpose

c6g, c5 – Compute optimized

r6g, r5 – Memory optimized

p3, g4ad – GPU instances

4. EBS (Elastic Block Store)
Persistent block storage for EC2.

Acts like a hard drive that you can attach/detach from instances.

Data persists even after an instance is stopped or terminated.

5. Security Groups
Virtual firewalls for your instances.

Control inbound and outbound traffic (e.g., allow SSH (port 22), HTTP (port 80), etc.)

6. Key Pairs
Used for SSH access to Linux instances.

AWS creates a public/private key pair.

You keep the private key, and AWS stores the public key to allow login.

7. Elastic IP
Static IPv4 address associated with your account.

Useful if your instance needs a consistent public IP address.

8. EC2 Pricing Models
On-Demand – Pay per hour/second with no commitment.

Reserved – 1 or 3 year commitment, cheaper than On-Demand.

Spot Instances – Bid for unused EC2 capacity, up to 90% cheaper, but can be terminated anytime.

Savings Plans – Flexible pricing model, similar to Reserved.

⚙️ How EC2 Works (High-Level Steps)
Choose AMI – Select the operating system and software.

Choose Instance Type – Decide CPU, memory, and storage.

Configure Instance – Add networking, IAM roles, monitoring.

Add Storage – Use EBS volumes.

Configure Security Group – Set up firewalls.

Launch and Connect – Use SSH or RDP to connect to the instance.

🧰 Use Cases of EC2
Hosting websites and web apps.

Running backend APIs or microservices.

High-performance computing (HPC).

Machine learning model training (with GPU instances).

Batch processing and data analysis.

🧩 Integration with Other AWS Services
Auto Scaling – Automatically scale instances based on demand.

Elastic Load Balancer (ELB) – Distribute traffic across instances.

CloudWatch – Monitor metrics and logs.

IAM – Manage permissions and access.

VPC – Securely isolate your instances within a virtual network.

🛠 Example EC2 Use Case: Host a Web App
Launch a Linux EC2 instance.

SSH into the instance using your key pair.

Install NGINX or Apache.

Deploy your code.

Open port 80 in the security group.

Associate an Elastic IP for a stable URL.

Set up a domain and route traffic via Route 53 (optional).

✅ Pros
Highly scalable.

Customizable.

Wide instance variety.

Integration with AWS ecosystem.

⚠️ Cons
Can get expensive if not managed well.

Steeper learning curve than some PaaS options.

Needs management (patching, monitoring, scaling).

Elastic bean stalk - 

What is Elastic Beanstalk?
Elastic Beanstalk is a Platform as a Service (PaaS) offered by AWS that allows developers to deploy, manage, and scale web applications and services.

⚡ Key Idea:
You just upload your code, and Beanstalk automatically:

Handles deployment

Provisions resources (like EC2, Load Balancers, RDS, etc.)

Monitors your app

Scales it up/down as needed

🧠 How Elastic Beanstalk Works
✅ You provide:
Application code (Java, Node.js, Python, Go, PHP, .NET, Ruby, Docker, etc.)

Configuration (optional)

🌐 Beanstalk handles:
Infrastructure provisioning (EC2, VPC, ELB, RDS)

Load balancing and auto-scaling

Health monitoring

Application updates

Environment management

⚙️ Deployment Workflow
Create an Application in the Beanstalk console or CLI.

Choose the platform (e.g., Node.js, Java, Python, Docker, etc.).

Upload your code (ZIP file or through Git).

Beanstalk:

Launches an environment

Provisions EC2, ELB, Auto Scaling Group

Deploys your app

You access your app via a public URL.

💡 Components of Elastic Beanstalk
1. Application
The collection of environments, versions, and configurations.

2. Environment
A version of your app running on AWS resources.

Two types:

Web server environment (e.g., Nginx/Apache + EC2)

Worker environment (for background tasks using SQS)

3. Environment Configuration
All settings for the environment, such as instance type, VPC, auto scaling rules, software config, etc.

4. Application Version
A specific bundle of deployable code (ZIP file).

Stored in S3.

🚀 Supported Platforms
Java (Tomcat)

.NET (IIS)

Node.js

Python

Ruby

PHP

Go

Docker (single container or multi-container)

Packer-based custom platforms

📦 Deployment Options
All at once – Fast, but causes downtime

Rolling – Update in batches, maintains some availability

Rolling with additional batch – Adds new instances before killing old ones

Immutable – Deploy new instances, swap them in only if healthy

📊 Monitoring & Logs
Integrated with Amazon CloudWatch.

Health monitoring with traffic light indicators.

Logs from EC2 can be viewed/downloaded through the console.

🔒 Security & IAM
You can attach IAM roles to instances.

Use security groups to control access to environments.

Integrate with RDS, S3, Secrets Manager, etc., securely.

💰 Pricing
Elastic Beanstalk itself is free – you only pay for the underlying resources (EC2, ELB, RDS, etc.).

🧰 Use Cases
Quickly deploy REST APIs and web apps

Handle backend services with minimal DevOps

Deploy scalable microservices

Run Docker containers with zero manual infrastructure

✅ Pros
Easy to use (great for beginners and rapid development).

Fully managed infrastructure.

Supports many platforms and CI/CD.

Built-in scaling, monitoring, and load balancing.

⚠️ Cons
Less control over infrastructure.

Not suited for highly customized architectures.

Can be overkill for very simple apps, or too limited for complex ones.

🔧 Example Scenario: Deploying a Node.js App
Create a ZIP of your app (e.g., app.js, package.json).

Go to Beanstalk console → Create Application.

Choose Node.js as platform.

Upload ZIP → Click deploy.

Beanstalk sets up:

EC2 instance with Node.js

Security group

Load balancer

Auto scaling group

Monitoring dashboard

Access your app at http://your-env.elasticbeanstalk.com.

---------------------------

Lecture | VPC, RDS and Final Deployment


VPC - virtual private cloud - 


What is a VPC (Virtual Private Cloud)?
Imagine you're moving into a massive data center — like AWS — but you don’t want your machines and data to be mixed in with everyone else's. Instead, you want your own private, secure space inside this massive facility. That's exactly what a VPC gives you: your own virtual network inside AWS that’s isolated from others, where you have full control over how your resources communicate with each other and the outside world.

In simpler terms:

A VPC is like a private data center inside AWS, and you control everything: network configuration, access to and from the internet, internal communication between your servers, etc.

🧠 Why does AWS offer VPC?
In real-world networking, every data center has firewalls, routers, switches, IP ranges, and public/private segments. AWS provides all these capabilities virtually, so you don’t need to physically manage them — but you still get all the control.

VPC lets you:

Define your own IP address range

Divide the network into subnets

Control access using firewall rules

Decide whether a server should be public or private

Control routing and internet access

Without a VPC, your resources would all be public or in some shared network space — which is a security and management nightmare.

🔧 How does a VPC work?
When you create a VPC, you’re creating a container for networking resources. But the container is empty by default. You then:

Choose an IP address range for your VPC.

Create subnets inside that range.

Add routing rules that define how traffic flows.

Attach gateways if you want the internet or another network to talk to your VPC.

Launch instances (like EC2 servers) and assign them to subnets.

Let’s unpack each of these.

🔹 CIDR Block (IP Address Range)
This is the foundation of your VPC. When you create one, you define the range of private IP addresses that AWS can use inside your network. This is done using CIDR notation (e.g., 10.0.0.0/16).

What this means:

You're reserving IP addresses from 10.0.0.0 to 10.0.255.255.

That's around 65,000+ private IP addresses that only your VPC will use.

This range doesn’t overlap with the public internet and is not exposed to the outside world unless you allow it.

🔹 Subnets (Smaller Network Zones)
Once your VPC has an IP range, you divide it into subnets. A subnet is a smaller block of IP addresses.

Why subnets?

Because not all your services need to be exposed to the internet. For example:

Your web servers (frontend) need to talk to the internet — these go into public subnets.

Your databases, internal APIs, or worker services don’t need to be exposed — these go into private subnets.

Every subnet:

Is associated with a specific availability zone (AZ) in AWS (for fault tolerance).

Is connected to a routing table that defines where packets go.

🔹 Routing Tables (Traffic Controllers)
Inside your VPC, you use routing tables to control how traffic moves between subnets and beyond.

A routing table has rules like:

Traffic destined for the internet → go through the Internet Gateway

Traffic between private subnets → stay inside the VPC

Traffic to another network → go through a VPN Gateway or Peering Connection

Without routing rules, your EC2 instance doesn’t know how to reach any destination — even the internet.

🔹 Internet Gateway (Access to the Internet)
By default, nothing in your VPC can access the internet.

If you want EC2 instances in a subnet to connect to the internet (say, to install packages, access APIs, or serve a website), you need to:

Attach an Internet Gateway (IGW) to your VPC.

Create a route in your routing table that sends traffic to the IGW.

Make sure your EC2 instances have public IPs.

Without all three, even if you think your instance is public, it won’t have internet access.

🔹 NAT Gateway (One-Way Internet for Private Subnets)
Now let’s say you have a database or backend service in a private subnet that shouldn't be exposed to the internet, but it needs to connect to the internet (e.g., to download updates or call a 3rd party API). You don't want to give it a public IP.

Here’s where the NAT Gateway comes in.

It sits in a public subnet.

Private subnet instances send outbound traffic to it.

The NAT gateway sends the request to the internet on their behalf and returns the response.

But inbound traffic from the internet can’t reach those private instances.

So, it's like a secure middleman.

🔹 Security Groups (Virtual Firewalls for Instances)
A Security Group is a set of firewall rules that define:

What traffic is allowed into an EC2 instance (inbound)

What traffic is allowed out of an instance (outbound)

They are stateful, which means:

If an inbound rule allows traffic from X, the response can automatically go back to X.

They apply to individual resources (like EC2 instances), not subnets.

🔹 Network ACLs (Firewalls for Subnets)
A Network Access Control List (NACL) works similarly to security groups but on a subnet level.

They are stateless — meaning if you allow inbound traffic, you must explicitly allow outbound too.

You can block specific IPs here — something you can’t do with Security Groups.

Most people use Security Groups more often, and only use NACLs when they need more granular control.

🔹 Peering, VPNs, and Direct Connect
Once you’ve mastered the basic VPC structure, you can connect it to:

Other VPCs using VPC Peering

On-premise data centers using VPN Gateways

AWS Direct Connect for high-speed, private links

This turns your VPC into a hybrid cloud network.

Putting It All Together – Real-World Example
Imagine you're building a real web application:

You create a VPC with 10.0.0.0/16.

You create two subnets:

10.0.1.0/24 – public

10.0.2.0/24 – private

You launch an EC2 instance in the public subnet, attach a public IP, and update the route table to use the Internet Gateway.

You launch a database (RDS) in the private subnet.

You create a NAT Gateway so the RDS instance can fetch updates.

You add Security Groups to:

Allow HTTP (port 80) traffic to the EC2 instance.

Allow only the EC2 instance to connect to the database.

You deploy your app and it’s live — secure, structured, and scalable.

Summary in One Sentence
VPC is your private network inside AWS where you have complete control over connectivity, security, and communication between your cloud resources — as if you're running your own data center, but without owning physical hardware.



Security Groups - 

What Is a Security Group in AWS?
A Security Group (SG) is a virtual firewall that controls the traffic to and from your AWS resources, especially EC2 instances. It is attached at the instance level, and not the subnet or network level. It defines what kind of traffic is allowed to reach your resource, and what traffic your resource is allowed to send out.

Think of it as the first line of defense for your EC2 — it decides who gets to talk to your server and on which ports.

🔁 Stateful Firewall
The most important thing to know:
Security Groups are stateful.

This means:

If an inbound rule allows traffic in (say, an SSH connection from your IP), the response is automatically allowed out, even if there’s no explicit outbound rule.

Likewise, if you allow outbound traffic to some IP, the response from that IP is automatically allowed in.

This is in contrast to Network ACLs, which are stateless — they require rules for both inbound and outbound traffic independently.

📌 Default Behavior of Security Groups
When you create a new security group:

No inbound traffic is allowed. Nothing can connect into your EC2 instance.

All outbound traffic is allowed. Your EC2 instance can reach the outside world, unless you explicitly change it.

This is a secure default because:

Nothing can connect to your EC2 until you say so.

But your EC2 can still download packages, reach APIs, etc., without any setup.

⚙️ Components of a Security Group
Every security group has two sets of rules:

1. Inbound Rules
Define what traffic can come into the EC2 instance:

From which IP address or CIDR block

On which protocol and port (e.g., TCP 22 for SSH, TCP 80 for HTTP)

Examples:

Allow SSH from your laptop → TCP, port 22, source YOUR_IP/32

Allow web traffic from the internet → TCP, port 80, source 0.0.0.0/0

2. Outbound Rules
Define what traffic the instance is allowed to send outward to the network or internet.

Examples:

Allow all outbound connections → All traffic, destination 0.0.0.0/0

Restrict outbound connections to just internal VPC → destination 10.0.0.0/16

🚀 How Are Security Groups Applied?
Security groups are applied directly to resources — usually EC2 instances.

You can attach multiple security groups to a single instance.

Each security group can be attached to multiple instances.

When you have multiple SGs on an instance:

The rules are combined: if any SG allows traffic, it’s allowed.

Example:

SG1 allows SSH (port 22)

SG2 allows HTTP (port 80)

Your EC2 will accept traffic on both 22 and 80

🧱 How Rules Are Evaluated (Key Points)
Default deny for inbound traffic: if no rule matches, the traffic is denied.

Default allow for outbound traffic: unless you restrict it.

All rules are evaluated as “allow” rules. There are no explicit "deny" rules.

Security Groups are not ordered. All rules are evaluated together; there’s no first-match-wins logic.

You cannot create a “block this IP” rule. If you want that, use NACLs (Network ACLs).

🧠 Deep Concept: Security Group References
One powerful feature: instead of allowing traffic from an IP, you can allow traffic from another security group.

Why is this useful?

Let’s say:

Your Web Server EC2 is in SG A.

Your Database EC2 is in SG B.

You want the DB to accept connections only from your web server, not the entire internet.

✅ You can add an inbound rule to SG B:

Allow traffic on port 3306 (MySQL) from security group A.

This is dynamic — if you later launch more web servers and assign them to SG A, they’ll automatically be allowed to talk to the database.

📦 Practical Use Cases
🧰 Use Case 1: Web Server
Let’s say you're running a public website:

Inbound rules:

Allow HTTP (80) and HTTPS (443) from anywhere (0.0.0.0/0)

Allow SSH (22) from your IP only

Outbound rules:

Allow all (or restrict to VPC/internet depending on setup)

🧰 Use Case 2: Private Database Server
Inbound rules:

Allow MySQL (3306) from your web server’s security group

Outbound rules:

Allow only responses (or maybe allow outbound DNS, NTP if needed)

🧪 Real Example
Suppose you want to connect to your EC2 from your laptop over SSH. You need to:

Create a Security Group (e.g., my-sg)

Add an inbound rule:

Protocol: TCP

Port range: 22

Source: YOUR_IP/32 (not 0.0.0.0/0, unless you know what you're doing)

Attach that SG to your EC2.

Now only your IP can SSH into the EC2 — no one else.

🧼 Best Practices
Principle of least privilege: only open the ports and IPs you need.

Never allow all traffic (0.0.0.0/0) on port 22 unless it's temporary and you remove it later.

Use SG references instead of IPs for internal communication — it’s cleaner and more flexible.

Group by purpose, not by instance — e.g., one SG for "web access", one for "admin access", one for "DB access".

Monitor your SGs over time — you might leave open rules you no longer need.

✨ Summary (in words, not bullets)
A Security Group in AWS is a stateful virtual firewall attached to your EC2 instances and other supported services. It controls which traffic is allowed to reach your instance (inbound), and what traffic your instance can send out (outbound). You define rules based on protocol, port, and source/destination IP or security group. The default posture is secure: no inbound is allowed, and outbound is open. You can assign multiple SGs to an instance and use SG-to-SG communication rules for flexibility and modularity. SGs are not a complete replacement for NACLs, but they are essential for instance-level traffic control and core to securing any AWS deployment.


RDS - 

 First, the Concept: What Is RDS Really?
Amazon RDS (Relational Database Service) is Amazon’s answer to a recurring problem in software systems: managing a relational database is a headache. Databases are the core of most applications, but keeping them performant, secure, resilient, and available is hard. RDS exists to abstract away that pain.

It gives you a managed relational database in the cloud. That means you still get a regular database — MySQL, PostgreSQL, SQL Server, etc. — but you don’t deal with the gritty details like installing it, setting up backups, managing failovers, or worrying about storage corruption or disk failures.

Under the hood, RDS is AWS taking over the infrastructure layer, the operating system layer, and part of the database engine layer, and handling the heavy lifting for you.

🛠️ What's Happening Internally?
When you create an RDS database, AWS provisions an EC2 instance under the hood. But that instance is invisible to you. You don’t have SSH access. It’s hardened and locked down. On that instance, AWS installs and configures the database engine of your choice — MySQL, PostgreSQL, etc.

But the genius of RDS is in what happens around that instance:

Storage is decoupled
The actual database data is not stored on a local disk like a traditional VM. It’s stored on Amazon EBS (Elastic Block Store) volumes. These are durable, persistent storage blocks that survive restarts and are replicated across multiple storage nodes behind the scenes.

Control Plane and Data Plane are separated
The control plane is the system AWS uses to manage your database: create, patch, backup, scale, failover, etc. The data plane is the actual DB engine processing SQL queries. AWS isolates these two, which lets them upgrade or manipulate infrastructure without interrupting your database in some cases.

State and configuration are stored in a service registry
When you create an RDS instance, AWS stores metadata (version, region, subnet, VPC, instance size, backup window, parameter groups, etc.) in a central registry that RDS uses to orchestrate everything.

Replication and High Availability are built into the system
If you enable Multi-AZ (availability zones), RDS spins up a synchronous standby replica in a separate physical data center. It applies changes in real-time via a replicated transaction log stream. If the primary fails, the system quickly promotes the standby.

Backups are coordinated at the storage layer
Unlike traditional manual dumps (mysqldump, pg_dump), RDS backups happen via block-level snapshots of the underlying EBS volumes. This means backups are fast, consistent, and don’t disrupt performance. For point-in-time recovery, RDS keeps a log of all transactions and can replay them onto a base snapshot to restore to any moment within your retention window.

Scaling is done by instance replacement or read replication
When you "scale up" your database, RDS actually replaces your instance with a bigger one, using a blue/green deployment style to minimize downtime. For read scaling, it spins up read-only replicas, which asynchronously replicate changes from the primary.

Security is embedded at every layer
RDS integrates tightly with IAM (identity and access management), KMS (encryption key management), and VPC (network isolation). You can encrypt your data at rest and in transit. You control who can talk to your database using security groups, which act like virtual firewalls. And AWS rotates encryption keys and credentials if you opt in.

🧩 Why Is RDS So Critical?
When you use a traditional self-managed database, you’re dealing with a fragile ecosystem:

OS patches might reboot your server

Storage failures might corrupt your data

Mistuned parameters might silently kill performance

Backups might break or never complete

High availability is complex and error-prone

RDS abstracts all of that into a resilient, repeatable, automated platform.

It’s like moving from managing your own water supply and plumbing to having a utility company that guarantees clean, pressure-controlled water on demand. You lose some low-level control, but gain enormous safety and simplicity.

🔍 What Happens During a Failover?
Let’s go into more detail here, because it shows the power of the architecture.

If your primary database crashes:

The health check service in the RDS control plane detects the failure within seconds

It automatically promotes the standby to become the new primary

It updates the DNS entry for the endpoint (the hostname you use to connect) to point to the new primary

Your application reconnects and continues without needing to know what changed

And this all happens without you writing a single line of failover logic. Internally, AWS uses routing layers and DNS tricks to handle this elegantly and quickly.

🧠 Performance Monitoring and Optimization
RDS doesn't just run your DB. It helps you understand and tune it:

It tracks performance metrics like CPU, memory, disk I/O, DB connections using CloudWatch

It provides Performance Insights, which logs query activity, wait times, and bottlenecks

You can attach a parameter group to tune the DB engine (e.g., buffer sizes, query cache, etc.)

You can even use Enhanced Monitoring to see what's happening inside the underlying EC2 instance — down to OS processes and thread-level activity

So although you can’t SSH into the instance, AWS still gives you visibility into its behavior.

🧠 Philosophical Shift: Pets vs Cattle
RDS embodies a core cloud principle: treat infrastructure like cattle, not pets.

In the past, you named your database server, patched it carefully, and coddled it like a pet. If it died, you cried.

With RDS, you treat your DB like cattle: reproducible, replaceable, automated. If it fails, you launch a new one with the same config, and keep moving. This shift — from artisanal to industrial — is part of what makes RDS so transformative.

🧱 So What Is RDS, Really?
At its heart, RDS is a platform that turns relational databases into durable, scalable cloud-native services. It gives you the reliability and performance of traditional databases, but with the elasticity and automation of the cloud.

It’s not just a tool for storing data — it’s a reimagining of what database operations look like in a modern, distributed, self-healing cloud architecture.

---------------------------


Lecture | Redis Integration in Project


Spring boot with redis - https://stackabuse.com/spring-boot-with-redis-hashoperations-crud-functionality/

install redis on windows - 

https://redis.io/docs/latest/operate/oss_and_stack/install/archive/install-redis/install-redis-on-windows/

what is caching - 

Caching is a technique used to store frequently accessed data in a temporary and fast-access storage (called a cache) so that future requests for that data can be served more quickly.

caches are faster and registers are faster than caches.

add below depedency to the pom - 

Spring boot starter data redis 

add below properties to the application.properties -

spring.data.redis.host = localhost
spring.data.redis.port = 6379


We also need to create the object of the redis in spring boot app - 

class - 

@Configuration
public class RedisConfig {

@Bean
RedisTemplate<key_data_type, value_data_type or Object> redisTemplate(RedisConnectionFactory redisConnectionFactory ) {

RedisTemplate<String, Object> redisTemplate = new RedisTemplate<>();

redisTemplate.setConnectionFactory(redisConnectionFactory);

return redisTemplate;
}

}


redis internally stores the data in the binary format and when we try to put the object into it. It converts the object into binary format by using serialization and while fetching object back from the redis it will deserialize the object and give the object.

 What is Redis?
Redis (REmote DIctionary Server) is an in-memory key-value store often used for caching, real-time analytics, session storage, and pub-sub systems.

🧠 How Redis Stores Data Internally
Redis stores everything as a key-value pair, but the underlying data structure used for values depends on the Redis data type and size of the data.

🧩 Core Components:
Keys

Stored as binary-safe strings (just like a Java String)

Internally managed using a dictionary (hash table)

Values
Can be stored using different internal data structures depending on the data type and size:

📦 Redis Data Types & Internal Representations

Redis Type	Internal Structure Used
String	SDS (Simple Dynamic String) — resizable byte arrays
List	QuickList (combines linked list + compressed ziplist)
Set	Hash Table or IntSet (if all elements are integers)
Hash	ZipList (compact) or Hash Table (for large data)
Sorted Set	SkipList + Hash Table
Bitmap	Raw byte arrays (bit-level operations)
HyperLogLog	Probabilistic algorithm-based structure
Stream	Radix tree + linked list for message IDs
🔁 Example: What Happens Internally?
Let’s say you run:

shell
Copy
Edit
SET user:1 "John"
Internally:

Redis creates a key user:1 (as an SDS string)

Value "John" is also stored as an SDS

It stores both in a dictionary (hash table)

📉 Memory Optimization
Redis automatically switches internal representations for memory and speed optimization:

Small hashes use ziplist (compact memory layout)

Small sets with only integers use intset

Lists internally compress adjacent elements using quicklist with ziplist nodes

You can configure these thresholds with Redis settings like:

ini
Copy
Edit
hash-max-ziplist-entries
hash-max-ziplist-value
📁 Persistence (RDB / AOF)
Although Redis is in-memory, data can be persisted to disk using:

RDB (snapshotting): periodic dump of data

AOF (Append Only File): logs every write command

🧠 Summary:
Keys are stored in a hash table

Values are stored using optimized internal structures

Redis chooses the best format dynamically for performance and memory efficiency



why redis takes two keys while putting the data in the redis - 


	Short Answer:
Redis itself only deals with one key.
But when using Redis through a framework like Spring Cache, it may look like you’re passing two keys because:

One is the Redis key (the main key used to store/retrieve)

The second is often part of a composite key or used in key generation logic

🔍 Example with Spring Boot (@Cacheable):
java
Copy
Edit
@Cacheable(value = "userCache", key = "#userId")
public User getUserById(String userId) {
    // fetch from DB if not in cache
}
Here:

"userCache" is the cache name (or namespace/grouping)

#userId is the actual key used inside that cache

So internally, Spring might generate a final Redis key like:


userCache::12345
Looks like two parts, but it’s just one Redis key under the hood.

🔧 Breakdown:

What You See	What Redis Sees
@Cacheable(value, key)	One final key string
redisTemplate.opsForHash()	Redis hash with field key
redisTemplate.opsForValue()	Simple key-value
🛠 With RedisTemplate and Hashes:


redisTemplate.opsForHash().put("userCache", "123", userObject);
"userCache" = Redis key (it's a hash)

"123" = field inside the hash

So Redis stores a hash structure:
Key = "userCache"
Field = "123"
Value = serialized userObject

This helps with grouping — all users in a single Redis hash.

✅ So Why Two Keys (Visually)?
It depends on how you're storing:

String key-value (via opsForValue()):


redisTemplate.opsForValue().set("user:123", user);
Hash key-field-value (via opsForHash()):


redisTemplate.opsForHash().put("users", "123", user);
So it’s not that Redis itself requires two keys — it's just that some APIs or patterns (like hashes or caching) involve compound structures.

Want a TL;DR?
Redis natively uses one key, but

Frameworks (like Spring) or data types (like Hash) might make you feel like you’re using two

It helps with organization, grouping, and namespacing

---------------------------------------------------



-------------------------------

Lecture | Searching , Paging and Sorting APIs Implementation in Product Catalog Service



3
4
5
6
7
8
9

---------------------------------------------------

Lecture | DSA: Arrays 3: Interview Problems
 
 Salt, pepper, lime, steamed food, salad, pulses, beans
 
 Meditation .
 
 Omega 3 fatty acid food.
 
 Omega 3 is non imflamatory and omega 6 is imflamatory.
 
 Need to get Omega 3 and Omega 6 levels checked. 
 
 Omega test, blood test,  microbiome test, genetic test.

 Omega 3 - Chia seed, flex seeds or flex seed oil, walnuts, dark chocolate - organic cocoa powder, dark coffee -   
 
 
 Research nootropics
 
 --------
 
 
 Creatine, Whey proteins, butter fruit (Avocado), Tofu, Dal, natural proteins.

Take 5 gm of creatine with breakfast. Test serum creatine levels to check if creatine is messing with it and consult a nephologist. 


Blood tests for gym - 

High sensitivity c-reactive protein (HS-CRP)- this is the inflammation marker, It measures sensitive marker of chronic inflammation and cardiovascular risk. Elevated hs-CRP levels are linked to heart disease, autoimmune conditions and metabolic dysfunction. Even low grade inflammation can accelerate aging and increase disease risk. 


Optimal range - 

Below 1.0 mg/L - Low risk
1.0 - 3.0 mg/L - Moderate risk
above 3.0 mg/L - high risk

2) RBC zinc - the bioavailable zinc test. It measures zinc inside red blood cells (RBC), giving a more accurate reflection of long term zinc status than serum zinc. 

It is crucial for immunity, brain function, testosterone production and wound healing. Deficiency can lead to poor immunity, fatigue, hair loss and lower testosterone levels. 

Optimal range - 

5.0 - 7.5 mg/L

3) Free T4 - the inactive thyroid hormone. It measures the inactive form of thyroid hormone that gets converted to T3, the active form. 

Low T4 can indicate hypothyroidism, which causes fatigue, weight gain, cold intolerance and sluggish metabolism. If T4 is normal but T3 is low, conversion issues ( like stress, poor liver function, or nutrient deficiencies) may be at play which is why its essential to measure both. 

Optimal range - 

0.8 to 1.8 ng/dL

4) Free T3 - the active thyroid hormone. It measures the active form of thyroid hormone responsible for metabolism, energy and fat loss. Low T3 can cause brain fog, fatigue, weight gain and cold extremities, even if T4 is normal. Chronic stress, fasting or low carb diets can impair T4 to T3 conversion leading to sluggish metabolism. 

Optimal range - 

3.5 to 4.2 pg/mL

5) Prolactin - the hidden hormone that lowers testosterone. It measures a harmone produced by the pituitary gland, which affects testosterone, dopamine and libido. 

High prolactin can supress testosterone, lower libido and even cause erectile dysfunction in men, elevated levels may indicate dopamine deficiency, stress or pituitary tumors ( prolactinomas). 

Optimal range - 

men - 4 - 15 ng/mL

6) Homocysteine - the cardiovascular and brain health marker - It measures amino acid linked to heart disease, stroke and cognitive decline. High homocysteine levels can damage blodd vessels and increase heart attach risk. It can indicate poor methylation due to vitamin B deficiencies. 

Optimal range -
Below 7 umol/L - ideal
above 15 umol/L - high risk

7) Free testosterone the hormone that drives performance - It measures the unbound, bilavailable testosterone that directly affects muscle mass, energy, libido and mood. Low free testosterone can cause fatigue, depression, low motivation and reduced muscle growth. Total testosterone does not tell the full story, free T is what your body can actually use. 

Optimal range - 

Men - 15 - 21 pg/mL

------------------------------

Soni's -


Bit Manipulation 1 - 


Problem solving framework - 

Start 25 mins timer

Read problem statement carefully

Understand the problem and identify the inputs and outputs

Think of a brute force approach to solve the problem

Optimise the brute force to reduce time complexity

if you are able to solve the problem within 25 mins then move on to next problem.

if you are not able to solve the problem within 25 mins then bookmark that problem for revision on upcoming sundays.

Take hints progressively from below points - 

1) Check if the class notes if we have solved the specific question in the class.

2) Utilise hints
3) Watch the video
4) Watch the video solution.


Solve this question and Ask yourself what concept if I knew would have helped me solve this question on my own without any help and note it down.



And move on to next question.

Bitwise operators - 

& - AND
| - OR 
^ -XOR 
~ - negate 
<< - Left shift
>> - Right shift



1 & 1 = 1
2 & 1 = 0
3 & 1 = 1
4 & 1 = 0

Basic properties - 

Even/Odd numbers - 

no & 1 == 1 then no is odd
no & 1 == 0 then no is even

Observations - 

A &  1 == 1 then A is odd
A & 1 == 0 then A is even


2) 

A & 0  = 0

3) 

A & A = A

4) 

A | 0 = A

5) 

A | A = A

6) 

A ^ 0 = A

A ^ A = 0

7) Comutative property - 

a & b = b & a
a ^ b = b ^ a
a | b = b | a

8) Associative property - 

a & b & c = (a & b) & c = a & ( b & c) and same goes for xor and or as well.

Left shift operator << ->

shifts the bits of the number by provided number of positions towards the left.

its like multiplying the number by 2 for each shift of bits by one position.

a << 1 = a * 2
a << 2 = a * 2^2
a << 3 = a * 2^3
a << n = a * 2^n

1 << n = 2^n


Right shift operator >> ->

Shifts the bits to the right of the number by provided number of bits.

Its like dividing the number by 2 for each shifting of a bit by one position.



Power of left shift operator - 


1) Set 5th bit of A - 

A = 21;

bit = 1 << 5;

ans = A | bit;

2) Toggle the ith bit - 

bit = 1 << i;

ans = A ^ bit ;


3) Unset ith bit - 

A = 21;

bit = 1 << i;

bit = ~bit;

ans = A & bit;


Q - Check whether ith bit is set or not.

A = 21;

i = 3;

int setBit = 1 << i;

 ans = A & setBit; 

if( ans > 0) {

bit is set.

}else{

bit is not set.

}


Q  - Given an integer N. Count the set bits in N.


Idea - 

check for each bit in range 0 to 31 if the bit is set then count++.

cnt = 0;

for( bit from 0 to 31 ) {

if( checkBit(N, bit)){
cnt++;

}

}

print(cnt);


---


checkBit(N, bit){

int setBit = 1 << bit;

 ans = N & setBit; 

if( ans > 0) {

return true;

}else{

return false;

}

}


TC - O(log(Integer.MAX_VALUE))
SC - O(1)



Or 


while( N > 0){

if( N & 1) == 1) cnt +=1;
N = N >> 1;

}

print(cnt);

TC - O(log(n))
SC - O(1)

Q - IRCTC wants to improve how it shows train options to its users. They have decided that trains which run more frequently should appear higher up in the search result. To figure this out, they look at a 28 day period to see how often each train runs.

Problem - for each train they have come up with a special number. This isn't just any number, though, if you were to write it down in binary form ( which is like a special code of 0s and 1s), each of the 28 digits corresponds to a day in that period. A '1' means the train runs on that day, and a '0' means it doesn't.

Task - Your task is to help IRCTC by writing a program. Given a list A of these special numbers for different trains, your program should find the train that runs the most.

solution - 

In this we have to find which no which has maximum number of 1's present in it.

count the number of set bits present in each number and return the index of that number.

Q - given three integers  - A, B, C. Return a decimal number which has A no of 0's as msb and then B number of 1's and then C number of 0's in binary representation.

solution - 

ans = 0

for( bit from C to C+B-1 ) {

ans = ans | ( 1 << bit);

}

---------------------------


DSA: Bit Manipulation 2



Q - Maximum and pair - 

Given N positive array elements. Find the maximum value of arr[i] & arr[j] where i != j.

BF - Take and of each element with other by using two loops.


for( i from 0 to N-1) {


for( j from 0 to N-1) {

if( i != j) {

ans = max(ans, A[i] & A[j] );
}

}

}

print(ans);


TC -  O(n^2)
SC - O(1)

Observations - 

MSB will give us the maximum and value.

count the elements of an array who has msb bit set. if it more than 1 then there are two elements which will give us maximum and value, and make elements whose that bit is not set 0.

like this move to the left side bits from msb. In this process make sure that at least two elements should remain non zero.


code - 


for( bit from 30 to 0) {

cnt =0;

for( i from 0 to N-1 ) {

if( A[i] & ( 1 << bit )  > 0 ) { 

cnt++;

}

}


if( cnt >= 2 ){
pair = pair | (1 << bit);


// set values in A to zero


for( i from 0 to N-1 ) {

if( A[i] & ( 1 << bit) == 0 ) {
A[i] =0;
}

}
}
}


TC - O(31 * N) = O(N)
SC - O(1)


Google interview question - 


Given N positive numbers count total no of pairs whose bitwise & is maximum. A[i] & A[j] and i != j

Solution - 

n = size of an array

ans = (n * ( n-1)) / 2.

---------------------

 Tools to enhance working memory and attention - 
 
 If dopamine levels are good then working memory work at its best.
 
 Tools to enhance dopamine levels - 
 
1) Yoga nidra - check videos for this.

2) Non sleep deep rest - video -> NSDR huberman 

do above 2 for 30 mins at the start and then reduce it to 10 mins.

3) Cold showers below neck, do not take water on head - for 10 mins.

4) Listen to Binural beats (40hz, 15hz)

--------------------------
 
 Load Balancing and consistent hashing - 
 
 Vertical scaling - Scaling up - This is simple, limited space for scaling and cost heavy.
 
 Horizontle scaling  - Scaling out - Load distibution is equal.
 
 Load balancer distributes the traffic to multiple servers. 
 
 Load balancer is singular unified entry points to the servers. 
 
 Load balancer helps in -
 1) State management - keeps track of the server availiability by using health check (pull mechanism - LB balancer calls healthcheck api of application server), Heart beat ( Push Mechanism - application servers sends the message to LB that it is alive, If LB does not get heartbeat from server for certain period of time like 10 sec then it will remove that server from LB mapping). Once server is online it will call the register api of load balancer to register itself. 
 
 Most cloud provides the heartbeat mechanism to keep track of application servers health.
 
 2) High availability - 
 
 This is achieved by using multiple machines and we can also keep few load balancers on stand by. Each geo location will have seperate load balancer and traffic to these load balancers will be routed by Geo DNS, Geo DNS gives the ip address closest to the user.  
 
 When any machine is active it is called as hot, when machine is not working then it is called as cold.
 
 On AWS load balancer keep sending health check messages to cloudwatch and when cloudwatch does not receive health check update from loadbalancer it will create alarm, and there are two ways to react to these alarms 
 
 1) Manual - In this admin will log in and debug the issue or start new load balancer. 
 2) Automatic  - This will create new loadbalancer instance automatically and assign the ip address to this new load balancer. 
 
 
 In geo dns we will only get the one ip address and We can also setup dns configuration like one domain name will have more than one ip address, in this case DNS will return all the ip addresses and browser will decide which ip to ping (Ip is decided either randomnly or first ip from the list of ip addresses )
 
 
 Scaling storage - 
 
 Data is stored on seperate server called database server. 
 
 we can vertically or horizontally scale a database. Scaling relational databases horizontally is very complicated. 

Scaling database vertically also has limitations, Performace will go down as we scale it vertically, so after one point we will have to scale horizontally. 

Vertical scalling has limitations, it is costly, and decreases performance as data on one server is increasing. 

Horizontle scalling in this data is divided based on sharding key and these chunks of data are stored on seperate database servers, and we can keep adding servers to store more data. 

Sharding - 

Sharding is a database architecture pattern in which a large dataset is horizontally partitioned across multiple independent databases (called shards), where each shard holds a subset of the total data based on a defined sharding key. It is primarily used to improve scalability, performance, and availability in high-volume systems by allowing parallel processing, independent storage, and distributed workload across multiple servers. Sharding introduces complexity in terms of data distribution, query routing, and consistency, but is essential for managing large-scale applications efficiently.

Sharding key -  It is identifier or column which is unique and used to divide the data across multiple database servers. 


Algorithms to divide data across multiple database servers - 

Digest IO - In this each machine will store some range of data like machine 1 will store the user ids from 1 to 100 and machine 2 will store user ids from 101 to 200. This concept is called as bucketing. 

It becomes extremly expensive. 

Consistent hashing -  

hashing is a technique to map data of arbitrary size to fixed-size addresses or keys, usually for the purpose of efficient storage and retrieval. It's particularly useful in systems where speed and constant-time access are critical.

A hash function is a function that takes an input (like a file, string, or key) and returns a fixed-size binary or hexadecimal value, called a hash code or hash value.

Consistent Hashing is a special type of hashing technique designed to minimize the number of keys that need to be remapped when the set of hash buckets (like servers, nodes, or partitions) changes.

It's widely used in distributed systems, load balancers, distributed caches, and databases like Cassandra, DynamoDB, Redis Cluster, etc.

Steps to create consistent hashing - 

1) Create hash ring which will store the range of values. 

2) Hash function - Create a hash function which will return hash value in range of hash ring for every input key. 

3) Hash function - This hash function will take machine id and it will match this machine to the range of values on the hash ring. This machine will store the mapped values in that machine. 

4) Map user id or key to the machine id.


In reality hash ring is array of hashed values and machines mapping. 


Example - 

Consistent hashing is a special hashing technique used in distributed systems to minimize the number of keys that need to be moved when nodes (like servers or cache nodes) are added or removed.

In traditional hashing using hash(key) % N, if the number of nodes N changes, almost all keys are rehashed. This causes a lot of unnecessary data movement.

Consistent hashing solves this by placing both nodes and keys on a virtual circular ring, also called a hash ring.

How Consistent Hashing Works

Hash both servers and keys into the same numeric space, for example, 0 to 2^32 (or 0 to 360 for simplicity).

Each server is assigned a position on the ring based on the hash of its ID or IP address.

Each key is hashed and placed on the ring as well.

To assign a key to a server, move clockwise around the ring from the key's hash until you find the first server. That server is responsible for that key.

Example

Suppose we have three servers:

Server A hashes to 120

Server B hashes to 250

Server C hashes to 80

Now, suppose the key "apple" hashes to 100. Going clockwise on the ring, the next server after 100 is 120, so "apple" is assigned to Server A.

Key "banana" hashes to 180. The next server after 180 is 250, so it goes to Server B.

Key "cherry" hashes to 300. Since 300 is past 250, it wraps around to Server C at 80.

Adding a New Server

Suppose we add Server D with a hash of 160.

Now, only the keys between 120 and 160 need to move from Server B to Server D. All other key assignments remain unchanged.

This is the main benefit of consistent hashing. It provides minimal disruption when nodes are added or removed.

Removing a Server

If Server A at 120 is removed, the keys that were assigned to it will now be routed to the next clockwise server, which is D at 160. Other key assignments remain unchanged.

Use Cases

Consistent hashing is used in distributed databases like Cassandra and DynamoDB, caching systems like Memcached and Redis Cluster, content delivery networks, and load balancers.

Virtual Nodes (VNodes)

To improve load distribution, each physical server can be assigned multiple positions on the ring. These are called virtual nodes. This helps avoid uneven distribution of keys when the number of servers is small.


Cascading Failure

A cascading failure occurs in a distributed system when the failure of one component triggers a chain reaction of failures in other components, eventually leading to a large part of the system becoming unresponsive or collapsing.

This often happens when:

A node becomes slow or unresponsive.

Its traffic or workload is redirected to other healthy nodes.

Those nodes become overloaded due to the extra load.

They also start failing or slowing down.

The cycle repeats and spreads across the system.

Example:
Imagine a distributed cache with 5 nodes. If one node crashes, all its keys must be fetched from the database and re-cached on other nodes. This sudden surge in database traffic and uneven cache load can overwhelm other nodes, leading to more crashes.

Cascading failures are especially dangerous in systems that serve real-time traffic, such as microservices, content delivery networks, or payment gateways.

Ways to mitigate cascading failures:

Implementing rate limiting

Circuit breakers (fail fast if downstream is unhealthy)

Auto-scaling infrastructure

Consistent hashing with load-aware balancing

Consistent Hashing with Virtual Nodes (VNodes)

Consistent hashing alone helps reduce remapping of keys when nodes are added or removed. However, it does not guarantee even distribution of load. Some nodes may get more keys than others, especially if the number of nodes is small or the hash distribution is uneven.

To solve this, virtual nodes are introduced.

Virtual nodes (VNodes) are multiple positions on the hash ring assigned to the same physical node. Instead of a node appearing only once on the ring, it appears multiple times with different hashes.

How it works:

Each physical node is assigned multiple virtual nodes.

Each virtual node has its own position on the hash ring.

When a key is hashed, it is assigned to the first virtual node clockwise from its position.

That virtual node maps back to a physical server.

Benefits:

Ensures more uniform distribution of keys.

Reduces the chance of one physical node becoming a bottleneck.

Makes it easier to rebalance load when a node is added or removed.

Minimizes the impact of failure by spreading load more evenly across all nodes.

Example:
Instead of having Server A at only one position, you might place it at positions 45, 120, and 300 on the ring. Similarly, Server B may appear at 60, 180, and 320.

If Server A goes down, its load is not transferred to just one node, but is split across multiple servers that take over its virtual nodes, preventing overload and reducing cascading failure risk.

In summary:

Cascading failure is a dangerous chain reaction of node failures caused by traffic redistribution.

Consistent hashing with virtual nodes helps distribute load more evenly and prevents hotspots.

Together, these techniques build more resilient and scalable distributed systems.


Hashing algorithms -
MD5, murmurhash - This one is the fastest.



1. What is Consistent Hashing?
------------------------------
Consistent hashing is a technique used to distribute data across multiple nodes in 
a distributed system such that minimal data is redistributed when nodes are added or removed.

Traditional hash mod approach:
- If we use: hash(key) % N (where N is the number of nodes),
  then adding or removing a node changes the mapping for most keys.

Consistent hashing solves this by organizing the nodes and keys in a circular hash space 
(often called a "hash ring").

--------------------------------------------------

2. How Consistent Hashing Works
-------------------------------
- All nodes and keys are assigned a position on a circular hash ring using a hash function
  (e.g., SHA-1 or MurmurHash).
- Each key is assigned to the first node that appears **clockwise** after its hash.

Example:
If we have 4 nodes: A, B, C, D
- Hash each node: hash(A), hash(B), hash(C), hash(D)
- Place them on a circle in order of their hash
- For a given key, find hash(key), then move clockwise to the nearest node

Key Benefits:
- When a node is added or removed, only a small portion of keys are remapped
- System scales gracefully

--------------------------------------------------

3. Problem with Basic Consistent Hashing
----------------------------------------
Uneven distribution:
- In practice, nodes may not be evenly spaced in the hash ring
- Some nodes may get significantly more keys than others

This leads to **load imbalance**

--------------------------------------------------

4. Virtual Nodes (VNodes) – Solution
------------------------------------
Virtual Nodes (VNodes) solve the problem of uneven load distribution.

What are Virtual Nodes?
- Instead of placing each physical node on the ring only once,
  we place it **multiple times** using different hash values
- Each physical node is associated with multiple virtual nodes

For example:
- Physical Node A may have VNode A1, A2, A3
- Each VNode is hashed separately and placed on the ring

How it helps:
- Balances load more evenly among physical nodes
- Prevents any single node from owning too much data
- Reduces risk of hotspots

When a node fails or is added:
- Only the virtual nodes of that physical node are affected
- Each vnode holds only a fraction of the total data

--------------------------------------------------

5. Steps Involved in Consistent Hashing with VNodes
----------------------------------------------------

1. Hash each physical node multiple times to create virtual nodes
2. Place each virtual node on the hash ring
3. For each key:
   - Hash the key
   - Find the nearest virtual node clockwise on the ring
   - Route the key to the physical node that owns the virtual node

--------------------------------------------------

6. Example
----------

Suppose we have 3 physical nodes: A, B, C  
And we assign 3 virtual nodes per physical node:

- A1, A2, A3 → hash(A1), hash(A2), hash(A3)
- B1, B2, B3 → hash(B1), hash(B2), hash(B3)
- C1, C2, C3 → hash(C1), hash(C2), hash(C3)

All 9 virtual nodes are placed on the ring.

Now, when we hash a key:
- Find its place on the ring
- Traverse clockwise to find the next virtual node
- Route the key to the physical node that owns that virtual node

If node B crashes:
- Only B1, B2, B3 need to be reassigned
- Minimal data redistribution is needed

--------------------------------------------------

7. Advantages of Virtual Nodes
------------------------------

- Better load balancing
- Easier to scale up or down
- Failure of a node causes minimal disruption
- Simplifies node replacement and data migration

--------------------------------------------------

8. Systems Using Consistent Hashing + VNodes
--------------------------------------------
- Apache Cassandra
- Amazon DynamoDB
- Riak
- Kafka (partitions and consumer rebalancing)

--------------------------------------------------

9. Summary
----------

- Consistent hashing minimizes remapping of keys when nodes change
- Virtual nodes improve load distribution across physical nodes
- Each physical node manages multiple virtual positions on the ring
- This approach is scalable, fault-tolerant, and used in real-world distributed systems




Caching - 

Caching is a technique used in computer systems to store frequently accessed data in a temporary storage location, so that future requests for that data can be served faster.

Instead of recomputing or refetching data from a slow backend (like a database, disk, or remote server), caching stores a copy of the data in a faster memory system (like RAM) and reuses it when needed.

The main goal of caching is to reduce latency, increase throughput, and improve overall system performance.

Content delivery network - 

CDN (Content Delivery Network) is a geographically distributed network of servers that work together to deliver digital content (like websites, videos, images, scripts, and stylesheets) faster and more reliably to users based on their location.

It is a key performance and scalability technology in modern web infrastructure.

Core Idea Behind CDN

Instead of having all users access a central server (like your web hosting origin), a CDN stores cached copies of your content on multiple servers located around the world. These servers are called edge servers.

When a user visits your website, the CDN routes the request to the nearest edge server in terms of network distance, reducing latency, bandwidth usage, and server load.


Youtube stores video metadata in database and video on the s3 bucket. S3 bucket is based on hard disk. 

1) Upload the metadata to DB 
2) Upload video to S3.
3) Update DB with S3 url.
4) Upload video to CDN.
5) Upload the cnd url to DB.


CDN example - 

A Content Delivery Network (CDN) is a distributed system of servers placed across the globe, designed to deliver web content like images, videos, JavaScript, CSS, and even entire web pages from a location geographically closer to the end user.

It solves the problem of latency, which is the delay that happens when data has to travel over long distances from the origin server to the end user.

The Problem Without CDN

Let’s say you host your website on a single server in New York, USA. Users from New York will experience fast performance.
But a user from Mumbai, India or Sydney, Australia will experience slower loading times because the request has to travel halfway across the world.

The longer the distance, the more time it takes for:

DNS resolution

TCP connection

HTTP requests and responses

How a CDN Works

CDNs solve this by replicating your content and storing it on multiple edge servers located around the world.

These edge servers cache your web content and serve it to users based on proximity and network speed, not just geography.

When a user requests a file (e.g., an image or a page):

The user types your website URL.

DNS resolves the domain to the nearest CDN edge server.

That edge server checks if it already has the content cached:

If yes (cache hit): It serves it directly.

If no (cache miss): It fetches from the origin server, stores it, and serves the user.

On future requests, the edge server will serve it from cache.

Types of Content a CDN Handles

Static content: HTML, CSS, JS, images, fonts, videos, downloadable files.

Dynamic content (advanced CDNs): Can cache or accelerate data fetched from APIs or databases using smart routing, even for dynamic pages.

Real-World Example: How CDN Works

Let’s say your website www.example.com is hosted in New York.

A user from Mumbai visits your site.

Without CDN:

The request goes all the way to New York.

Takes around 300–400 ms for round-trip.

Images, scripts, and videos take even longer.

Result: Slow page load.

With CDN (e.g., using Cloudflare or Akamai):

Your site is behind a CDN.

The CDN has an edge node in Mumbai.

The user’s request is redirected by DNS to that Mumbai node.

The edge node checks its local cache:

If the content is present (cache hit), it serves it instantly (e.g., 30 ms).

If not, it pulls it from the New York origin, serves it, and stores it locally for future users.

Result: Faster loading, reduced latency, and lower load on your server.

Benefits of Using CDN

Reduced Latency
Content is served from a node closer to the user, reducing round-trip time.

Faster Load Times
Optimizes delivery of web resources, reducing render-blocking and improving Core Web Vitals.

Scalability
Easily handles high traffic spikes during peak times or product launches.

Reliability and Redundancy
If one node goes down, traffic is automatically routed to the next closest node.

Offloading Server Load
Reduces origin server requests, saving bandwidth and CPU cycles.

Security Features
Many CDNs offer DDoS protection, web application firewalls (WAF), bot mitigation, and TLS/SSL termination.

Common CDN Providers

Cloudflare

Akamai

Amazon CloudFront (part of AWS)

Google Cloud CDN

Microsoft Azure CDN

Fastly

StackPath

Use Cases of CDN

E-commerce websites with global customers

Video streaming platforms like Netflix or YouTube

Web apps and APIs with users in different regions

Gaming services to reduce lag and update time

News websites needing fast load and failover

CDN Caching Policies and Expiry

Content can be cached for seconds, minutes, or days depending on cache-control headers or CDN rules.

You can configure cache invalidation to purge outdated content.

Summary

A CDN reduces distance and load by caching content on edge servers close to the user. This results in faster page load, improved availability, scalability under load, and better overall user experience. It is an essential part of any modern, high-performance web application or service.

CDN provideers - 
Akamai
Cloudfront
Cloudflair
Fastly

Caching within the application - 

Redis is single threaded server used for caching.

Local cache - It is the cache which is present on application server.

It will store frequently accessed data in application server so that latency can be reduced.

Hit ratio in this caching is very less.
and each server will have the seperate data stored which can cause redudancy.

Local cache on application server is very volatile.


Hit ratio = total no of cache hits / total no of requests.


To solve these issues we will use global cache.

Global cache resides between application server and the database server. This is different layer which stores the frequently accessed data from database in cache memory and reduces the latency.

Global cache will have cluster of caching machines and it can be distributed also (This is verticle scalling).


Local Cache
--------------
Definition:
Local cache is a cache that resides within the memory of a single application instance. 
Each instance maintains its own independent cache.

Characteristics:
- Stored in local memory (e.g., RAM)
- Fastest access (no network involved)
- Each node has a separate copy of the cache
- Not shared across instances

Use Cases:
- Single-instance applications
- Data that does not change frequently
- When extremely low latency is required
- Caching configuration values or lookup data in microservices

Examples:
- Java applications using Caffeine or Guava cache
- Python’s functools.lru_cache
- Spring Boot applications using @Cacheable with in-memory cache

Limitations:
- Risk of stale data due to no coordination between instances
- Cache invalidation is difficult across nodes
- Not suitable for distributed systems

3. Global Cache
---------------
Definition:
Global cache is a shared cache that is accessible by multiple application instances, 
typically over the network. It ensures a consistent view of cached data across all nodes.

Characteristics:
- External to the application process
- Shared and accessible by all instances
- Involves network access (higher latency than local cache)
- Centralized or distributed caching system

Use Cases:
- Distributed systems and microservices
- Situations requiring consistent and synchronized cached data
- High-availability cloud-native applications

Examples:
- Redis
- Memcached
- Amazon ElastiCache
- Azure Cache for Redis

Benefits:
- Shared data view across multiple instances
- Easier to manage and synchronize cache contents
- Consistency in caching across services

Limitations:
- Slower than local cache due to network latency
- Requires external infrastructure and monitoring
- Can become a bottleneck if not scaled properly

4. Comparison: Local vs Global Cache
------------------------------------
Location:
- Local: within application memory
- Global: external/shared system

Speed:
- Local: very fast (no network calls)
- Global: slower due to network overhead

Scope:
- Local: limited to one application instance
- Global: shared across all instances

Consistency:
- Local: possible inconsistency across nodes
- Global: consistent and synchronized

Use Case Suitability:
- Local: simple, single-instance, or read-heavy apps
- Global: distributed, cloud-native, scalable systems

5. Hybrid Approaches
--------------------
Multi-Level Cache (L1 + L2):
- L1 cache is local (in-memory)
- L2 cache is global (e.g., Redis)
- First check L1; if not found, fetch from L2 and store in L1

Near-Cache Pattern:
- Keeps a subset of global cache in local memory for faster access
- Common in distributed data grid systems like Hazelcast

Benefits of Hybrid Caching:
- Low latency from local cache
- Consistency and scalability from global cache

6. Summary
----------
- Use local cache for performance-critical, single-instance data that doesn’t change often
- Use global cache in distributed systems to maintain a consistent cache view
- Combine both for high-performance and consistency in large-scale applications

In Global caching hit ratio is very high as compared to the local caching. 


Latency numbers - 

https://gist.github.com/jboner/2841832


Sticky session - 


Sticky Session, also known as Session Affinity, is a mechanism used in load balancing where all requests from a particular client (typically a user or browser session) are routed to the same backend server for the duration of the session.

This ensures that session-related data (like login info, cart contents, etc.) is consistently available on that specific server.

2. Why Sticky Sessions are Needed

In a load-balanced environment with multiple servers, each server is usually stateless, meaning it doesn't remember any previous requests from a user. However, some applications store session data (like user login, shopping cart, etc.) in the server memory. Without sticky sessions:

A user’s request might go to a different server that doesn’t know their session

This can result in session loss, forced logouts, or inconsistent behavior

Sticky sessions ensure that all user interactions go to the same server, avoiding these issues.

3. How Sticky Sessions Work

When a client makes the first request, the load balancer selects a backend server (say Server A) and routes the request.

To ensure the same server is used for future requests, the load balancer uses one of the following:

Cookies:
The load balancer sets a cookie (e.g., JSESSIONID, ROUTEID, AWSALB) in the user's browser that identifies the server. On subsequent requests, the cookie is read to route the user to the correct server.

IP Hashing / Source IP Binding:
The load balancer maps the client IP address to a server. All requests from that IP go to the same server.

Session Tokens in Headers:
Some systems pass a session token in the request header that the load balancer uses to route requests.


Cache Invalidation - 

Cache invalidation is the process of removing or updating stale data from the cache 
so that users always get fresh and consistent data from the system.

It ensures that:
- Cached data remains consistent with the source of truth (like database)
- Users don't see outdated information

Without invalidation, cache might return stale (incorrect) results.

--------------------------------------------------

2. Why Cache Invalidation is Important
--------------------------------------
- Keeps data consistent across cache and data source
- Prevents users from seeing outdated or incorrect data
- Helps maintain trust and correctness in critical systems

--------------------------------------------------

3. Cache Invalidation Strategies
-------------------------------

There are three main strategies:

A. Write-Through Cache
-----------------------
- Data is written to both the cache and the database at the same time
- Ensures cache is always in sync with the database

Pros:
- Data consistency is high
- Simplifies reads (always valid cache)

Cons:
- Write latency increases (due to double write)
- Cache is used even when data isn't read often

Use Case:
- Read-heavy applications where consistency is critical
- These are immediately consistent.

B. Write-Around Cache
---------------------
- Data is written directly to the database
- Cache is not updated during the write
- Cache is updated only when the data is read (on a cache miss)

Pros:
- Reduces cache pollution for rarely accessed data

Cons:
- Cache misses immediately after writes
- Data can be stale in the cache

Use Case:
- Write-heavy systems where only frequently accessed data should be cached

C. Write-Behind (Write-Back) Cache
----------------------------------
- Data is written to the cache first
- Then asynchronously written to the database

Pros:
- Fast writes
- Useful for batching writes

Cons:
- Risk of data loss if cache crashes before flushing to DB
- More complex to manage

Use Case:
- Systems tolerant to slight data loss or where performance is more important
-These are eventually consistent.

--------------------------------------------------

4. Invalidation Policies (Eviction)
-----------------------------------

A. Time-to-Live (TTL)
---------------------
- Each cached item has an expiry time
- After TTL, the item is automatically invalidated

B. Manual Invalidation
----------------------
- Application explicitly removes or updates cache on certain operations (e.g., delete, update)

C. LRU / LFU
------------
- LRU (Least Recently Used): Removes items that haven't been accessed recently
- LFU (Least Frequently Used): Removes items accessed the least

--------------------------------------------------

5. Redis Invalidation Mechanisms
-------------------------------

Redis is a widely used in-memory cache. It supports both **lazy** and **eager** invalidation:

A. Lazy Invalidation (Passive Expiry)
-------------------------------------
- Redis sets TTL (expiry time) on keys
- It does NOT actively check and delete them
- Keys are deleted only:
  - When accessed (read/write)
  - And found expired

Pros:
- Less CPU usage
- Efficient if expired keys are rarely accessed

Cons:
- Expired keys may occupy memory until accessed
- Data may appear stale if not accessed quickly

Use Case:
- Suitable when reads are frequent and consistency isn't critical

B. Eager Invalidation (Active Expiry)
-------------------------------------
- Redis runs a **background task** (every 100ms by default) to randomly sample expired keys and remove them
- Also used in combination with lazy expiration

Pros:
- Frees up memory proactively
- Reduces chance of stale data staying in memory

Cons:
- More CPU-intensive
- Might not catch all expired keys immediately

Use Case:
- Suitable for memory-sensitive applications needing aggressive cleanup

Note:
Redis uses a **hybrid approach**:
- Lazy: On key access
- Eager: Active background cleanup of a sample of expired keys

--------------------------------------------------

6. Summary
----------

- Cache invalidation is essential to maintain consistency between cache and database.
- Strategies include:
  - Write-through
  - Write-around
  - Write-behind
- Invalidation can be based on TTL, manual logic, or eviction policies like LRU.
- Redis supports:
  - Lazy invalidation: Deletes keys only when accessed and expired
  - Eager invalidation: Periodically scans and deletes expired keys in background


If TTL is too high then we will run into stale data or data will be inconsistent. 

If TTL is too low then hit ratio will be very low. 


Cache eviction strategies - 


1. What is Cache Eviction?
---------------------------
Cache eviction is the process of removing items from the cache to:
- Free up space when the cache is full
- Maintain the most relevant or frequently used data in memory

Eviction happens when:
- The cache reaches its maximum capacity
- Items expire based on some policy

--------------------------------------------------

2. Why Eviction is Important
-----------------------------
- Ensures efficient use of limited memory
- Maintains fast access to important or frequently used data
- Removes stale or rarely used items

--------------------------------------------------

3. Common Cache Eviction Strategies
-----------------------------------

A. LRU (Least Recently Used)
----------------------------
- Removes the item that was **used least recently**
- Assumes recently used data is more likely to be accessed again

Pros:
- Simple and effective for general-purpose caching
- Good for workloads with temporal locality

Cons:
- Can be inefficient if access pattern is uniform or unpredictable

Use Case:
- Web caches, in-memory stores (like Redis), and application caches

B. LFU (Least Frequently Used)
------------------------------
- Removes the item that was **used least often**
- Maintains a counter for how many times each item is accessed

Pros:
- Retains highly popular items even if accessed long ago

Cons:
- Needs additional storage for counters
- Difficult to adapt to changing access patterns

Use Case:
- Useful when certain data is always "hot" regardless of recency

C. FIFO (First-In First-Out)
----------------------------
- Removes the **oldest inserted** item in the cache
- Simple queue-based approach

Pros:
- Very simple to implement

Cons:
- Does not consider access patterns
- May evict frequently accessed data just because it was added earlier

Use Case:
- Basic caches, or cases where freshness is not critical

D. Random Replacement
---------------------
- Randomly removes one item from the cache

Pros:
- Extremely easy to implement
- Avoids overhead of tracking usage

Cons:
- May evict important data
- Not ideal for performance-critical applications

Use Case:
- Embedded systems or constrained environments

E. MRU (Most Recently Used)
----------------------------
- Removes the item that was **most recently accessed**
- Opposite of LRU

Pros:
- Useful in rare cases where recent data is unlikely to be reused

Cons:
- Counter-intuitive for most real-world workloads

Use Case:
- Some database systems or edge-case scenarios

F. Time-Based Expiry (TTL)
---------------------------
- Items are evicted based on a predefined Time-To-Live (TTL)
- Once TTL expires, the item is removed

Pros:
- Simple and deterministic
- Prevents stale data from lingering

Cons:
- May remove useful data if TTL is too short
- May keep useless data if TTL is too long

Use Case:
- Session management, temporary caching of responses

--------------------------------------------------

4. Advanced Strategies and Hybrids
----------------------------------

A. ARC (Adaptive Replacement Cache)
-----------------------------------
- Balances between LRU and LFU using two separate lists
- Adapts dynamically to workload

Use Case:
- Advanced caching systems, file systems like ZFS

B. CLOCK Algorithm
------------------
- A more efficient approximation of LRU using a circular buffer
- Used in OS page replacement

Use Case:
- Operating systems, virtual memory management

--------------------------------------------------

5. Eviction in Redis
---------------------
Redis supports:
- LRU
- LFU
- Random
- TTL
- Volatile variants (applied only to keys with expiry)

Eviction policies in Redis include:
- noeviction
- allkeys-lru
- volatile-lru
- allkeys-random
- volatile-random
- allkeys-lfu
- volatile-lfu
- volatile-ttl

--------------------------------------------------

6. Summary
----------

- Cache eviction ensures limited memory is used efficiently
- Common strategies:
  - LRU: remove least recently accessed
  - LFU: remove least frequently accessed
  - FIFO: remove oldest added
  - Random: remove any item at random
  - MRU: remove most recently used
  - TTL: expire after fixed time

- Hybrid and adaptive strategies exist for more intelligent cache management

Choose eviction strategy based on:
- Access patterns
- Memory constraints
- Performance requirements
- Consistency needs


---

HLD Case study - 

Code judge - 

When we solve question on any platform, there we will see question, and code editor. 

APIS for that will be - 

GET /problems/{id}
POST /problems/{id}/submit


This problem will be stored in database - id, description, title, input date file link, output data file link.


These input and output data files are stored on S3 file data storage. 

These input file will have test cases and output file will have output of those input test cases. 

When moderator creates a problem it will store the problem metadata and testcases input, output files on the application server and S3.

Submitting a solution - 

In this user will solve the problem and submit the solution 

APIS - 
POST /problems/{id}/submit

once problem is submitted it will fetch the problem details from the DB and testcases input and output files from S3 and check the code for all input testcases. 

The problem here is input and output test cases files are huge in size and fetching them each time will result in very high latency, To solve this problem we will use the global cache to store the input and output files of testcases. 

For the first time it will be cache miss as files will not be available on the cache, application server will fetch the files from S3 and store it on the global cache and then run the code. Same process will run for each problem. 

This partially solves our problem. As cache will be stored on the different cluster (one more hop) and it has size limitations as input and output data files can be very large (in GB). 

There is another option - we can use the local cache. We will use local hard disk of the application server to cache the input and output files, This will reduce the network hop.

As we will have multiple servers we will have to have some mechanism which will route the request for particular problem to the appropriate server. Each server will serve the reqeusts for few selected problems. 

These requests will be routed by the consistent hashing mechanism and problem id will become our sharding key. 

But using this consistent hasing mechanism will result into hotspots, the popular problems will be solved frequently and it will lead to huge load on few servers and other servers will be idle.

Again the space on local machine has limited space and we can not scale this infinitely.


We use global cache when all of our application servers shares that data and data changes very frequently.

Local cache is used when data size is huge and downloading it from global cache becomes an issue.

Cache invalidation in our local caching mechanism without consistent hashing - 

TTL - if TTL is high then data will be stale, and TTL is low then miss rate will be high, this is not suitable option in our case.

Lazy Invalidation -  

In this it will check input output file metadata from database, if file is updated recently then it will give the local cache miss and fetch the file from S3 and store it on cache and start code validation. if file is not updated in database and same file is present on the local cache then it will give the cache hit and will use the same file for code validation.

This file metadata will have update_at timestamp information in it or we can also store the checksum, in this it will store the hash of file content in DB and compare the hash of new file and old file and decide whether file has changed or not.

Code judge and Leaderboard case study notes - 

https://docs.google.com/document/d/1p5iq1hIGte7psJDPgOZuD1AJaxSJItN1yrIOScpWv78/edit?tab=t.0

------------


CAP/ PACELC theorem and Master Slave - 


CAP - 

The CAP theorem, also known as Brewer's theorem, is a fundamental principle in distributed systems that describes the trade-offs between three desirable properties:

Consistency (C)

Availability (A)

Partition Tolerance (P)

According to the CAP theorem, a distributed system can satisfy at most two out of these three properties at the same time, but not all three. Let's explore each concept in depth, understand what the theorem implies, and how it affects real-world distributed system design.

1. Understanding the Three Properties
a. Consistency
Consistency means that all nodes see the same data at the same time. After an update operation completes, any subsequent read operation should return that updated value. In other words:

Strong consistency ensures that once a write is acknowledged, all subsequent reads will reflect that write.

It behaves like a single-node system, where reads and writes are atomic and synchronized.

Example: In a banking system, if you transfer ₹100 from Account A to B, consistency ensures that A’s balance is immediately reduced and B’s balance is immediately increased—no matter which server you query.

b. Availability
Availability means that every request (read or write) receives a non-error response, regardless of the state of any individual node. It does not guarantee that the data is the most recent, only that the system will respond.

Availability prioritizes uptime and responsiveness.

Even in the face of some failures, the system continues to operate.

Example: A shopping website that always shows you some product listings even if a few backend services are down is prioritizing availability.

c. Partition Tolerance
Partition tolerance means the system continues to function even if network partitions (i.e., communication breakdowns) occur between nodes in the system.

In distributed systems, network failures are inevitable.

A partition divides the network into subsets of nodes that cannot communicate with each other.

Partition tolerance is non-negotiable in modern systems. If a system does not tolerate partitions, a single network failure could bring down the entire system.

Example: Two datacenters lose connectivity due to a network failure, but both must continue serving requests from their local users.

2. The CAP Theorem Statement
Formally introduced by Eric Brewer and later proven by Gilbert and Lynch in 2002, the CAP theorem states:

In the presence of a network partition, a distributed system must choose either Consistency or Availability.

This leads to the idea that you can pick any two of the three, but not all three simultaneously.

3. Analyzing the Trade-offs
Case 1: CP (Consistency + Partition Tolerance)
The system sacrifices availability.

When a network partition occurs, the system will reject requests to ensure consistency.

Common in systems where accuracy is more critical than uptime.

Example: A distributed database like HBase or MongoDB (when configured for strong consistency).

Use Case: Financial systems, banking, inventory management.

Case 2: AP (Availability + Partition Tolerance)
The system sacrifices consistency.

During a partition, the system continues to serve requests but may return stale or inconsistent data.

Often uses eventual consistency: updates propagate over time.

Example: Couchbase, DynamoDB, Cassandra.

Use Case: Social media feeds, product catalogs, DNS.

Case 3: CA (Consistency + Availability)
The system sacrifices partition tolerance, which is impractical in real distributed systems.

CA systems work well only when the network is perfectly reliable (no partitions).

In practice, partitions do occur, so this model is not realistic for large-scale distributed systems.

Example: Traditional RDBMS running on a single machine (e.g., PostgreSQL, MySQL).

Use Case: Local applications, systems without replication.

4. CAP in Real-World System Design
In modern systems, partition tolerance is usually required due to the nature of networks and distributed infrastructure. So in practice, designers must choose between consistency and availability when a partition occurs.

To manage this trade-off:

Systems often implement tunable consistency, where developers can choose the consistency level (e.g., quorum reads/writes).

Eventual consistency is widely adopted for availability-focused applications.

Consensus protocols (e.g., Paxos, Raft) are used when consistency is critical.

5. Common Misconceptions
CAP does not imply that systems cannot be consistent and available most of the time. It only states that during a partition, you must choose.

It’s not a trilemma for normal operation—only when partitions occur does the limitation apply.

6. Beyond CAP: PACELC Theorem
While CAP addresses behavior during partitions, it doesn’t say anything about trade-offs during normal operation. This is where the PACELC theorem extends CAP:

If there is a Partition (P), the system must choose between Availability (A) and Consistency (C); Else (E), when there is no partition, the system must choose between Latency (L) and Consistency (C).

This gives a more complete view:

Systems always have to trade between consistency and latency, even when no partition exists.

It helps explain why some systems are eventually consistent even under normal conditions (for better latency).

Summary
CAP theorem highlights the fundamental limitations of distributed systems under network failures.

You must choose Consistency or Availability in the presence of Partition Tolerance.

Modern systems adopt hybrid approaches with configurable consistency levels, quorum-based reads/writes, or eventual consistency depending on their goals.

Partition Tolerance is non-negotiable, so the practical decision is between C and A.

1. Immediate Consistency (Strong Consistency)
Definition:
Immediate consistency (also called strong consistency) ensures that any read to a data item always returns the most recent write, no matter which node in the system handles the read.

Characteristics:
Once a write is acknowledged, all future reads will reflect that update.

The system acts like a single, centralized database, even though it is distributed.

Requires coordination between nodes to make sure all replicas are updated before confirming a write.

Example:
Imagine you transfer ₹1,000 from Account A to B:

With immediate consistency, any read of either account's balance from any server will reflect the new balances immediately after the transaction.

Pros:
Guarantees correctness and accuracy.

Easier to reason about the state of the system.

Cons:
Higher latency due to synchronization overhead.

Lower availability during network partitions or delays.

Reduced scalability because consistency requires coordination.

Use Cases:
Banking systems.

Inventory management (to prevent overselling).

Systems where accuracy is more important than speed.

2. Eventual Consistency
Definition:
Eventual consistency guarantees that, in the absence of new writes, all replicas of a data item will eventually converge to the same value.

There is no guarantee that a read will return the latest write immediately.

Over time (milliseconds to seconds), updates are propagated asynchronously to all replicas.

Characteristics:
Reads might return stale (outdated) data shortly after a write.

Useful in systems where availability and speed are more important than absolute correctness in the short term.

Example:
You post a photo on a social media app:

With eventual consistency, it might show up instantly for you, but your friend may see it a few seconds later as it propagates across servers.

Pros:
High availability and responsiveness.

More tolerant of network partitions.

Scales well to large numbers of users and servers.

Cons:
May lead to temporary inconsistencies (stale or conflicting data).

Requires conflict resolution mechanisms if concurrent writes happen.

Developers must design applications to tolerate inconsistency.

Use Cases:
Social media feeds, product reviews, comment systems.

DNS (Domain Name System).

Distributed caches, large-scale NoSQL databases (e.g., Cassandra, DynamoDB).

1. Definition of Availability
Availability is the percentage of time a system remains operational and able to respond to user requests. It’s commonly measured as a percentage over a given time period (usually per year).

Formula (simplified):
Availability
=
Uptime
Uptime
+
Downtime
×
100
Availability= 
Uptime+Downtime
Uptime
​
 ×100
2. High Availability (HA)
Definition:
A highly available system is designed to ensure minimal downtime, typically with 99.9% or higher uptime. It continues to function even when parts of the system fail.

Characteristics:
Redundancy: Multiple instances of critical components (e.g., servers, databases).

Failover mechanisms: Automatic switching to backup systems on failure.

Load balancing: Spreads traffic across multiple nodes to reduce strain.

Monitoring and alerts: Detect issues before they become outages.

Geographic distribution: Services spread across multiple data centers.

Uptime Levels:
Availability	Downtime per year
99%	~3.65 days
99.9%	~8.76 hours
99.99%	~52.6 minutes
99.999%	~5.26 minutes

Example:
A payment gateway with 99.99% uptime.

Amazon AWS services.

Google Search.

Use Cases:
Financial transactions

Healthcare systems

Mission-critical infrastructure (air traffic control, telecom, etc.)

3. Low Availability
Definition:
A low-availability system is one that experiences frequent or long periods of downtime, making it unreliable for consistent usage.

Characteristics:
Single points of failure: No redundancy or failover.

Manual recovery from crashes or outages.

Longer response times during issues.

Lack of monitoring or automation.

Effects:
Poor user experience (e.g., timeouts, errors).

Business loss (missed transactions or orders).

Damaged reputation and user trust.

Example:
A local server without backup or UPS that goes offline during power cuts.

Applications hosted on a single machine with no load balancing or auto-restart.

Use Cases:
Personal hobby projects

Internal tools not used in real-time

Legacy systems without modern failover

4. Designing for High Availability
To move from low to high availability, engineers apply several design principles:

Replication: Keep multiple copies of data/services.

Failover: Have standby components ready to take over.

Auto-healing: Automatically replace failed parts (e.g., auto-scaling in cloud).

Stateless services: Easier to restart, scale, and distribute.

Backups and disaster recovery: For catastrophic failures.

5. Availability vs Reliability
These are related but different:

Availability is about being up and responsive at any moment.

Reliability is about performing correctly over time (i.e., fewer failures).

A system can be highly available but not reliable if it responds quickly but often with errors. Similarly, it can be reliable when it's up, but not available due to frequent outages.


ATMs are eventually consistent.

In distributed system partition tolerance always will be required so we will have to choose between Consistency and Availability. 

Most of the systems in the world are Partition tolerant and Available.

Booking systems like stock broaker, makemytrip or ticket booking are Partition tolerant and Consistent. 

If system is eventually consistent then it will have low latency and if system is highly consistent then it will have high latency. 
 
PACELC Theorem - 
 
If system is low consistent then it will have low latency and will be highly available, and if system is highly consistent then it will have high latency and will have low availability as it will have to reject few requests to be consistent.

The PACELC theorem is an important concept in distributed systems that expands upon the limitations of the CAP theorem. While CAP focuses on how systems behave during network partitions, PACELC adds insight into system behavior even when the network is healthy. It introduces a more comprehensive framework for understanding the trade-offs between consistency, availability, and latency, depending on whether or not a partition is present.

Let’s explore the PACELC theorem in detail.

1. The Problem with CAP Theorem Alone
The CAP theorem, originally proposed by Eric Brewer and later formally proven, states that in the presence of a network partition, a distributed system can choose to provide either:

Consistency: All nodes see the same data at the same time.

Availability: Every request receives a (non-error) response, without guarantee that it contains the latest data.

But CAP only applies when a network partition has occurred — it says nothing about what trade-offs a system must make when no partition exists.

In reality, distributed systems are not always under partition. Much of the time they operate in a healthy state. But even then, designers must still decide whether they want to prioritize latency or consistency.

2. Introduction to PACELC Theorem
To address this gap, Daniel Abadi proposed the PACELC theorem. This framework states the following:

If a network Partition occurs (P), a distributed system must choose between Availability (A) and Consistency (C). Else (E), when the system is functioning normally (no partition), it must choose between Latency (L) and Consistency (C).

This breaks system behavior into two distinct cases:

When a partition happens: the system must trade consistency for availability or vice versa.

When there is no partition: the system must trade latency for consistency or vice versa.

This more accurately reflects the real-world design decisions that system architects face.

3. The Two Phases of PACELC
Phase 1: Behavior During a Network Partition (P)
Just like CAP, PACELC acknowledges that network partitions can and do occur. During such failures, a system must choose:

To be available: continue serving client requests, possibly with stale or inconsistent data.

Or to be consistent: delay or reject some client requests until the partition is resolved, to ensure all replicas remain in sync.

This is the classic CAP trade-off.

Phase 2: Behavior Else (E), When the Network is Healthy
When the system is not under partition, it still faces a critical trade-off: whether to prioritize low latency or strong consistency.

If you prioritize consistency, you may need to wait for confirmation from multiple nodes before responding to a client request. This increases response time.

If you prioritize latency, you may respond as soon as the nearest or fastest replica returns data — but that data may not be the most up-to-date.

This trade-off exists even in the absence of failures and must be accounted for in system design.

4. Implications of PACELC
The PACELC theorem reflects the reality that consistency is often expensive not only in failure scenarios (as CAP suggests) but also during normal operation. To maintain consistency, systems may have to perform additional coordination, which introduces latency. If a system instead opts for faster responses (i.e., low latency), it might need to sacrifice consistency, for example by returning stale data.

In this sense, PACELC suggests that latency is a cost of consistency, regardless of system health.

It also highlights that systems are not simply "consistent or available" as CAP might imply. Rather, they often make different trade-offs in different conditions — dynamically balancing between availability, consistency, and latency.

5. Real-World Interpretation
Let’s look at how different systems reflect PACELC principles.

Highly consistent systems, such as Google Spanner, opt for consistency over both availability and latency. During a partition, Spanner will reject some requests to avoid inconsistency. In normal operation, it synchronizes data across data centers using tightly synchronized clocks, introducing extra latency to ensure consistency.

Highly available systems, such as Amazon DynamoDB, are designed to prioritize availability during partitions and low latency during normal operation. This means they accept temporary inconsistency, especially during failure and under high load, to maintain responsiveness and uptime. Over time, they achieve eventual consistency.

This shows how PACELC more precisely classifies systems than CAP. CAP would label both DynamoDB and Cassandra as “AP” systems (favoring availability over consistency during partitions). But PACELC would further distinguish them as systems that also favor latency over consistency in healthy conditions.

6. Summary of PACELC’s Value
PACELC gives distributed system designers a more complete mental model than CAP. It recognizes that:

Network partitions are not the only time consistency and availability come into conflict.

Even when systems are healthy, there's a fundamental trade-off between how quickly you respond and how accurate the data is.

System architects must make dual decisions: one for the partitioned case (availability vs consistency), and one for the normal case (latency vs consistency).

Understanding these trade-offs allows engineers to make informed decisions based on system requirements. For example, financial systems may accept higher latency for consistent, correct transactions, while social media platforms might tolerate stale reads for faster, highly available service.

Below doc has CAP and PACELC and Replication notes - 

https://docs.google.com/document/d/1PuOwgCJJ1cwWavwESir-poBz4sKFPZDLD06x-vYB-iM/edit?tab=t.0


Facebook Newsfeed case study - 

https://github.com/kanmaytacker/system-design/blob/master/case-studies/02-newsfeed-answers-T.md

Sharding - 

Database Sharding – In-Depth Explanation
Database sharding is a technique used in distributed databases to horizontally partition data across multiple databases or servers (called shards) to improve scalability, performance, and availability.

At its core, sharding is about breaking a large database into smaller, faster, and more manageable pieces, without compromising the ability to retrieve the data when needed.

1. What is Sharding?
Sharding is a form of horizontal partitioning where each shard holds a subset of the rows from a table. Each shard has the same schema but stores different data. The application or database layer decides which shard to query based on some logic (like user ID or geographic region).

For example, instead of storing 1 billion users in one database, you can split the users across 10 different databases (shards), each storing 100 million users.

2. Why Sharding is Needed
As applications grow in data size and number of users, a single database server may become a bottleneck. Common problems that lead to sharding:

Storage limits: A single machine can’t store all the data.

Performance degradation: Reads/writes become slow due to increased load.

Throughput limitations: A single server has limited CPU, RAM, and IOPS.

Scaling vertically (adding resources to a single server) becomes expensive and eventually hits a ceiling.

Sharding addresses these by distributing load and data across multiple machines, allowing horizontal scaling—you can simply add more machines as needed.

3. How Sharding Works
The basic steps in sharding are:

Select a sharding key: A field in the data used to decide which shard holds which data. For example, user_id, region, customer_id, etc.

Define a sharding strategy: Decide how to split data across shards using the key.

Route queries: When a request comes in, use the sharding logic to route the query to the appropriate shard.

4. Sharding Strategies
There are several ways to distribute data across shards:

a. Range-Based Sharding
Data is split based on ranges of the sharding key.

Example:

User IDs 1–1,000,000 in Shard 1

User IDs 1,000,001–2,000,000 in Shard 2

Pros:

Simple and intuitive.

Easy to query a range of data.

Cons:

Risk of hotspots (some shards get more traffic than others).

Imbalanced shards if data distribution is skewed.

b. Hash-Based Sharding
Applies a hash function to the sharding key, then uses the hash result to decide the shard.

Example:

hash(user_id) % number_of_shards

Pros:

Better data distribution.

Reduces risk of hotspots.

Cons:

Difficult to query ranges.

Adding/removing shards is complex (requires resharding).

c. Directory-Based Sharding
Maintains a central directory or lookup service that maps keys to shards.

Pros:

Fully customizable data placement.

Easy to handle complex distributions.

Cons:

The directory becomes a single point of failure or bottleneck.

Higher complexity and management overhead.

d. Geographic or Feature-Based Sharding
Data is divided based on logical boundaries such as region, language, or feature type.

Example:

Users in Europe go to EU shard

Users in Asia go to Asia shard

Pros:

Naturally aligned with user behavior.

Improves latency if shards are geo-distributed.

Cons:

Uneven data distribution is likely.

Doesn’t work well if users move across regions.

5. Challenges of Sharding
While sharding helps scale systems, it introduces complexity:

a. Query Complexity
Cross-shard queries become harder.

Joins across shards are expensive or not supported.

b. Resharding
When data outgrows current shards or needs to be redistributed, resharding is required.

Resharding involves moving data between shards, which can be resource-intensive and disruptive.

c. Data Integrity and Transactions
Multi-shard transactions are complex or avoided.

Many systems sacrifice ACID guarantees for performance and scalability.

d. Operational Complexity
Backups, monitoring, and deployments must be shard-aware.

Requires additional infrastructure to manage routing and balancing.


How to Decide a Sharding Key – In-Depth Explanation
The sharding key is a specific field or attribute in a database record used to determine which shard (or database server) will store that record.
Choosing the right sharding key is critical because it impacts system performance, scalability, data distribution, and maintainability.

1. What Makes a Good Sharding Key?
A good sharding key should ideally satisfy the following:

Evenly distributes data across shards (avoids hotspots).

Minimizes cross-shard queries (improves efficiency).

Stable and not likely to change frequently (avoids data migration).

Correlates with query patterns (localizes reads/writes to one shard).

Supports horizontal scaling (new shards can be added easily).

2. Criteria for Choosing a Sharding Key
a. Cardinality
The key should have high cardinality (many distinct values) to ensure even distribution.

b. Access Pattern
Choose a key based on how the application reads and writes data. If most queries are by user_id, then that may be a good sharding key.

c. Write Distribution
The key should prevent write hotspots, where too many writes target the same shard.

3. Sharding Key Examples Across Use Cases
Example 1: Social Media Platform (Users and Posts)
Use Case:
Millions of users.

Users create posts, comments, and likes.

Most operations are per user (fetch posts, feed, etc.).

Ideal Sharding Key: user_id
Why?

High cardinality: millions of users.

Queries are user-centric (fetch user’s data).

Easy to horizontally scale.

Even distribution assuming users are uniformly active.

Pitfall to avoid:

If only a few users are highly active (e.g., celebrities), their shards can become hotspots. In such cases, consider composite keys or rate-limiting.

Example 2: E-commerce Platform (Orders)
Use Case:
Orders table grows rapidly.

Orders are mostly accessed per user.

Some analytics need to scan recent orders.

Sharding Key Options:
user_id

Great for retrieving a user's orders.

Poor for global order analytics — needs cross-shard access.

order_id with hash-based sharding

Distributes evenly.

Prevents hotspots.

Makes it hard to query by user (requires secondary indexes or query routing).

Composite Key: region + user_id

Helps geo-localize data.

Reduces cross-country traffic.

Best choice depends on primary access pattern — if most reads are user-specific, user_id works. If most writes are massive and need even spread, order_id with a hash may be better.

Example 3: Logging System
Use Case:
Millions of log entries per hour.

Most queries are time-based (last 1 hour logs).

Writes are continuous and high-volume.

Ideal Sharding Key: timestamp (with ranges or bucketed time windows)
Why?

Keeps time-based queries efficient.

Allows archiving or pruning of old data.

But needs careful handling to avoid write hotspots (e.g., all logs for “now” go to one shard).

Improvement: Use time-windowed hashing, like hash of (timestamp / 10 minutes) to balance load.

Example 4: Multi-Tenant SaaS Application
Use Case:
Each customer (tenant) has isolated data.

Tenants can be very large or very small.

Access patterns are tenant-scoped.

Ideal Sharding Key: tenant_id
Why?

Segregates tenant data.

Enables per-tenant scaling and isolation.

Supports compliance and security constraints.

Challenge: Some tenants may be huge (uneven load). For large tenants, consider sub-sharding using tenant_id + customer_id.

Example 5: Geo-Distributed Messaging App
Use Case:
Users send messages to others.

Messages are fetched per conversation.

Users are from different regions.

Options:
user_id – localizes to sender, but conversations involve multiple users.

conversation_id – better fit if all messages in a conversation are stored together.

region + user_id – useful for geo-locality, reduces latency.

Best choice: conversation_id, assuming most queries are to load chat threads. To avoid hotspots, use a hashed conversation ID.

4. Common Mistakes in Choosing a Sharding Key
Low cardinality keys (e.g., gender, country) lead to uneven data distribution.

Mutable keys (keys that can change, like email or phone number) make re-sharding difficult.

Choosing a key that doesn’t match query patterns leads to constant cross-shard joins.

Ignoring growth trends — initial key might work well with 10K users but not with 10 million.


Master-Slave Architecture & Read Replicas – In-Depth Explanation
In distributed database systems, master-slave architecture and read replicas are techniques used to scale reads, improve fault tolerance, and sometimes increase availability. However, they come with trade-offs around consistency, latency, and complexity.

Let’s break each of these down in detail, including how high- and low-consistent master-slave setups work.

1. What is Master-Slave Architecture?
In a master-slave (or primary-secondary) setup:

One node is designated as the master (or primary).

One or more nodes are designated as slaves (or secondaries).

Roles:
Master: Handles all write operations (INSERT, UPDATE, DELETE).

Slaves: Handle read operations and replicate changes from the master.

Replication from master to slaves can be:

Synchronous: Slaves are updated immediately with each master write.

Asynchronous: Slaves are updated after the master has written the data (with a delay).

2. What is a Read Replica?
A read replica is a read-only copy of a database used to scale read operations. It’s a specific case of a slave in master-slave setups, optimized for high-volume reads.

Key Points:
Replicated from the master.

Cannot accept writes (write attempts are either blocked or cause errors).

Usually asynchronous, meaning there's a replication lag between master and replica.

Useful for reporting, analytics, dashboards, and load balancing read-heavy applications.

3. Benefits of Master-Slave and Read Replicas
a. Improved Read Scalability
Reads can be distributed across slaves/replicas, reducing the load on the master.

b. Separation of Concerns
Analytics and reporting can be done on replicas, isolating them from the critical path of application queries.

c. High Availability (to some extent)
If the master fails, a slave can be promoted to become the new master (requires failover logic).

d. Backup and Maintenance Offload
Backups can be run on replicas to avoid performance hits on the master.

4. Drawbacks and Trade-offs
a. Replication Lag
In asynchronous setups, replicas may lag behind the master, leading to stale reads.

b. Write Scalability Limitations
Writes can only happen on the master. To scale writes, you need sharding, not just replication.

c. Failover Complexity
Handling master failure and promoting a slave requires careful coordination, and might cause downtime or data inconsistency if not done properly.

d. Eventual Consistency
Systems may show stale or outdated data temporarily because slaves haven’t yet caught up with the master.

5. High vs Low Consistent Master-Slave Architectures
The consistency level in a master-slave system depends on how writes are replicated and how reads are managed.

High-Consistency Master-Slave Architecture
Characteristics:
Replication is synchronous: the master waits until slaves acknowledge the write before confirming it to the client.

Reads are mostly directed to the master, or only sent to slaves that are guaranteed to be up-to-date.

Guarantees strong consistency: once a write is acknowledged, all future reads reflect the change.

Pros:
No stale reads.

Predictable system behavior.

Cons:
High latency for writes (master has to wait for slaves).

Reduced write throughput.

If a slave is slow or down, writes may block or fail.

Use Cases:
Financial systems, banking applications.

Systems where accuracy is more important than speed.

Low-Consistency Master-Slave Architecture
Characteristics:
Replication is asynchronous: master confirms writes immediately, slaves update later.

Reads are directed to any slave, regardless of replication lag.

Results in eventual consistency: there’s a window of time when some nodes have old data.

Pros:
High throughput and low latency for writes.

Reads can be scaled out massively.

Suitable for high-traffic systems where occasional stale data is acceptable.

Cons:
Clients may read stale or incorrect data.

Conflict resolution is needed in failover or recovery scenarios.

Requires developers to reason about inconsistencies.

Use Cases:
Social media timelines, user activity logs, comment systems.

Analytics and metrics platforms where perfect freshness isn’t required.

6. Real-World Example: MySQL Replication
MySQL supports master-slave replication out of the box.

A master writes changes to a binary log.

Slaves read from this log and apply the changes.

Admins can set:

Asynchronous replication (default).

Semi-synchronous replication, where master waits for at least one slave to acknowledge.

Group replication for stronger consistency with quorum-style writes.

7. Read-After-Write Consistency Problem
In low-consistent setups, there's a common issue called read-after-write inconsistency:

A client performs a write to the master and then performs a read from a slave (before replication completes), getting outdated data.

Solutions include:

Reading from the master for a short duration after writing.

Using a “sticky” read-replica strategy (route read and write to same node temporarily).

Delay reads from slaves or only allow "read your writes" on master.

8. Failover in Master-Slave Systems
When a master fails:

One of the slaves is promoted to become the new master.

Other slaves reconfigure to follow the new master.

This may involve data loss if replication was asynchronous and not fully applied.

To prevent this:

Use semi-synchronous or fully synchronous replication.

Employ a consensus protocol (e.g., Raft, Paxos) to safely elect a new master.

Conclusion
Master-slave architecture and read replicas are essential techniques for scaling read-heavy applications and separating concerns between operational and analytical workloads.

Read replicas improve scalability but introduce challenges around consistency.

The choice between high-consistent and low-consistent architectures depends on application needs:

If correctness is critical, use synchronous replication and read from the master.

If speed and scalability matter more than freshness, use asynchronous replication with read replicas.


------------

SQL Vs NoSQL - 

To scale the database we do the sharding but this also has issues like Single point of failure, bottleneck and low throughtput.

If we replicate each shard then it solves the issue of Single point of failure, bottleneck, and low throughput as reads will be handled by replicas and if master fails then replica will be promoted to the master and will start serving reads and writes. 


Replication lag - It is the time taken to replicate the writes from the master node.

 
 Relational database - 
 
 stores data in structured format.
 Follows acid properties.
 
 Normalization - relational databases helps to avoid the data redudancy.
 
 In case of ecommerce platform the product will have different parameters ex - Cloths will have color, size, style and laptop will have ram, hdd, processor etc.
 
 This kind of data is not structured and it can not be stored in the Relational database. 
 
 We can also store the JSON data in one column in a relation databases but again it is not possible normalize the data.
 
 Scaling - Relational databases do not have out of the box sharding support. 
 
 And if we shard the relational database then it nullifies the ACID properties.
 
 When to use SQL - 
 
 if data is structured, Scale, Read heavy system (indexing).
 
 When to use NoSQL - 
 
 if data is not structured or semi-structured, Large Scale, write heavy system (no indexing). Sharding is supported out of the box.


NoSQL databases properties -  

NoSQL databases are built by keeping horizontle scalling in mind. 

 1) Basically Available - It means database accesibility is available at all the times but in sudden surge in traffic a system might prioritize certain workflows compared to others.

2) Eventually Consistent.  
3) Horizontle scalling - out of the box support.
4) Denormalised.
 
 
 Types of NoSQL databases - 
 
 1) Key-Value -
 
 {
 "name":"Sarthak"
 }
 
 This is hashmap kind of structure and both the keys and values are stored as strings. 
 
 Redis, Memcache, DynamoDB are the examples of key value database.
 
2) Document - This can store the nested key value pairs (key value pairs inside other key values).

This database is an extended form of the key value database.

MongoDB, CouchBase, CockroachDB, Elastic Search etc.

3) Columnar (wide column database) - in this data is stored in columns instead of rows. 

This is used for analytics purpose as we only will have to fetch the data from single row. 

Cassandra, clickhouse, HBase are the examples of columnar database. 

Redis hands on practice - https://github.com/kanmaytacker/system-design/blob/master/demos/nosql/redis.md


Redis stores values in volatile memory and also in disk if configured. 

It will first store data in RAM in key value format and if we add configuration in redis.conf then it will store these values from RAM in file called dump.rdb and this file will be updated from time to time as configured in the configuration. 
When system restarts then redis will get all data from dump file and store it in the RAM for faster access. 

dump.rdb file will be stored in hard disk.

This process of storing data from RAM to file is known as snapshot.

There are two ways in redis to store the data from RAM to file - 
1) Save - This is blocking call. It blocks the request from user if it comes while updating data in the file.

2) BGSAVE - This updates data in background and it is non blocking call.

when to use - 

RDBMS - Structured data, ACID properties, Read Heavy (indexing), Hard to Scale. 

NoSQL - Semi structured, BASE, Write Heavy(Indexing), Supports out of the box sharding and scale.


Twitter - hash tags 

-> The most popular and recent tweets for a hashtags, for this feature write throughput should be very high and in this case we can not use RDBMS.   

To store most recent tweets for a hashtags we will use key value NoSQL database. 

And to store all tweets for each hashtag we will use columnar database as In this we will store all the tweets of one hashtag in one column and its easy to fetch. 


Sport Match live score - 

This system will be write heavy. In this we can use key value database.  


Any system will have multiple databases for seperate feature so it will scale and handle the traffic efficiently. 


Consistent hashing + Replication - 

In this on hash ring for each machine or node there will be replication nodes which will duplicate data of master node and serve the read requests. In this if master node goes down then it will promote one of the slave node to master node. and if one of the slave node crashes then next slave node will serve the read requests. 

When one machine crashes it will promote other machine this results into less data movement from one master node on hash ring to other master node on hash ring. 

Leader election can be done by using zoookeeper. 

Consistent hashing is already implemented by all databases.


Database orchestration - Zookeeper helps in database orchestration. 


There will be standby machines which will replace the crashed machine as soon as it happens. First data will be replicated in the new machine and then it will start serving the requests. 

The standby machine is also called as cold standby and the standby machine which is in sync with master node data is called hot standby. Hot standby will start serving requets instantly.

Q - What happens to system consistency as the sum of "R" and "W" (R+W) increases in a Multi-Master database system?


Ans - system consistency improves.

 In multi-master systems, as R+W increases, more nodes participate in read/write operations, improving data consistency across the system. While this increases database operations, it ensures better data synchronization, crucial for applications like stock trading where up-to-date information is vital.
 
 
 Q - In a Multi-Master distributed database system, what does "R" represent in the context of system consistency?
 
 Ans - The minimum number of machines from which data must be read for a consistent query.
 
 Q - In a Multi-Master distributed database system, what is the relationship between "R," "W," and "X"?
 
 Ans - R and W must be less than or equal to X.
 
 R - Read, W - Write and X - No of nodes.
 
----------
 
Case Study - Type Ahead 

Get clarity from the interviewer about the problem statement. 

Ask clarifying questions about the problem statement, functional requirement, non functional requirements ( Performance, latency, scale, no of active users, Read heavy or write heavy, highly consistent or highly available), Scale estimations.

Then go for design - first go for brute force solution then improve on it. 


Design a type ahead search - 

Functional Requirements - 

For partial query give the typeahead suggestions.

Suggestions should be popular. 

Top 5 suggestions should be returned. 


Do not suggest features randomnly and also you will have to figure out implement solution for that. List of features should come from an interviewer not from you.


Once user enters more than 3 characters, user should get the suggestions.


Clear all of your doubts by asking questions.


We can store count of search terms, like how many times particular term is searched. even if this this count is not accurate it will still work, so for this feature we can have eventual consistency and stale data.


But this system should be highly available. 

PACELC - In this we are prioritizing availability and low latency like less than 200ms.

Scale estimations - 

A percentile tells you the value below which a given percentage of observations in a group falls.

For example:

P50 (50th percentile) → 50% of requests are faster than or equal to this time. This is the median.

P95 (95th percentile) → 95% of requests are faster than or equal to this time, and 5% are slower.

P99 → 99% of requests are below this threshold; used to highlight rare but significant slowdowns.

The scale of data will helps us in deciding type of database to use, to use sharding or not, Replication etc.

DAU - daily active users - 

Total users are 5 billion out of these only 20% are DAU = 1 Billion.


Total number of searches per day = DAU * Average_no_of_searches_per_user_per_day = 1 Billion * 20 = 20 Billion.


Per second we will get 2 lakh searches. 

For each search we will have 5 to 7 calls to system. 

so we will get 2 Million calls to the system per second. 

Every day we will get 2 Billion write calls as we will be storing the frequency of the search terms.

If average size of each search term is 20 bytes then per day data size will be = 20 bytes * 2 Billion = 40 GB per day.

so for whole year we will get = 40 * 365 = 14,600 GB = 150 TB approx.

Considering data size we will need sharding. 


APIs - 

Read - getSuggestions
Write - logSearch

for every search we are reading at least 10 times. So this system will be read heavy and also we will have to optimize the writes.


To store the count of each searched term if we use hashmap or database kind of structure it will be very slow to fetch the data. 


Redis can handle 1 lakh writes per second and 10 lakhs reads per second.

Type ahead notes - 

https://docs.google.com/document/d/1vyXOphd6pe2Lrt-9IC4Grkjrd3rBqoLVghxOmxLWXjo/edit?tab=t.0


We can use the trie to store the character in each node and next character of the term can be stored in child node of current node. For returning the results we will return the entire subtree of the node which is matching the searched term.

Trie is very efficient for prefix searches. 

But trie has huge memory requirement and just to generate the top 5 suggestions we will have to navigate the whole subtree. 

No real world database supports trie approach.

It is not possible to shard the trie with even data distribution as we will shard based on characters and this will cause the uneven data distribution.

We can also take hash of characters and then shard based on the hash.

In trie we will have to update the count of all prefix characters as we get the search for that term 

Read and write - 

Our system is not only read heavy it is also write heavy, there is no database which supports read and write heavy operations. We can reduce the no of writes by using caching, and update the database in batch. Like we will keep updating the count of searched terms in cache and then update it in the database asynchronously in batches after some threshold crosses ( ex count of likes - keep it updating on cache and update it on the database once it crosses some threshold every time), This will reduce the write load on database. 

So in this case we will use Relational database as now our system is read heavy.

----

NoSQL internals - 

Why is storing data in a NoSQL database is harder?

The data size in NoSQL is not fixed like Relational database. 

in relational database we can preallocate the space, perform indexing, directly jump from one data point to another very efficiently because datasize is predictable. 

Storage engine for MySQL is InnoDB.

Wired Tiger is storage engine of MongoDB.


Case study - key value database -

This will have get(key), set(key, value) apis. 

data will be stored in key and value format and it will be stored in csv file.

TC - 

get(key) - O(n)
set(key, value) - o(n) - as need to check if data is already present or not.


Solution 2 - optimise the set operation. 

insert happens in constant time but update takes time. 

to optimse we will append the each input data to the file even though the record already exists. This concept is called as Write ahead logging and this optimizes the write and updates ( O(1) ).

This can use more memory as it may store the duplicate records. 


Now to remove duplicate entries and compact the space used by the file we can use compaction. The compaction job will run periodically and It will create new file and store the latest data for all entries and remove old entries. This will improve the space complexity of the file.

If file is very large then compaction will be done in chunks. 


Solution 3 - Optimizing the read queries - 

We will read data from the end of the file from write ahead log file, so we will get the latest data always.

To optimise the reads we will store the key and memory address of the value locations of those keys in hashmap. And read the data from this hashmap. Now reads will have time complexity of O(1). This hashmap will be stored in the memory not in disk.

SQL internal notes - 

https://github.com/kanmaytacker/system-design/blob/master/notes/no-sql-internals.md

NoSQL update problem notes - 

https://docs.google.com/document/d/1PXT_e4JYdME7Ezcmmy6jGbxxIZFV6fhVoNS2-nX-H74/edit?tab=t.0


Log structured merge tree - 






An LSM Tree (Log-Structured Merge Tree) is a data structure designed for efficient write performance, especially when handling large-scale, high-throughput systems. It is commonly used in NoSQL database engines such as Cassandra, RocksDB, LevelDB, and HBase. The core idea behind an LSM Tree is to batch writes in memory and then persist them to disk in a sequential, sorted manner, minimizing the costly random I/O operations that are typical in traditional B-Tree-based systems.

What Problem Does LSM Tree Solve

Traditional databases often use B-Trees or B+ Trees to store and retrieve data. These structures are efficient for read-heavy workloads but become a bottleneck for write-heavy workloads due to random disk I/O during insertions and updates. LSM Trees were developed to address this problem by turning random writes into sequential writes and by delaying the actual persistence of data to disk until it can be batched.

This strategy significantly improves performance in systems where writes are much more frequent than reads, or where maintaining fast write latency is critical.

Basic Structure of an LSM Tree

An LSM Tree consists of multiple components that manage data in both memory and disk:

MemTable (Memory Table)

This is an in-memory, sorted data structure (often implemented as a skip list or a balanced tree).

All incoming writes are first written to the MemTable.

This allows for fast in-memory insertions and updates.

Write-Ahead Log (WAL)

Before inserting data into the MemTable, it is appended to a write-ahead log.

This log ensures durability. In case of a crash, the system can recover data from the WAL.

SSTables (Sorted String Tables)

When the MemTable reaches a certain size, it is flushed to disk as an immutable, sorted file called an SSTable.

SSTables are stored in levels or tiers on disk.

They are optimized for sequential I/O and maintain sorted order.

Compaction Process

Over time, multiple SSTables accumulate. To avoid performance degradation, a background process called compaction merges multiple SSTables.

During compaction, overlapping entries are reconciled, deleted data is purged, and duplicate keys are removed.

Compaction ensures that lookup time remains reasonable by reducing the number of files that must be searched.

Write Path in LSM Tree

A write request arrives with a key-value pair.

The write is appended to the WAL for durability.

The data is inserted into the MemTable.

If the MemTable exceeds its size limit, it is flushed to disk as a new SSTable.

Background compaction processes may merge SSTables to keep the system efficient.

Read Path in LSM Tree

A read request first checks the MemTable.

If the key is not found, the system searches through SSTables starting from the newest to the oldest.

To optimize this search, Bloom filters are often used with each SSTable to check if a key might be present before performing disk I/O.

Indexes and metadata help speed up the search within SSTables.

Compaction Strategies

Different compaction strategies determine how SSTables are merged:

Size-Tiered Compaction

SSTables of similar sizes are grouped and merged.

Used in systems like Apache Cassandra.

Leveled Compaction

SSTables are organized into levels with progressively larger size limits.

SSTables in a level do not overlap, which reduces the number of files to search during reads.

Used in LevelDB and RocksDB.

Time-Window Compaction

Data is grouped based on time intervals, typically used in time-series databases.

Helps optimize for data that is read or deleted in time-based patterns.

Benefits of LSM Trees in NoSQL Databases

High Write Throughput

Since writes are batched in memory and written to disk sequentially, write operations are extremely fast compared to traditional B-Tree systems that perform random I/O for each write.

Efficient Use of Disk I/O

LSM Trees minimize random writes to disk, which are expensive, and instead rely on sequential I/O, which is much faster.

Compression and Compaction

Immutable SSTables can be compressed and periodically merged, reducing disk space and improving read performance.

Scalability

LSM Trees are well-suited for large-scale data systems, particularly those dealing with append-heavy workloads, like logging systems, time-series databases, and distributed databases.

Trade-offs and Challenges

Read Amplification

Since data might exist in multiple SSTables, and the MemTable, a single read may need to check several locations. Compaction reduces this effect, but it cannot be eliminated entirely.

Write Amplification

Data is written multiple times throughout its lifecycle. First to the WAL, then to the SSTable, and potentially multiple times during compaction. This increases the I/O workload.

Compaction Overhead

Compaction is an I/O-intensive process. If not managed properly, it can affect system performance and lead to latency spikes.

Latency Variability

Reads and writes may experience latency variability due to background compaction processes and disk I/O spikes.

How LSM Tree is Used in Popular NoSQL Systems

Apache Cassandra

Cassandra uses LSM Trees for its storage engine. Each column family has its own MemTable and SSTables. Compaction strategies are pluggable and configurable.

RocksDB

RocksDB is a high-performance embedded key-value store developed by Facebook. It is a direct descendant of LevelDB and uses a leveled compaction strategy by default.

LevelDB

A key-value store developed by Google, LevelDB uses LSM Trees and leveled compaction. It is lightweight and suitable for embedded systems.

HBase

HBase, built on top of Hadoop and HDFS, uses MemStores (MemTables) and HFiles (SSTables). It also incorporates compaction and Bloom filters for performance.

Summary

An LSM Tree is a disk-friendly, write-optimized data structure that is particularly well-suited for NoSQL systems that handle massive volumes of writes. It works by writing data to memory first, and then flushing it to disk in immutable, sorted files called SSTables. Background compaction helps merge and reorganize data to maintain read efficiency. The LSM Tree's design trades some read complexity for drastically improved write performance, making it ideal for workloads where write throughput is a primary concern.

what happens to the MemTable when a machine storing the data reboots?

The MemTable is reconstructed by replaying Write-Ahead Logs (WAL).

What is the purpose of the in-memory index in a NoSQL database's SSTable?

To facilitate binary searches in the file





Bloom filter - 


A Bloom filter is a space-efficient probabilistic data structure that is used to test whether an element is a member of a set. It is primarily used in scenarios where space is at a premium and occasional false positives are acceptable, but false negatives are not.

This means:

If the Bloom filter tells you an element is not present, it is definitely not present in the set.

If it tells you the element might be present, there is a small probability that this is a false positive.

In other words, Bloom filters guarantee no false negatives, but they may return false positives.

Core Concepts and How Bloom Filter Works

Bit Array Initialization

A Bloom filter uses a bit array of fixed size m, where all bits are initially set to 0. This array is the only structure used to track the existence of elements.

Hash Functions

The Bloom filter uses k different, independent hash functions. Each hash function takes an input and maps it uniformly to one of the positions in the bit array — from 0 to m-1.

Insertion Operation

To insert an element into the Bloom filter:

The element is passed through all k hash functions.

Each hash function returns a position (index) in the bit array.

The bits at each of these k positions are set to 1.

Since multiple elements can hash to the same position, some bits may already be set to 1 due to previous insertions.

Query Operation

To check if an element is present in the Bloom filter:

The element is hashed again using the same k hash functions.

The bits at each of the k computed positions are checked.

If any one of them is 0, the element is definitely not in the set.

If all of them are 1, the element may be in the set.

This is what makes Bloom filters probabilistic. There can be a case where the bits are set by other elements, leading to a false positive.

Advantages of Bloom Filter

Extremely space-efficient

It can represent large sets using very little memory compared to storing all elements.

Fast insertion and query

Both operations take constant time O(k), where k is the number of hash functions.

No need to store actual elements

The Bloom filter only uses a bit array and does not require keeping the actual data.

Scales well with data

Especially useful in read-heavy or write-heavy systems where latency and memory use are critical.

Disadvantages of Bloom Filter

False positives

Bloom filters may return that an element is in the set when it is not. This is called a false positive.

Cannot delete elements (in standard Bloom filter)

Once bits are set, unsetting them could affect the membership of other elements. However, some variants like Counting Bloom Filters solve this by maintaining counters instead of bits.

Efficiency degrades with more elements

As more elements are added, the probability of false positives increases because more bits are set to 1.

Use Cases of Bloom Filter

Databases and Key-Value Stores

In systems like Cassandra, LevelDB, RocksDB, and HBase (which use LSM trees), Bloom filters are used to avoid unnecessary disk reads. When searching for a key, the Bloom filter helps decide whether a particular on-disk file (SSTable) could contain the key. If the Bloom filter says the key is not in that file, the database skips reading it from disk.

Caching Systems

Bloom filters are used to check if an item is in the cache before attempting to fetch it from the backend. This prevents cache pollution and improves performance.

Web Security

Used by browsers to maintain compact lists of malicious URLs. If the URL is not in the Bloom filter, the browser can safely skip further checks.

Spell Checkers

Used to quickly test if a word is likely valid, reducing the need for full dictionary lookups.

Distributed Systems

Useful in membership testing in distributed hash tables and content delivery networks.

False Positive Rate and Trade-offs

The false positive rate of a Bloom filter depends on:

m: size of the bit array

n: number of inserted elements

k: number of hash functions used

The more bits you allocate and the more hash functions you use, the lower the false positive rate, up to a point. However, increasing k also increases the time to insert and query.

The optimal number of hash functions is:

ini
Copy
Edit
k = (m / n) * ln(2)
And the false positive probability (roughly) is:

ini
Copy
Edit
p = (1 - e^(-kn/m))^k
This means that the probability of a false positive increases as more elements are inserted or if the bit array size m is small relative to the number of elements n.

Variants of Bloom Filter

Counting Bloom Filter

Instead of a bit array, it uses an array of counters. Allows deletions by decrementing the counters.

Scalable Bloom Filter

Dynamically grows the size of the Bloom filter as more elements are added, keeping the false positive rate bounded.

Compressed Bloom Filter

Compresses the bit array for transmission over networks.

Summary

A Bloom filter is a fast, memory-efficient, probabilistic data structure used to test set membership. It is widely adopted in large-scale data systems, especially NoSQL databases, for optimizing read paths by avoiding unnecessary disk access. It works well in environments where false positives are acceptable but false negatives are not. Its design makes it especially suitable for high-throughput, low-latency systems.






----------------

Case study - messaging - 

Notes - 

https://docs.google.com/document/d/1mA8puhUUIYPTFbAURF53bNK3PPV5eOJpr_ttQkZjwGw/edit?tab=t.0


https://github.com/kanmaytacker/system-design/blob/master/case-studies/03-messenger-questions.md


User should be able to send messages to other user.

User should be able to see recent messages.

User should be able to see rececived messages.



getRecentConversation( userId)

getMessages( conversation_id, sender_id, limit, offset) - limit and offset for paginating the response and conversation_id is the unique id which identifies the conversation between two users.

sendMEssage( conversation_id, sender_id, message)


Cardinality between conversation and messages - one conversation can have many messages and one message can be part of only one conversation.


500 million are the daily active users of the facebook.

on average each user will have 5 no of conversations.

Average no of messages in each conversation are 10. 

500 Million * 5 * 10 = 25 Billion messages per day. 

average size of each 
Each message will have -

message id - 8 bytes
conversation id - 8 bytes
sender id - 8 bytes
receiver id - 8 bytes
message - approx 170 bytes

so total each message size will be = 200 bytes.


25 Billion * 200 = 5 TB of data each day. 


5 TB * 365 * 10 = 18250 TB = 18.25 PB.


This system is write and read heavy. To handle this situation we will have to either handle reads or writes seperately without database. In this case we will reduce the number of reads, by using caching like mechanism.


This system should be highly consistent, but this will increase the latency, so to reduce the latency we will have to compromise the consistency. 

How to avoid sending duplicate messages - 

if user 1 sends a message with id m1 and it does not receive the acknowledgement from the server that this message is received, in this case system will send same message again, this scenario will result into sending one message multiple times. 

To avoid this duplicate message it uses the concept of idempotency. 

Idempotency means for the same input one must receive the same output. 

when front end will send the message id with the message, even if front end sends one message multiple times then backend will check if the message with same message id is already sent to the receiver, if it is already sent then it will not resend the same message to the receiver and it will send the acknowledgement to the sender that this message is received by the receiver. 


Sharding key - 

In this we can have user_id or conversation_id based sharding.


conversation id already has user ids of sender and receiver. conversation_id = senderId_recevierId.

so out of these two which key will reduce the no of hops in each operation (read and write).

 
Q - In the context of sharding strategies for messaging systems, which use case is best suited for shard-by-user-id and shard-by-conversation-id approaches, respectively?

Shard by user id: Messenger, Shard by conversation id: Slack.

groups are created in slack, not in messanger.

Q - In a messaging system, which design approach is preferable for handling message sending to ensure high consistency?

Write to the recipient's database first, then the sender's.











 



 



 




------------

Monday - chest + Abs
Tuesday - Biceps + Abs
Wednesday - Shoulders + Abs.
Thursday - Back + Abs.
Friday - Triceps + Abs.
Saturday - Legs + Abs.

--------

Naam Jap.
Meditation.
Write down your goal.
Switch off your phone.
Follow Do, Will do, What we achieved today, Regret - write down this daily.

---------------



Namaste Javascript - Akshay saini

LLD + HLD by Shrayansh Jain

SQL by Ankit Bansal

Java & Spring Boot by Shreyansh Jain

Frontend Machine Coding & System Design by Piyush Agarwal

System Design by Gaurav Send
Desigining Microservices by Arpit Bhayani

Mern Stack project Playlist by Kishan Shah
Mern Stack projects playlist by Geeksforgeeks


----------------


Magicpatterns to generate UI designs.


Prompt - Do not make any changes, until you have 95% confidence that you know what to build ask me follow up questions untill you have that confidence.


